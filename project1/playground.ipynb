{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc1b6346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2402.10928.pdf', '2207.09537.pdf', '2404.04372.pdf', '2412.10603.pdf', '2406.17607.pdf', '2409.16727.pdf', '2409.05756.pdf', '2502.11667.pdf', '2404.18936.pdf', '2405.12643.pdf', '2408.01743.pdf', '2311.06594.pdf', '2405.02212.pdf', '2412.12794.pdf', '2310.11651.pdf', '2412.06834.pdf', '2501.06936.pdf', '2204.03369.pdf', '2411.10812.pdf', '2302.14634.pdf', '2412.11670.pdf', '2403.17102.pdf', '2411.13035.pdf', '2406.01963.pdf', '2412.01499.pdf', '2502.12081.pdf', '2502.01595.pdf', '2412.04112.pdf', '2407.16324.pdf', '2502.12069.pdf', '2502.12070.pdf', '2502.09951.pdf', '2502.11217.pdf', '2207.12374.pdf', '2502.06883.pdf', '2411.17601.pdf', '2410.08790.pdf', '2501.17290.pdf', '2501.07357.pdf', '2310.01615.pdf', '2410.00298.pdf', '2412.10269.pdf', '2403.10628.pdf', '2411.09071.pdf', '2408.06498.pdf', '2403.05398.pdf', '2409.08412.pdf', '2305.11992.pdf', '2502.11297.pdf', '2209.08931.pdf', '2311.07055.pdf', '2107.04792.pdf', '2308.00058.pdf', '2405.08836.pdf', '2410.16263.pdf', '2201.05926.pdf', '2502.11946.pdf', '2411.16973.pdf', '2411.09477.pdf', '2410.21061.pdf', '2412.14960.pdf', '2207.14424.pdf', '2501.09664.pdf', '2212.13652.pdf', '2502.08657.pdf', '2501.05562.pdf', '2501.01895.pdf', '2412.15313.pdf', '2412.01117.pdf', '2412.07624.pdf', '2502.08990.pdf', '2502.00936.pdf', '2501.12002.pdf', '2502.07794.pdf', '2502.12135.pdf', '2412.06801.pdf', '2406.01627.pdf', '2501.07917.pdf', '2408.12491.pdf', '2410.22801.pdf', '2501.07354.pdf', '2502.01769.pdf', '2501.01495.pdf', '2502.05658.pdf', '2204.14257.pdf', '2501.14584.pdf', '2412.08304.pdf', '2408.03850.pdf', '2501.08080.pdf', '2405.10900.pdf', '2301.06653.pdf', '2410.23028.pdf', '2308.09814.pdf', '2501.10051.pdf', '2501.18412.pdf', '2210.03100.pdf', '2411.16221.pdf', '2410.19058.pdf', '2307.06216.pdf', '2310.14263.pdf', '2312.14754.pdf', '2502.10198.pdf', '2412.08570.pdf', '2412.12032.pdf', '2501.18204.pdf', '2502.08861.pdf', '2502.11047.pdf', '2410.19083.pdf', '2502.07449.pdf', '2502.07260.pdf', '2501.10434.pdf', '2502.08645.pdf', '2111.09604.pdf', '2502.02515.pdf', '2310.05749.pdf', '2502.03699.pdf', '2111.02884.pdf', '2407.14417.pdf', '2502.10866.pdf', '2212.05169.pdf', '2501.17240.pdf', '2402.12358.pdf', '2210.16387.pdf', '2302.10230.pdf', '2404.03083.pdf', '2411.19264.pdf', '2306.06460.pdf', '2312.07708.pdf', '2501.08685.pdf', '2502.01838.pdf', '2308.15826.pdf', '2310.09882.pdf', '2502.08012.pdf', '2203.09328.pdf', '2406.01704.pdf', '2203.10608.pdf', '2407.21277.pdf', '2502.06592.pdf', '2501.01911.pdf', '2502.06637.pdf', '2305.02462.pdf', '2412.14702.pdf', '2112.00829.pdf', '2203.04724.pdf', '2404.05721.pdf', '2407.00469.pdf', '2409.05388.pdf', '2501.14627.pdf', '2301.01689.pdf', '2401.04525.pdf', '2409.18349.pdf', '2502.11479.pdf', '2305.01205.pdf', '2411.07053.pdf', '2409.02244.pdf', '2501.08168.pdf', '2411.04793.pdf', '2409.11300.pdf', '2306.17449.pdf', '2502.08984.pdf', '2411.06599.pdf', '2501.00754.pdf', '2502.04253.pdf', '2411.12417.pdf', '2411.15221.pdf', '2412.08471.pdf', '2412.01383.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "pdfs_folder_path = \"pdfs\"\n",
    "pdf_files = os.listdir(pdfs_folder_path)\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aff0b720883dd329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T09:28:40.266725Z",
     "start_time": "2025-02-17T09:28:38.583084Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + \"\\n\"\n",
    "    return text\n",
    "\n",
    "def extract_texts_from_pdfs(folder_path):\n",
    "    pdf_texts = []\n",
    "    pdf_queue = deque(os.listdir(folder_path))\n",
    "\n",
    "    while pdf_queue:\n",
    "        pdf_file = pdf_queue.popleft()\n",
    "        if pdf_file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, pdf_file)\n",
    "            pdf_text = extract_text_from_pdf(pdf_path)\n",
    "            pdf_texts.append(pdf_text)\n",
    "    \n",
    "    return pdf_texts\n",
    "\n",
    "pdf_texts = extract_text_from_pdf(\"/root/arxiv-and-scholar-scraping/project1/pdfs/2411.15221.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2fa003cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Submissions and Reflections from the 2024 Large Language Model\\n(LLM) Hackathon for Applications in Materials Science and\\nChemistry\\nYoel Zimmermann† 38, Adib Bazgir† 54, Zartashia Afzal1, Fariha Agbere2, Qianxiang\\nAi 3, Nawaf Alampara 4, Alexander Al-Feghali 5, Mehrad Ansari 6, Dmytro Antypov 7,\\nAmro Aswad8, Jiaru Bai 9, Viktoriia Baibakova10, Devi Dutta Biswajeet11, Erik Bitzek 70,\\nJoshua D. Bocarsly 12, Anna Borisova13, Andres M. Bran 13, L. Catherine Brinson 17,\\nMarcel Moran Calderon13, Alessandro Canalicchio14, Victor Chen15, Yuan Chiang 10,16,\\nDefne Circi 17, Benjamin Charmes9, Vikrant Chaudhary 18,19, Zizhang Chen20, Min-Hsueh\\nChiu 21, Judith Clymo7, Kedar Dabhadkar22, Nathan Daelman 18, Archit Datar 74, Wibe\\nA. de Jong 16, Matthew L. Evans 23,24, Maryam Ghazizade Fard25, Giuseppe Fisicaro 26,\\nAbhijeet Sadashiv Gangan27, Janine George 4,43, Jose D. Cojal Gonzalez 18, Michael\\nGo¨tte28, Ankur K. Gupta 16, Hassan Harb 20, Pengyu Hong 21, Abdelrahman Ibrahim4,\\nAhmed Ilyas18, Alishba Imran31, Kevin Ishimwe2, Ramsey Issa33, Kevin Maik Jablonka 4,\\nColin Jones2, Tyler R. Josephson 2, Gergely Juhasz 34, Sarthak Kapoor 18, Rongda\\nKang35, Ghazal Khalighinejad 17, Sartaaj Takrim Khan 8, Sascha Klawohn 18, Suneel\\nKuman36, Alvin Noe Ladines 18, Sarom Leang37, Magdalena Lederbauer 13,38, Sheng-Lun\\n(Mark) Liao 35, Hao Liu39, Xuefeng Liu15,73, Stanley Lo 8, Sandeep Madireddy 20,\\nPiyush Ranjan Maharana 72, Shagun Maheshwari40, Soroush Mahjoubi 3, Jos´e A.\\nMa´rquez 18, Rob Mills 13, Trupti Mohanty 33, Bernadette Mohr 18,41, Seyed Mohamad\\nMoosavi 6,8, Alexander Moßhammer14, Amirhossein D. Naghdi 42, Aakash Naik 4,43,\\nOleksandr Narykov 20, Hampus Na¨sstr¨om 18, Xuan Vu Nguyen44, Xinyi Ni 30, Dana\\nO’Connor 45, Teslim Olayiwola 46, Federico Ottomano 7, Aleyna Beste Ozhan 3,\\nSebastian Pagel47, Chiku Parida48, Jaehee Park 15, Vraj Patel12, Elena Patyukova 7,\\nMartin Hoffmann Petersen 48, Luis Pinto49, Jos´e M. Pizarro 18, Dieter Plessers 50,\\nTapashree Pradhan50, Utkarsh Pratiush 51, Charishma Puli2, Andrew Qin15, Mahyar\\nRajabi 8, Francesco Ricci 16, Elliot Risch52, Martin˜o R´ıos-Garc´ıa4,53, Aritra Roy 71,\\nTehseen Rug14, Hasan M Sayeed 33, Markus Scheidgen 18, Mara Schilling-Wilhelmi 4,\\nMarcel Schloz 18, Fabian Scho¨ppach18, Julia Schumann 18, Philippe Schwaller 13, Marcus\\nSchwarting 15, Samiha Sharlin 2, Kevin Shen 55, Jiale Shi 3, Pradip Si56, Jennifer\\nD’Souza 57, Taylor Sparks 33, Suraj Sudhakar 15, Leopold Talirz 32, Dandan Tang 58,\\nOlga Taran59, Carla Terboven28, Mark Tropin61, Anastasiia Tsymbal 62,63, Katharina\\nUeltzen 43, Pablo Andres Unzueta 64, Archit Vasan 20, Tirtha Vinchurkar40, Trung\\nVo 11, Gabriel Vogel65, Christoph Vo¨lker 14, Jan Weinreich 66, Faradawn Yang15, Mohd\\nZaki 67, Chi Zhang7, Sylvester Zhang 5, Weijie Zhang 58, Ruijie Zhu 15, Shang Zhu 69,\\nJan Janssen 70, Calvin Li75, Ian Foster 15,20, and Ben Blaiszik∗ 15,20\\n1University of the Punjab\\n2University of Maryland, Baltimore County\\n3Massachusetts Institute of Technology\\n4Friedrich-Schiller-Universita¨t Jena\\n1\\n5202\\nnaJ\\n3\\n]GL.sc[\\n2v12251.1142:viXra\\n5McGill University\\n6Acceleration Consortium\\n7University of Liverpool\\n8University of Toronto\\n9University of Cambridge\\n10University of California at Berkeley\\n11University of Illinois at Chicago\\n12University of Houston\\n13EPFL\\n14iteratec GmbH\\n15University of Chicago\\n16Lawrence Berkeley National Laboratory\\n17Duke University\\n18Humboldt University of Berlin\\n19Technology University of Darmstadt\\n20Argonne National Laboratory\\n21University of Southern California\\n22Lam Research\\n23Universit´e catholique de Louvain\\n24Matgenix SRL\\n25Queen’s University\\n26CNR Institute for Microelectronics and Microsystems, Catania\\n27University of California at Los Angeles\\n28Helmholtz-Zentrum Berlin fu¨r Materialien und Energie GmbH\\n30Brandeis University\\n31Kleiner Perkins\\n32Schott\\n33University of Utah\\n34Tokyo Institute of Technology\\n35Factorial Energy\\n36Molecular Forecaster\\n37EP Analytics, Inc.\\n38ETH Zurich\\n39Fordham University\\n40Carnegie Mellon University\\n41University of Amsterdam\\n42IDEAS NCBR\\n43Federal Institute of Materials Research and Testing (BAM)\\n44Universita` degli Studi di Milano\\n45Pittsburgh Supercomputing Center\\n46Louisiana State University\\n47University of Glasgow\\n48Technical University of Denmark\\n49Independent Researcher\\n50KU Leuven\\n51University of Tennessee, Knoxville\\n2\\n52Enterprise Knowledge\\n53Instituto de Ciencia y Tecnolog´ıa del Carbono\\n54University of Missouri-Columbia\\n55NobleAI\\n56University of North Texas\\n57TIB Leibniz Information Centre for Science and Technology\\n58University of Virginia\\n59University of California at Davis\\n60Helmholtz-Zentrum Berlin fu¨r Materialien und Energie GmbH\\n61Windmill Labs\\n62Rutgers University\\n63University of Pennsylvania\\n64Stanford University\\n65Delft University of Technology\\n66Quastify GmbH\\n67Indian Institute of Technology Delhi\\n68RWTH Aachen University\\n69University of Michigan-Ann Arbor\\n70Max-Planck Institute for Sustainable Materials\\n71London South Bank University\\n72CSIR-National Chemical Laboratory\\n73LinkDot.AI\\n74Celanese Corporation\\n75Fum Technologies, Inc.\\n∗Correspondingauthor: blaiszik@uchicago.edu†Theseauthorsalsocontributedsubstantiallytocompiling\\nteam results and other paper writing tasks\\nAbstract\\nHere, we present the outcomes from the second Large Language Model (LLM) Hackathon for Appli-\\ncations in Materials Science and Chemistry, which engaged participants across global hybrid locations,\\nresulting in 34 team submissions. The submissions spanned seven key application areas and demon-\\nstrated the diverse utility of LLMs for applications in (1) molecular and material property prediction;\\n(2)molecularandmaterialdesign;(3)automationandnovelinterfaces;(4)scientificcommunicationand\\neducation;(5)researchdatamanagementandautomation;(6)hypothesisgenerationandevaluation;and\\n(7) knowledge extraction and reasoning from scientific literature. Each team submission is presented in\\na summary table with links to the code and as brief papers in the appendix. Beyond team results, we\\ndiscuss the hackathon event and its hybrid format, which included physical hubs in Toronto, Montreal,\\nSan Francisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable local and virtual\\ncollaboration. Overall,theeventhighlightedsignificantimprovementsinLLMcapabilitiessincethepre-\\nviousyear’shackathon,suggestingcontinuedexpansionofLLMsforapplicationsinmaterialsscienceand\\nchemistryresearch. TheseoutcomesdemonstratethedualutilityofLLMsasbothmultipurposemodels\\nfor diverse machine learning tasks and platforms for rapid prototyping custom applications in scientific\\nresearch.\\nIntroduction\\nScience hackathons have emerged as a powerful tool for fostering collaboration building, innovation, and\\nrapid problem-solving in the scientific community [1–3]. By leveraging social media, virtual platforms, and\\n3\\nhybrid event structures, such hackathons can be organized in a cost-effective manner while maximizing\\ntheir impact and reach. In this article, we first introduce the project submissions to the second Large\\nLanguageModelHackathonforApplicationsinMaterialsScienceandChemistry,detailingthebroadclasses\\nof problems addressed by teams, while analyzing trends and patterns in the approaches taken. We then\\npresent each team submission in turn, plus a summary table with the names of team members and links\\nto code repositories where available. Finally, we include the detailed project documents submitted by each\\nteam, showcasing the depth and breadth of innovation demonstrated during the hackathon.\\nOverview of Submissions\\nThe hackathon resulted in 34 team submissions (with 32 submissions with a written description included\\nhere), categorized as shown in Table 1. From these submissions, we identified seven key application areas:\\n1. Molecular and Material Property Prediction: Forecasting chemical and physical properties of\\nmolecules and materials using LLMs, particularly excelling in low-data environments and combining\\nstructured/unstructured data.\\n2. Molecular and Material Design: Generation and optimization of novel molecules and materials\\nusing LLMs, including peptides, metal-organic frameworks, and sustainable construction materials.\\n3. Automation and Novel Interfaces: Development of natural language interfaces and automated\\nworkflows to simplify complex scientific tasks, making advanced tools and techniques more accessible\\nto researchers.\\n4. ScientificCommunicationandEducation: Enhancementofacademiccommunication,automation\\nof educational content creation, and facilitation of learning in materials science and chemistry.\\n5. Research Data Management and Automation: Streamlining the handling, organization, and\\nprocessing of scientific data through LLM-powered tools and multimodal agents.\\n6. Hypothesis Generation and Evaluation: Generation, assessment, and validation of scientific hy-\\npotheses using LLMs, often combining multiple AI agents and statistical approaches.\\n7. Knowledge Extraction and Reasoning: Extraction of structured information from scientific liter-\\nature and sophisticated reasoning about chemical and materials science concepts through knowledge\\ngraphs and multimodal approaches.\\nWe next discuss each application area in more detail and highlight exemplar projects in each.\\n1. Molecular and Material Property Prediction\\nLLMshaverapidlyadvancedinthisarea,employingbothtextualandnumericaldatatoforecastawiderange\\nof properties. Recent studies [4–6] show LLMs performing comparably to, or even surpassing, conventional\\nmachine learning methods in this domain, particularly in low-data environments. Their flexibility in pro-\\ncessing both structured and unstructured data, as well as their general applicability to regression tasks [7],\\nmakes them a powerful tool for diverse predictive tasks in molecular and materials science.\\nExemplar projects: The Learning LOBSTERs team (Ueltzen et al.) integrated bonding analysis data\\ninto LLMs to predict the phonon density of states (DOS) for crystal structures, showing that combining\\ncovalent bonding information with LLM capabilities can improve prediction accuracy. The Liverpool Ma-\\nterials team (Ottomano et al.) demonstrated how adding context from materials literature, via models like\\nMatSciBert [8], could improve the prediction of Li-ion conductivity, particularly when dealing with limited\\ntrainingdata. TheMolFoundationteam(Harbetal.) benchmarkedChemBERTa[9]andT5-Chem[10]for\\nmolecularpropertypredictions,findingthatpre-trainedmodelsperformedcomparablytofine-tunedmodels,\\nsuggesting that expensive fine-tuning might not always be required. Another contribution came from the\\nGeometric Geniuses team (Weinreich et al.), who focused on incorporating 3D molecular geometries into\\nLLMs, experimenting with encoding geometric features to improve prediction outcomes.\\n4\\n2. Molecular and Material Design\\nLLMs have also advanced in molecular and material design, proving capable in both settings [11–15], es-\\npecially if pre-trained or fine-tuned with domain-specific data [16]. However, despite these advancements,\\nLLMs still face limitations in practical applications [17].\\nExemplar projects: During the hackathon, teams tackled these challenges through different approaches.\\nTheteambehindMC-Peptide(Branetal.) developedaworkflowharnessingLLMsforthedesignofmacro-\\ncyclic peptides (MCPs). By employing semantic search and constraint-based generation, they automated\\nthe extraction of literature data to propose MCPs with improved permeability, crucial for drug develop-\\nment [18,19]. Meanwhile, the MOF Innovators team (Ansari et al.) focused on metal-organic frameworks\\n(MOFs). Their AI agent utilized retrieval-augmented generation (RAG) [20] to incorporate design rules\\nextracted from the literature, and an ensemble of fine-tuned surrogate models to optimize MOF’s band gap\\nproperty. In pursuit of sustainable materials, the Green Construct team (Canalicchio et al.) investigated\\nsmall-scale LLMs such as Llama 3 [21] and Phi-3 [22] to streamline the design of alkali-activated concrete.\\nTheirmodelsprovidedinsightsintoreducingemissionsoftraditionalmaterialslikePortlandcementthrough\\nzero-shot learning.\\n3. Automation and Novel Interfaces\\nLLMs are increasingly used to simplify and streamline access to complex tools [23–25], autonomously plan\\nandexecutetasks[26], andinterfacewithroboticsystemsinlabsettings[27,28]. Thesecapabilitiesenhance\\ntheefficiencyofscientificworkflows,allowingresearcherstofocusonhigher-levelproblem-solvingratherthan\\nroutine tasks. As LLMs continue to evolve, they are expected to further transform laboratory practices and\\ndemocratize access to advanced experimental and computational techniques.\\nExemplar projects: The team behind LangSim (Chiang et al.) addressed the complexity of atomistic\\nsimulation software by creating a natural language interface to automate the calculation of material prop-\\nerties such as bulk modulus. By integrating LangChain [29] agents with LLMs, LangSim enables flexible\\nconstruction and execution of simulation workflows, reducing user barriers and expanding functionality to\\nmulti-component alloys. Similarly, LLMicroscopilot (Schloz & Gonzalez) seeks to simplify the operation\\nof sophisticated microscopes by using LLM-powered agents to automate tasks like experimental parame-\\nter estimation. This could reduce the need for highly trained operators, making advanced microscopy tools\\nmoreaccessible. IntherealmofDensityFunctionalTheory(DFT)calculations,theteambehindT2Dllama\\n(Parida&Petersen)developedatoolthatusesRAGtoextractoptimizedsimulationparametersfromscien-\\ntificliterature,aimingtoassistexperimentalistswithcomplexDFTsetups. Thetoolsimplifiestheprocessof\\nobtaining reliable parameters and reduces dependency on computational chemistry expertise. Complement-\\ning this, Materials Agent (Datar et al.) provides a tool-calling capability for cheminformatics, combining\\nmolecularpropertycalculations,simulations,anddocumentinteractionthroughanaturallanguageinterface.\\nThis agent was developed to lower the barrier for researchers in cheminformatics and to accelerate the pace\\nof research by integrating tools like RDKit [30] and custom simulations into an intuitive system.\\n4. Scientific Communication and Education\\nLLMs are transforming how scientific and educational content is created and shared, enhancing accessibility\\nand personalized learning [31–34]. By automating tasks like question generation, feedback, and grading,\\nLLMs streamline educational processes, freeing educators to focus on individual learning needs. Addi-\\ntionally, LLMs assist in translating complex scientific findings into accessible formats, broadening public\\nengagement [34]. However, technological readiness, transparency, and ethical concerns around data privacy\\nand bias remain critical challenges to address [31,33].\\nExemplar projects: One submission in this category is MaSTeA (Circi et al.), an automated system\\nfocused on evaluating the effectiveness of LLMs as teaching assistants by testing their performance on ma-\\nterials science questions from the MaScQA [35] dataset. By analyzing different types of questions—such as\\nmultiple-choice, match-the-following, and numerical problems—across 14 topical categories, the team iden-\\ntified the strengths and weaknesses of various models. An interactive platform was developed to facilitate\\neasy model testing and, as models get better, to eventually help students practice answering questions and\\n5\\nunderstand solution steps. Meanwhile, the LLMy Way team (Zhu et al.) focused on simplifying the pro-\\ncessofcreatingacademicpresentationsbyusingLLMstoautomaticallygeneratestructuredslidedecksfrom\\nresearcharticles. Thetoolformatssummariesofpapersintotypicalsections—suchasbackground,methods,\\nand results—and transforms them into presentation slides. It offers customization based on audience exper-\\ntise and time constraints, aiming to make the communication of complex topics more efficient and effective.\\nLastly, the WaterLLM team (Baibakova et al.) sought to address water pollution challenges, particularly\\nin decentralized communities lacking centralized water treatment infrastructure. They developed a chatbot\\nthat uses LLMs enhanced with RAG to suggest optimal water purification methods for microplastic con-\\ntamination. Groundedinup-to-datescientificliterature, thechatbotprovidestailoredpurificationprotocols\\nbasedoncontaminantcharacteristics,resourceavailability,andcostconsiderations,promotingeffectivewater\\ntreatment solutions for underserved areas.\\n5. Research Data Management and Automation\\nVarious submission were received in this area that attempt to enhance the management, accessibility, and\\nautomation of scientific data workflows using LLMs. These efforts, often leveraging multimodal agents, aim\\nto simplify complex data handling, improve reproducibility, and accelerate insights across diverse scientific\\ndisciplines.\\nExemplar projects: yeLLowhaMmer (Evans et al.), a multimodal LLM-based data management agent\\nautomates data handling within electronic lab notebooks (ELNs) and laboratory information management\\nsystems (LIMS). The tool processes text and image instructions to generate Python code targeting the\\nAPI of the datalab ELN/LIMS system [1], performing tasks such as summarizing experiments or digitizing\\nhandwritten notes. Powered by the Claude 3 Haiku model [36] and LangChain [29], yeLLowhaMmer was\\ndevelopedtosimplifytheinteractionwithscientificdataandhasfuturepotentialforintegratingaudiovisual\\ndata processing. In parallel, the team behind LLMads (Kapoor et al.) explored how LLMs can be used\\nto automate data parsing by converting raw scientific data, like X-ray diffraction (XRD) measurements,\\ninto structured metadata schemas. LLMads aims to replace the need for custom parsers with models like\\nMixtral-8x7b [37], which extract data from raw files and populate schemas automatically. The process\\ninvolves chunking raw data and prompt engineering, although challenges such as hallucinations and the\\nextractionofmulti-dimensionaldatastillrequirefurtherrefinement. TheNOMADQueryReporterteam\\n(Daelman et al.) developed an LLM-based agent that uses RAG to generate context-aware summaries from\\nlarge materials science repositories like NOMAD [38]. The tool produces detailed reports on experimental\\nmethodologies and results, which could help researchers in drafting publication sections. Using a self-hosted\\nLlama3 70B model, the system engages in multi-turn conversations to ensure context retention. Further, in\\nthe Speech-schema-filling project, N¨asstr¨om et al. integrated speech recognition and LLMs to automate\\nthe conversion of spoken language into structured data within lab environments. This method, utilizing\\nOpenAI’s Whisper [39] for transcription and LLMs for schema selection and population, allows researchers\\nto input experimental data verbally, which is then structured into JSON or Pydantic models for entry into\\nELNs, which could be especially helpful in situations where manual entry is difficult.\\n6. Hypothesis Generation and Evaluation\\nLLMs can be leveraged to streamline scientific inquiry, hypothesis generation, and verification. Recent\\nwork across psychology, astronomy, and biomedical research demonstrates their capacity to generate novel,\\nvalidatedhypothesesbyintegratingdomain-specificdatastructureslikecausalgraphs[40–43]. Althoughstill\\nlargelyuntappedinchemistryandmaterialsscience,thisapproachholdssubstantialpromiseforaccelerating\\ndiscovery and innovation in these fields [44,45].\\nExemplar projects: In an individual contribution, Marcus Schwarting employed LLMs and temporal\\nBayesianstatisticstoassessscientificclaims,exemplifiedbyevaluatingthehypothesisthatLK-99isaroom-\\ntemperature superconductor. By using LLM-based natural language inference (NLI) to classify relevant\\nstudies and applying Bayesian updating, his system tracks how the scientific consensus evolves over time,\\nwith the goal of allowing even non-experts to gauge quickly the validity of a claim. In a related project, the\\nThoughtful Beavers team (Ozhan & Mahjoubi) prototyped a Multi-Agent System of specialized LLMs\\nthat aims to accelerate scientific hypothesis generation by coordinating agents that extract background\\n6\\ninformation, generate inspirations, and propose hypotheses, which are then evaluated for feasibility, utility,\\nandnovelty. Byapplyingthe“TreeofThoughts”framework[46],thissystemstreamlinesthecreativeprocess\\nand improves the quality of scientific hypotheses, as demonstrated through a case study on sustainable\\nconcrete design. Another project, ActiveScience (Chiu), integrated LLMs, knowledge graphs, and RAG\\nto extract high-level insights from scientific articles. Focused on materials science research, particularly\\nalloys, this framework organizes extracted information into a Neo4j knowledge graph, allowing for complex\\nquerying and discovery. Additionally, a first-pass peer review system was developed using fine-tuned LLMs,\\nG-Peer-T (Al-Feghali & Zhang), to assess materials science papers. By analyzing the log probabilities of\\nabstracts, the system flags works that deviate from typical scientific language patterns, helping to identify\\nboth highly innovative and potentially nonsensical research. Preliminary findings suggest that highly cited\\npapers tend to use less typical language, highlighting the potential for LLMs to support the peer review\\nprocess and detect outlier research.\\n7. Knowledge Extraction and Reasoning\\nHere the interest is in LLMs enabling extraction of structured scientific knowledge from unstructured text,\\nassisting researchers in navigating complex academic content [47–50]. These systems streamline tasks like\\nnamed entity recognition and relation extraction, offering flexible solutions tailored to materials science and\\nchemistry [48]. Tool-augmented frameworks help LLMs address complex reasoning by leveraging scientific\\ntools and resources, expanding their utility as assistants in scientific research [51].\\nExemplar projects: The team behind ChemQA (Khalighinejad et al.) created a multimodal question-\\nand-answering dataset for chemical understanding. It highlights the importance of combining textual and\\nvisual inputs (e.g., SMILES and molecule images) to improve model accuracy in tasks like counting atoms,\\nmolecular weight calculations, and retrosynthesis planning. This multimodal approach shows how founda-\\ntional models in chemistry benefit from rich, diverse data representations. In the realm of lithium metal\\nbatteries, LithiumMind (Ni et al.), integrated LLMs to extract Coulombic Efficiency (CE) and electrolyte\\ninformationfromscientificliterature. TheteamalsodevelopedachatbotpoweredbyRAGtoanswerqueries\\non lithium battery research. By constructing a knowledge graph, the system visualizes relationships be-\\ntween materials and properties, enhancing research accessibility and fostering domain-specific insights. The\\nKnowMat project (Sayeed et al.) tackled the transformation of unstructured materials science literature\\ninto structured formats, using LLMs to extract essential information and convert it into machine-readable\\ndata (JSON). KnowMat’s customizable prompts and integration capabilities streamline data extraction and\\nanalysis, making it a potentially valuable tool for researchers looking to accelerate their workflow in ma-\\nterials science. A similar project, Ontosynthesis (Ai et al.), explored extracting structured data from\\norganic synthesis texts using LLMs. By employing Resource Description Framework (RDF) graphs, the\\nproject improved how chemical reactions are represented, with the extracted data aiding in retrosynthesis\\nand condition recommendation tasks. This approach could help bridge the gap between unstructured chem-\\nical descriptions and standardized data formats. For high entropy alloys (HEAs) in hydrogen storage, the\\nInsightEngineersteam(Pradhan&Biswajeet)exploredsyntheticdatagenerationasameanstoaccelerate\\npredictivemodeling. TheteamproposedusingLLMslikeGPT-4[52],alongsidecustompromptsandaRAG\\nframework, to generate synthetic data for machine learning interatomic potentials (MLIPs) to overcome the\\ncomputational challenges associated with HEA properties prediction. Lastly, GlossaGen (Lederbauer et\\nal.) tackled the challenge of generating glossaries for academic papers and grant proposals. It uses LLMs to\\nextractscientifictermsanddefinitionsfromPDForTeXfilesandpresentstheminaknowledgegraph. This\\nvisualization of relationships between concepts enhances understanding, and there are possible expansions\\nof the tool to offer LaTeX integration and more refined ontology creation to aid researchers in navigating\\ncomplex scientific terminology.\\nHackathon Event Overview\\nOn May 9th, 2024, we hosted the second annual Large Language Model (LLM) Hackathon for Applications\\nin Materials Science and Chemistry. The event brought together students, postdocs, researchers from in-\\ndustry, citizen scientists, and more, with participants joining both virtually and at in-person sites across\\n7\\nFigure1: LLMHackathonforApplicationsinMaterialsandChemistryhybridhackathon. Researcherswere\\nable to participate from both remote and in-person locations (purple pins).\\nmultiple continents (Figure 1). The event was a follow-up to the previous hackathon described in detail\\nhere. [53] The event began with a kickoff panel featuring leading researchers from academia and industry,\\nincluding Elsa Olivetti (MIT), Jon Reifsneider (Duke), Michael Craig (Valence Laboratories), and Marwin\\nSegler (Microsoft). The charge of the hackathon was intentionally open-ended; i.e., to explore the vast po-\\ntentialapplicationspace, andcreatetangibledemonstrationsofthemostinnovative, impactful, andscalable\\nsolutions in a constrained time using open-source and best-in-class multimodal models applied to problems\\nin materials science and chemistry.\\nEvent registration included 556 participants and over 120 researchers comprising 34 teams that sub-\\nmitted completed projects. The hybrid format proved particularly successful, with physical hub locations\\nin Toronto, Montreal, San Francisco, Berlin, Lausanne, and Tokyo facilitating local collaboration while\\nmaintaining global connectivity across the physical hubs and remote participants through virtual platforms\\n(Figure 1). This distributed approach enabled researchers to participate from either a local site or remotely\\nfrom anywhere on Earth. This format blended the strengths of in-person events with the flexibility of re-\\nmote participation leading to an inclusive event that led to team formation that crossed international and\\ninstitutional boundaries, the submitted projects in this paper, and the growth of a new persistent online\\ncommunity of 483 researchers via Slack.\\n8\\nConclusion\\nTheLLMHackathonforApplicationsinMaterialsScienceandChemistryhasdemonstratedthedualutility\\nand immense promise of large language models serving as 1) multipurpose models for diverse machine\\nlearning tasks and 2) platforms for rapid prototyping. Participants effectively utilized LLMs to tackle\\nspecific challenges while rapidly evaluating their ideas over a short 24-hour period, showcasing their ability\\nto enhance the efficiency and creativity of research processes in highly diverse ways. It’s important to note\\nthatmanyprojectsbenefitedfromsignificantadvancementsinLLMperformancesincelastyear’shackathon.\\nThat is, the performance across the diverse application space was improved simply via the release of new\\nversions of Gemini, ChatGPT, Claude, Llama, and other models. If this trend continues, we expect to see\\neven broader applications in subsequent hackathons, and in materials science and chemistry more generally.\\nAdditionally, the hackathon hybrid format has proven effective towards creating new scientific collabora-\\ntionsandcommunities. Byunitingindividualsfromvariousbackgroundsandareasofexpertise,theseevents\\nfacilitate knowledge exchange and promote interdisciplinary approaches, which are essential for advancing\\nresearch in this rapidly evolving field.\\nAs the integration of LLMs continues to expand, collaborative initiatives like hackathons will play a\\ncritical role in driving innovation and addressing complex challenges in chemistry, materials science, and\\nbeyond. The outcomes from this event highlight the significance of leveraging LLMs for their adaptability\\nand their potential to accelerate the development of new concepts and applications.\\nTable 1: Overview of the tools developed by the various tools, and links to source code repositories. Full\\ndescriptions of the projects can be found in the appendix.\\nProject Authors Links\\nMolecular and Material Property Prediction\\nLeveraging Orbital-Based Bonding Analysis Katharina Ueltzen, Aakash Naik, GitHub\\nInformation in LLMs Janine George\\nContext-Enhanced Material Property Prediction Federico Ottomano, Elena GitHub\\n(CEMPP) Patyukova, Judith Clymo, Dmytro\\nAntypov, Chi Zhang, Aritra Roy,\\nPiyush Ranjan Maharana, Weijie\\nZhang, Xuefeng Liu, Erik Bitzek\\nMolFoundation: Benchmarking Chemistry LLMs on Hassan Harb, Xuefeng Liu, GitHub\\nPredictive Tasks Anastasiia Tsymbal, Oleksandr\\nNarykov, Dana O’Connor, Shagun\\nMaheshwari, Stanley Lo, Archit\\nVasan, Zartashia Afzal, Kevin Shen\\n3D Molecular Feature Vectors for Large Language Jan Weinreich, Ankur K. Gupta, GitHub\\nModels Amirhossein D. Naghdi, Alishba\\nImran\\nLLMSpectrometry Tyler Josephson, Fariha Agbere, GitHub\\nKevin Ishimwe, Colin Jones,\\nCharishma Puli, Samiha Sharlin, Hao\\nLiu\\nMolecular and Material Design\\nMC-Peptide: An Agentic Workflow for Data-Driven Andres M. Bran, Anna Borisova, GitHub\\nDesign of Macrocyclic Peptides Marcel M. Calderon, Mark Tropin,\\nRob Mills, Philippe Schwaller\\n9\\nProject Authors Links\\nLeveraging AI Agents for Designing Low Band Gap Sartaaj Khan, Mahyar Rajabi, Amro GitHub\\nMetal-Organic Frameworks Aswad, Seyed Mohamad Moosavi,\\nMehrad Ansari\\nHow Low Can You Go? Leveraging Small LLMs for Alessandro Canalicchio, Alexander GitHub\\nMaterial Design Moßhammer, Tehseen Rug,\\nChristoph V¨olker\\nAutomation and Novel Interfaces\\nLangSim Yuan Chiang, Giuseppe Fisicaro, GitHub\\nGreg Juhasz, Sarom Leang,\\nBernadette Mohr, Utkarsh Pratiush,\\nFrancesco Ricci, Leopold Talirz,\\nPablo A. Unzueta, Trung Vo, Gabriel\\nVogel, Sebastian Pagel, Jan Janssen\\nLLMicroscopilot: assisting microscope operations Marcel Schloz, Jose C. Gonzalez GitHub\\nthrough LLMs\\nT2Dllama: Harnessing Language Model for Density Chiku Parida, Martin H. Petersen GitHub\\nFunctional Theory (DFT) Parameter Suggestion\\nMaterials Agent: An LLM-Based Agent with Archit Datar, Kedar Dabhadkar GitHub\\nTool-Calling Capabilities for Cheminformatics\\nLLM with Molecular Augmented Token Luis Pinto, Xuan Vu Nguyen, Tirtha GitHub\\nVinchurkar, Pradip Si, Suneel\\nKuman\\nScientific Communication and Education\\nMaSTeA: Materials Science Teaching Assistant Defne Circi, Abhijeet S. Gangan, GitHub\\nMohd Zaki\\nLLMy-Way Ruijie Zhu, Faradawn Yang, Andrew GitHub\\nQin, Suraj Sudhakar, Jaehee Park,\\nVictor Chen\\nWaterLLM: Creating a Custom ChatGPT for Water Viktoriia Baibakova, Maryam G. GitHub\\nPurification Using PromptEngineering Techniques Fard, Teslim Olayiwola, Olga Taran\\nResearch Data Management and Automation\\nyeLLowhaMMer: A Multi-modal Tool-calling Agent Matthew L. Evans, Benjamin GitHub\\nfor Accelerated Research Data Management Charmes, Vraj Patel, Joshua D.\\nBocarsly\\nLLMads Sarthak Kapoor, Jos´e M. Pizarro, GitHub\\nAhmed Ilyas, Alvin N. Ladines,\\nVikrant Chaudhary\\nNOMAD Query Reporter: Automating Research Nathan Daelman, Fabian Sch¨oppach, GitHub\\nData Narratives Carla Terboven, Sascha Klawohn,\\nBernadette Mohr\\n10\\nProject Authors Links\\nSpeech-schema-filling: Creating Structured Data Hampus N¨asstr¨om, Julia Schumann, GitHub\\nDirectly from Speech Michael G¨otte, Jos´e A. M´arquez\\nHypothesis Generation and Evaluation\\nLeveraging LLMs for Bayesian Temporal Evaluation Marcus Schwarting GitHub\\nof Scientific Hypotheses\\nMulti-Agent Hypothesis Generation and Verification Aleyna Beste Ozhan, Soroush GitHub\\nthrough Tree of Thoughts and Retrieval Augmented Mahjoubi\\nGeneration\\nActiveScience Min-Hsueh Chiu GitHub\\nG-Peer-T: LLM Probabilities For Assessing Scientific Alexander Al-Feghali, Sylvester GitHub\\nNovelty and Nonsense Zhang\\nKnowledge Extraction and Reasoning\\nChemQA Ghazal Khalighinejad, Shang Zhu, GitHub\\nXuefeng Liu\\nLithiumMind - Leveraging Language Models for Xinyi Ni, Zizhang Chen, Rongda GitHub\\nUnderstanding Battery Performance Kang, Sheng-Lun Liao, Pengyu\\nHong, Sandeep Madireddy\\nKnowMat: Transforming Unstructured Material Hasan M. Sayeed, Ramsey Issa, GitHub\\nScience Literature into Structured Knowledge Trupti Mohanty, Taylor Sparks\\nOntosynthesis Qianxiang Ai, Jiaru Bai, Kevin Shen, GitHub\\nJennifer D’Souza, Elliot Risch\\nKnowledge Graph RAG for Polymer Simulation Jiale Shi, Weijie Zhang, Dandan GitHub\\nTang, Chi Zhang\\nSynthetic Data Generation and Insightful Machine Tapashree Pradhan, Devi Dutta GitHub\\nLearning for High Entropy Alloy Hydrides Biswajeet\\nChemsense: Are large language models aligned with Martin˜o R´ıos-Garc´ıa, Nawaf GitHub\\nhuman chemical preference? Alampara, Mara Schilling-Wilhelmi,\\nAbdelrahman Ibrahim, Kevin Maik\\nJablonka\\nGlossaGen Magdalena Lederbauer, Dieter GitHub\\nPlessers, Philippe Schwaller\\nAcknowledgments\\nPlanningforthiseventwassupportedbyNSFAwards#2226419and#2209892. Wewouldliketothankevent\\nsponsors who provided platform credits and prizes for teams, including RadicalAI, Iteratec, Reincarnate,\\nAcceleration Consortium, and Neo4j.\\nReferences\\n[1] A.Nolte, L.B.HaydenandJ.D.Herbsleb, Proc. ACM Hum.-Comput. Interact., 2020, 4, 1–23.https:\\n//doi.org/10.1145/3392830.\\n11\\n[2] E.P.P.Pe-ThanandJ.D.Herbsleb, inLecture Notes in Computer Science, Springer, 2019, vol.11546,\\npp. 27–37. https://doi.org/10.1007/978-3-030-15742-5_3.\\n[3] B.Heller, A.Amir, R.WaxmanandY.Maaravi, J. Innov. Entrep., 2023, 12, 1.https://doi.org/10.\\n1186/s13731-023-00269-0.\\n[4] K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero and B. Smit, Nat. Mach. Intell., 2024, 6, 161–169.\\nhttps://doi.org/10.1038/s42256-023-00788-1.\\n[5] C. Qian, H. Tang, Z. Yang, H. Liang and Y. Liu, arXiv, 2023. https://arxiv.org/abs/2307.07443.\\n[6] R. Jacobs, M. P. Polak, L. E. Schultz, H. Mahdavi, V. Honavar and D. Morgan, arXiv, 2024. https:\\n//arxiv.org/abs/2409.06080.\\n[7] R. Vacareanu, V. A. Negru, V. Suciu and M. Surdeanu, in First Conference on Language Modeling,\\n2024. https://openreview.net/forum?id=LzpaUxcNFK.\\n[8] A.K.GuptaandK.Raghavachari,J.Chem.TheoryComput.,2022,18,2132–2143.https://doi.org/\\n10.1021/acs.jctc.1c00504.\\n[9] S. Chithrananda, G. Grand and B. Ramsundar, arXiv, 2020. https://arxiv.org/abs/2010.09885.\\n[10] J.LuandY.Zhang,J.Chem.Inf.Model.,2022,62,1376–1387.https://doi.org/10.1021/acs.jcim.\\n1c01467.\\n[11] D. Bhattacharya, H. J. Cassady, M. A. Hickner and W. F. Reinhart, J. Chem. Inf. Model., 2024, 64,\\n7086–7096. https://doi.org/10.1021/acs.jcim.4c01396.\\n[12] G.Liu,M.Sun,W.Matusik,M.JiangandJ.Chen,arXiv,2024.https://arxiv.org/abs/2410.04223.\\n[13] S. Jia, C. Zhang and V. Fung, arXiv, 2024. https://arxiv.org/abs/2406.13163.\\n[14] H. Jang, Y. Jang, J. Kim and S. Ahn, arXiv, 2024. https://arxiv.org/abs/2410.03138.\\n[15] J. Lu, Z. Song, Q. Zhao, Y. Du, Y. Cao, H. Jia and C. Duan, arXiv, 2024. https://arxiv.org/abs/\\n2410.18136.\\n[16] A. Kristiadi et al., in Proceedings of the 41st International Conference on Machine Learning, PMLR,\\n2024, vol. 235, pp. 25603–25622. https://proceedings.mlr.press/v235/kristiadi24a.html.\\n[17] S. Miret and N. M. A. Krishnan, arXiv, 2024. https://arxiv.org/abs/2402.05200.\\n[18] X. Ji, A. L. Nielsen and C. Heinis, Angew. Chem. Int. Ed., 2023, 63, 3. https://doi.org/10.1002/\\nanie.202308251.\\n[19] M. L. Merz et al., Nat. Chem. Biol., 2023, 20, 624–633. https://doi.org/10.1038/\\ns41589-023-01496-y.\\n[20] P. Lewis et al., arXiv, 2020. https://arxiv.org/abs/2005.11401.\\n[21] A. Dubey et al., arXiv, 2024. https://arxiv.org/abs/2407.21783.\\n[22] M. Abdin et al., arXiv, 2024. https://arxiv.org/abs/2404.14219.\\n[23] A. M. Bran, S. Cox, O. Schilter, C. Baldassari, A. D. White and P. Schwaller, arXiv, 2023. https:\\n//arxiv.org/abs/2304.05376.\\n[24] Y. Song et al., arXiv, 2023. https://arxiv.org/abs/2306.06624.\\n[25] H. Zhang, Y. Song, Z. Hou, S. Miret and B. Liu, arXiv, 2024. https://arxiv.org/abs/2409.00135.\\n[26] D. A. Boiko, R. MacKnight, B. Kline and G. Gomes, Nature, 2023, 624, 570–578. https://doi.org/\\n10.1038/s41586-023-06792-0.\\n12\\n[27] K. Darvish et al., arXiv, 2024. https://arxiv.org/abs/2401.06949.\\n[28] G. Tom et al., Chem. Rev., 2024, 124, 9633–9732. https://doi.org/10.1021/acs.chemrev.4c00055.\\n[29] H. Chase, Langchain, 2024. https://github.com/langchain-ai/langchain.\\n[30] RDKit: Open-source cheminformatics, http://www.rdkit.org.\\n[31] L. Yan et al., Br. J. Educ. Technol., 2023, 55, 90–112. https://doi.org/10.1111/bjet.13370.\\n[32] S. Wang et al., arXiv, 2024. https://arxiv.org/abs/2403.18105.\\n[33] E. Kasneci et al., Learn. Individ. Differ., 2023, 103, 102274. https://doi.org/10.1016/j.lindif.\\n2023.102274.\\n[34] M. S. Sch¨afer, J. Sci. Commun., 2023, 22, 2. https://doi.org/10.22323/2.22020402.\\n[35] M. Zaki, Jayadeva, Mausam and N. M. A. Krishnan, arXiv, 2023. https://arxiv.org/abs/2308.\\n09115.\\n[36] Anthropic,TheClaude3ModelFamily: Opus, Sonnet, Haiku,2024.https://assets.anthropic.com/\\nm/61e7d27f8c8f5919/original/Claude-3-Model-Card.pdf.\\n[37] A. Q. Jiang et al., arXiv, 2024. https://arxiv.org/abs/2401.04088.\\n[38] C. Draxl and M. Scheffler, J. Phys. Mater., 2019, 2, 036001. https://doi.org/10.1088/2515-7639/\\nab13bb.\\n[39] A. Radford et al., arXiv, 2022. https://arxiv.org/abs/2212.04356.\\n[40] Y.Zhou,H.Liu,T.Srivastava,H.MeiandC.Tan,arXiv,2024.https://arxiv.org/abs/2404.04326.\\n[41] A. Abdel-Rehim et al., arXiv, 2024. https://arxiv.org/abs/2405.12258.\\n[42] S. Tong, K. Mao, Z. Huang, Y. Zhao and K. Peng, Humanit. Soc. Sci. Commun., 2024, 11, 1. https:\\n//doi.org/10.1057/s41599-024-03407-5.\\n[43] I. Ciuc˘a, Y.-S. Ting, S. Kruk and K. Iyer, arXiv, 2023. https://arxiv.org/abs/2306.11648.\\n[44] Q. Liu et al., arXiv, 2024. https://arxiv.org/abs/2409.06756.\\n[45] O. Shir, ChemRxiv, 2024. https://doi.org/10.26434/chemrxiv-2024-lf2xx.\\n[46] S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao and K. Narasimhan, in Ad-\\nvances in Neural Information Processing Systems, Curran Associates, Inc., 2023, vol.\\n36, pp. 11809–11822. https://proceedings.neurips.cc/paper_files/paper/2023/file/\\n271db9922b8d1f4dd7aaef84ed5ac703-Paper-Conference.pdf.\\n[47] M. Shamsabadi, J. D’Souza and S. Auer, arXiv, 2024. https://arxiv.org/abs/2401.10040.\\n[48] J. Dagdelen et al., Nat. Commun., 2024, 15, 1. https://doi.org/10.1038/s41467-024-45563-x.\\n[49] D. Xu et al., arXiv, 2024. https://arxiv.org/abs/2312.17617.\\n[50] J. Li, M. Zhang, N. Li, D. Weyns, Z. Jin and K. Tei, ACM Trans. Auton. Adapt. Syst., 2024, 19, 1–60.\\nhttps://doi.org/10.1145/3686803.\\n[51] Y. Ma et al., arXiv, 2024. https://arxiv.org/abs/2402.11451.\\n[52] OpenAI et al., arXiv, 2023. https://arxiv.org/abs/2303.08774.\\n[53] K. M. Jablonka et al., Digital Discovery, 2023, 2, 1233–1250. https://doi.org/10.1039/d3dd00113j.\\n13\\nAppendix: Individual Project Reports\\nTable of Contents\\n1 LeveragingOrbital-BasedBondingAnalysisInformationinLLMs 15\\n2 Context-EnhancedMaterialPropertyPrediction(CEMPP) 17\\n3 MolFoundation: BenchmarkingChemistryLLMsonPredictiveTasks 19\\n4 3DMolecularFeatureVectorsforLargeLanguageModels 22\\n5 LLMSpectrometry 25\\n6 MC-Peptide: AnAgenticWorkflowforData-DrivenDesignofMacrocyclicPeptides 27\\n7 LeveragingAIAgentsforDesigningLowBandGapMetal-OrganicFrameworks 30\\n8 HowLowCanYouGo? LeveragingSmallLLMsforMaterialDesign 34\\n9 LangSim 37\\n10 LLMicroscopilot: assistingmicroscopeoperationsthroughLLMs 40\\n11 T2Dllama: HarnessingLanguageModelforDensityFunctionalTheory(DFT)ParameterSuggestion 42\\n12 MaterialsAgent: AnLLM-BasedAgentwithTool-CallingCapabilitiesforCheminformatics 45\\n13 LLMwithMolecularAugmentedToken 49\\n14 MaSTeA:MaterialsScienceTeachingAssistant 52\\n15 LLMy-Way 55\\n16 WaterLLM:CreatingaCustomChatGPTforWaterPurificationUsingPrompt-EngineeringTechniques 57\\n17 yeLLowhaMMer: AMulti-modalTool-callingAgentforAcceleratedResearchDataManagement 60\\n18 LLMads 62\\n19 NOMADQueryReporter: AutomatingResearchDataNarratives 63\\n20 Speech-schema-filling: CreatingStructuredDataDirectlyfromSpeech 65\\n21 LeveragingLLMsforBayesianTemporalEvaluationofScientificHypotheses 68\\n22 Multi-AgentHypothesisGenerationandVerificationthroughTreeofThoughtsandRetrievalAugmentedGeneration 71\\n23 ActiveScience 75\\n24 G-Peer-T:LLMProbabilitiesForAssessingScientificNoveltyandNonsense 77\\n25 ChemQA:EvaluatingChemistryReasoningCapabilitiesofMulti-ModalFoundationModels 78\\n26 LithiumMind-LeveragingLanguageModelsforUnderstandingBatteryPerformance 81\\n27 KnowMat: TransformingUnstructuredMaterialScienceLiteratureintoStructuredKnowledge 84\\n28 Ontosynthesis 86\\n29 KnowledgeGraphRAGforPolymerSimulation 89\\n30 SyntheticDataGenerationandInsightfulMachineLearningforHighEntropyAlloyHydrides 91\\n31 Chemsense: Arelargelanguagemodelsalignedwithhumanchemicalpreference? 93\\n32 GlossaGen 96\\n14\\n1 Leveraging Orbital-Based Bonding Analysis Information in LLMs\\nAuthors: Katharina Ueltzen, Aakash Naik, Janine George\\nLLMswererecentlydemonstratedtoperformwellformaterialspropertyprediction,especiallyinthelow-\\ndatalimit[1,2]. TheLearningLOBSTERsteamfine-tunedmultipleLlama3modelsontextualdescriptions\\nof1264crystalstructurestopredictthehighest-frequencypeakintheirphonondensityofstates(DOS)[3,4].\\nThistargetisrelevanttothethermalpropertiesofmaterials,andthistargetdatasetispartoftheMatBench\\nbenchmark project [3,4].\\nThetextdescriptionsweregeneratedusingtwopackages: theRobocrystallographerpackage[5]generates\\ndescriptionsofstructuralfeatureslikebondlengths,coordinationpolyhedra,orstructuretype. Ithasrecently\\nemerged as a popular tool for materials property prediction models that require text input [6–8]. Further,\\ntext descriptions of orbital-based bonding analyses containing information on covalent bond strengths or\\nantibonding states were generated using LobsterPy [9]. The data used here is available on Zenodo [10] and\\nwas generated as part of our previous study, in which the importance of such bonding information for the\\nsame target via an RF model was demonstrated [10].\\nFigure 2: Schematic depicting the prompt for fine-tuning the LLM with Alpaca prompt format.\\nInthehackathon,oneLlamamodelwasfine-tunedwiththeAlpacapromptformatusingbothRobocrys-\\ntallographer and LobsterPy text descriptions, and another one using solely Robocrystallographer input.\\nFigure2depictsthepromptusedtofine-tuneanLLMtopredictthelastphononDOSpeak. Thetrain/test/-\\nvalidation split was 0.64/0.2/0.16. The models were trained for 10 epochs with a validation step after each\\nepoch. ThetextualoutputwasconvertedbackintonumericalfrequencyvaluesforthecomputationofMAEs\\nand RMSEs. Our results show that including bonding-based information improved the model’s prediction.\\nTheresultsalsocorroborateourpreviousfindingthatquantum-chemicalbondstrengthsarerelevantforthis\\nparticular target property [10]. Both model performances (Robocrystallographer: 44 cm-1, Robocrystallog-\\nrapher+LobsterPy: 38cm-1)arecomparabletoothermodelsoftheMatBenchtestsuite,withMAEsranging\\nfrom 29 cm-1 to 68 cm-1 at time of writing [11]. However, due to the time constraints of the hackathon, no\\nfive-fold cross-validation was implemented for our model.\\nAlthoughthepreliminaryresultsseemverypromising,themodelshavenotyetbeenexhaustivelyanalyzed\\nor improved. As the prediction of a numerical value and not its text embedding is of interest to our task,\\nfurthermodeladaptationmightbebeneficial. Forexample, Rubungoetal.[7]modifiedT5[12], anencoder-\\ndecoder model, for regression tasks by removing its decoder and adding a linear layer on top of its encoder.\\nHalving the number of model parameters allowed them to fine-tune on longer input sequences, improving\\nmodel performance [7].\\nEasy-to-usepackageslikeUnsloth[13]allowedustointegrateourmaterialsdataintofine-tuninganLLM\\nfor property prediction with very limited resources and time.\\n15\\nReferences\\n[1] K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero, B. Smit, Nat Mach Intell, 2024, 6, 161–169.\\n[2] K. Choudhary, J. Phys. Chem. Lett., 2024, 6909–6917.\\n[3] G.Petretto,S.Dwaraknath,H.P.C.Miranda,D.Winston,M.Giantomassi,M.J.vanSetten,X.Gonze,\\nK. A. Persson, G. Hautier, G.-M. Rignanese, Sci Data, 2018, 5, 180065.\\n[4] A. Dunn, Q. Wang, A. Ganose, D. Dopp, A. Jain, npj Comput Mater, 2020, 6, 1–10.\\n[5] A. M. Ganose, A. Jain, MRS Communications, 2019, 9, 874–881.\\n[6] H. M. Sayeed, S. G. Baird, T. D. Sparks, 2023, DOI 10.26434/chemrxiv-2023-3q8wj.\\n[7] A. N. Rubungo, C. Arnold, B. P. Rand, A. B. Dieng, 2023, DOI 10.48550/arXiv.2310.14029.\\n[8] V. Moro, C. Loh, R. Dangovski, A. Ghorashi, A. Ma, Z. Chen, S. Kim, P. Y. Lu, T. Christensen, M.\\nSoljaˇci´c, 2024, DOI 10.48550/arXiv.2312.00111.\\n[9] A. A. Naik, K. Ueltzen, C. Ertural, A. J. Jackson, J. George, Journal of Open Source Software, 2024,\\n9, 6286.\\n[10] A. A. Naik, C. Ertural, N. Dhamrait, P. Benner, J. George, 2023, DOI 10.5281/zenodo.8091844.\\n[11] The Matbench Test Suite, Phonon dataset as per 12.07.2024, https://matbench.materialsproject.\\norg/Leaderboards.\\n[12] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, Journal of\\nMachine Learning Research, 2020, 21, 1–67.\\n[13] The Unsloth package, https://github.com/unslothai/unsloth, 2024.\\n16\\n2 Context-Enhanced Material Property Prediction (CEMPP)\\nAuthors: Federico Ottomano, Elena Patyukova, Judith Clymo, Dmytro Antypov, Chi Zhang,\\nAritra Roy, Piyush Ranjan Maharana, Weijie Zhang, Xuefeng Liu, Erik Bitzek\\n2.1 Introduction\\nThe Liverpool Materials team sought to improve composition-based property prediction models for novel\\nmaterials by providing natural language input alongside the chemical composition of interest. In doing\\nso, we leverage the ability of large language models (LLMs) to capture the broader context relevant to\\nthe composition. We demonstrate that enriching materials representation can be beneficial where training\\ndata is limited. Code to reproduce the experiments described below is available at https://github.com/\\nfedeotto/lpoolmat_llms.\\nOurexperimentsarebasedonRoost[1],adeepneuralnetworkforpredictinginorganicmaterialproperties\\nfrom their composition. Roost consists of a graph attention network that creates an embedding of the\\ninput composition (which we will enrich with context embedding), and a residual network that acts on this\\nembedding to predict the target property value (Figure 3, top).\\nFigure 3: Model architecture and the schema of the second experiment. Material composition is encoded\\nwith Roost encoder, additional information extracted from cited paper with Llama3, and encoded with\\nMat(Sci)Bert. Composition and LLM embeddings are aggregated and passed through the Residual Net\\nprojection head to predict the property. At the inference stage, the average LLM embedding from 5 nearest\\nneighbors in composition space is taken. Results show MAE for adding different types of context (top to\\nbottom): adding random context; not adding context; adding consistently structured context for chemical\\nand structural family (data extracted by humans); adding automatically extracted context for experimental\\nconditions and structure-property relationship.\\n17\\n2.2 Experiment 1: Using latent knowledge\\nWe prompt two LLMs trained on materials science literature, MatBert [2] and MatSciBert [3], to directly\\nattempt the prediction task (”What is the property of material?”). The embedding of the response and\\nRoost’scompositionembeddingareaggregated(viasummationorconcatenation)andpassedtotheresidual\\nnetwork.\\nWe consider two datasets: matbench-perovskites [4] has a target property of formation energy and Li-\\nion conductors [5] dataset has a target property of ionic conductivity. The former has low stoichiometric\\ndiversityintheinputs(allentrieshaveageneralformulaofABX3)andthelatterislimitedinsize(only403\\ndistinct compositions), making the prediction tasks especially challenging. We observe a 26-32% decrease\\nin mean absolute error (MAE) for all four settings tested (two LLMs and two aggregation schemes) in the\\nLi-ion conductors task, and a 2-4% decrease in MAE in the matbench-perovskites task.\\n2.3 Experiment 2: Using knowledge from literature\\nWe obtain information from the papers reporting materials in the Li-ion conductors dataset by using\\nPyMuPDF[6]parserandscanningforkeywordstofindrelevantsections. WepromptLlama3-8b-instruct[7]\\nto extract and summarize specific information from the text, creating an embedding from the response\\n(Figure 3).\\nGivenourgoalofpredictingpropertiesforcompositionsnotyetsynthesized,andthereforenotreferenced\\nin any academic text, for testing and inference we average the summary embeddings of the five closest\\nmaterials in the training set to the composition of interest. We define the distance between compositions\\nusing the Element Movers Distance [8].\\nWe find that providing context embeddings generated from automatically extracted experimental condi-\\ntions and structure-property relationships reduce MAE by 3–10% and 10–15% respectively. Using context\\nembeddings based on information extracted by human experts [9] from the same academic papers reduces\\nMAE by 17–21%. In contrast, using random embeddings as context increases MAE by 7–28%. We conclude\\nthat automatically extracted context can aid property prediction for new compositions. The performance\\nboostislessthanthatachievedusinghuman-extractedandconsistentlystructuredcontext,whichhighlights\\nboththefurtherpotentialofourmethodandtheharmfuleffectofnoiseintheautomaticallyextractedtext.\\nReferences\\n[1] R. Goodall, A. J. et al., “Predicting materials properties without crystal structure: deep representation\\nlearning from stoichiometry”, Nature Communications, vol. 11, no. 6280, 2020.\\n[2] A.Trewartha, et al., ’Quantifyingtheadvantageofdomain-specificpre-trainingonnamedentity recog-\\nnition tasks in materials science’, Patterns, vol. 3, no. 8, 2022.\\n[3] V. Gupta, et al., MatSciBERT: A materials domain language model for text mining and information\\nextraction, npj Computational Materials, vol. 8, no. 1, 2022.\\n[4] MatbenchPerovskitesdatasetprovidedbytheMaterialsProject,https://ml.materialsproject.org/\\nprojects/matbench_perovskites.json.gz.\\n[5] Li-ion Conductors Database, https://pcwww.liv.ac.uk/˜msd30/lmds/LiIonDatabase.html.\\n[6] PyMuPDF, https://pypi.org/project/PyMuPDF.\\n[7] AI@Meta, Llama 3 Model Card, 2024, https://github.com/meta-llama/llama3/blob/main/MODEL_\\nCARD.md.\\n[8] Hargreaves et al., The Earth Mover’s Distance as a Metric for the Space of Inorganic Compositions,\\nChem. Mater. 2020.\\n[9] Hargreaves et al., A Database of Experimentally Measured Lithium Solid Electrolyte Conductivities\\nEvaluated with Machine Learning, npj Computational Materials 2022.\\n18\\n3 MolFoundation: Benchmarking Chemistry LLMs on Predictive\\nTasks\\nAuthors: HassanHarb, XuefengLiu, AnastasiiaTsymbal, OleksandrNarykov, DanaO’Connor,\\nShagun Maheshwari, Stanley Lo, Archit Vasan, Zartashia Afzal, Kevin Shen\\n3.1 Summary\\nThe MolFoundation team is focused on enhancing the prediction capabilities of pre-trained large language\\nmodels (LLMs) such as ChemBERTa and T5-Chem for specific molecular properties, utilizing the QM9\\ndatabase. Targeting properties like dipole moment and zero-point vibrational energy (ZPVE), our approach\\ninvolves retraining the last layer of these models with a selected dataset and fine-tuning the embeddings to\\nrefine accuracy. After making predictions and evaluating performance, our results indicate that T5-Chem\\noutperformsChemBERTa. Additionally,wefoundlittledifferencebetweenfinetunedandpre-trainedresults,\\nsuggesting that the computationally expensive task of finetuning may be avoided.\\nFigure 4\\n3.2 Methods\\nThe models are downloaded from HuggingFace (ChemBERT and T5-Chem). Using the provided code we\\ncan tokenize our datasets (QM9). The datasets must contain SMILES and we checked that the tokenizer\\nhad all the necessary tokens for our datasets. To make the LLMs compatible with the regression tasks in\\nthe QM9 dataset, we froze the LLM embeddings and fine-tuned on the regression layer. Training on the full\\nLLMend-to-endisinfeasiblegivenourresources,sotrainingonasinglelinearlayerwasmuchmoreefficient.\\n3.3 Results\\nWe compare the out-of-the-box (pre-trained) and fine-tuned ChemBERT and T5-Chem to predict all the\\nmolecules’ zero-point vibrational energy (ZPVE) in the QM9 dataset. We hypothesized that the fine-tuned\\nmodels would perform better. However, across all models, there is no significant improvement in the fine-\\ntuned models as measured by R2 (Figure 6, Figure 7). We noticed that the LLMs required approximately\\n100K datapoints to show improvements in the modeling performance, indicating a saturation regime for the\\nmodels. Lastly, the T5-Chem model performs significantly better than ChemBERT.\\n19\\nFigure 5\\nFigure 6: Model comparison of the pre-trained and fine-tuned T5-Chem on zero-point vibrational energy\\n(ZPVE).\\nFigure7: Modelcomparisonofthepre-trainedandfine-tunedChemBERTaonzero-pointvibrationalenergy\\n(ZPVE).\\n3.4 Transferability + Outlook\\nFrom our experiments, it was unexpected to find that there was a lack of transfer learning from the pre-\\ntrained LLMs to the predictive tasks on the QM9 dataset. Nevertheless, we need more experimentation on\\n20\\ndifferent datasets, models, and fine-tuning strategies (i.e., end-to-end retraining, multi-task fine-tuning) to\\ndetermine the efficacy of LLMs on chemistry prediction tasks.\\nMolFoundation Github: https://github.com/shagunm1210/MolFoundation\\nReferences\\n[1] Kristiadi, A.; Strieth-Kalthoff, F.; Skreta, M.; Poupart, P.; Aspuru-Guzik, A.; Pleiss, G. A Sober Look\\nat LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?\\narXiv May 28, 2024. http://arxiv.org/abs/2402.05015 (accessed 2024-06-29).\\n21\\n4 3D Molecular Feature Vectors for Large Language Models\\nAuthors: Jan Weinreich, Ankur K. Gupta, Amirhossein D. Naghdi, Wibe A. de Jong, Alishba\\nImran\\nLink to code repo: https://github.com/janweinreich/geometric-geniuses\\nDirectlinktotutorial: https://github.com/janweinreich/geometric-geniuses/blob/main/tutorial.\\nipynb\\nAccuratechemicalpropertypredictionisacentralgoalincomputationalchemistryandmaterialsscience.\\nWhile quantum chemistry methods offer high precision, they often suffer from computational bottlenecks.\\nLarge language models (LLMs) have shown promise as a computationally efficient alternative [1]. However,\\ncommonstring-basedmolecularrepresentationslikeSMILESandSELFIES,despitetheirsuccessinLLMap-\\nplications,inherentlylack3Dgeometricinformation. Thislimitationhinderstheirapplicabilityinpredicting\\nproperties for different conformations of the same molecule - a capability essential for practical applications\\nsuch as crystal structure prediction and molecular dynamics simulations. Moreover, recent studies have\\ndemonstrated that naive encoding of geometric information as numerical data can negatively impact LLM\\nprediction accuracy [2].\\nMolecular and materials property prediction typically leverages engineered feature vectors, such as those\\nutilized in quantitative structure-activity relationship (QSAR) models. In contrast, physics-based represen-\\ntations, which center on molecular geometry due to their direct relevance to the Schr¨odinger equation, have\\ndemonstrated efficacy in various deep learning architectures [3–5]. This research investigates new strategies\\nfor encoding 3D molecular geometry for LLMs. We hypothesize that augmenting the simple SMILES rep-\\nresentation with geometric features could enable the integration of complementary information from both\\nmodalities, ultimately enhancing the predictive power of LLMs in the context of molecular properties.\\nFigure 8: Schematic representation of the training process for a regression model illustrating the novel\\nstring-basedencodingof3DmoleculargeometryforLLM-basedenergyprediction. Theworkflowinvolves(1)\\nComputationofhigh-dimensionalfeaturevectorsrepresentingthe3Dmoleculargeometryofeachconformer.\\n(2)Dimensionalityreductiontoobtainamorecompactrepresentation. (3)Conversionofthereducedvectors\\ninto a histogram, where unique string characters are assigned to each bin. (4) Input of the resulting set of\\nstrings (one per conformer) into an LLM for energy prediction.\\nTo achieve the integration of geometric information, we begin by computing a high-dimensional feature\\nvectorforeachmoleculargeometry. Whileanyrepresentationcapableoffaithfullyencodingthe3Dstructure\\nofamoleculecould,inprinciple,beutilized,thisworkspecificallyemploysapreviouslyestablishedgeometry-\\ndependent representation [6] based on many-body expansions of distances and angles. As a benchmark\\ndataset, we utilize a diverse set of ethanol and benzene structures generated using molecular dynamics\\ncalculations utilizing density functional theory (DFT) [7,8]. The target property we aim to predict is the\\nabsolute energy of each conformer. The energy scale is shifted such that the lowest energy conformer within\\nthedatasetisassignedanenergyvalueofzero. Wetransformourhigh-dimensionalgeometricfeaturevectors\\ninto string representations suitable for LLM training. First, we apply principal component analysis (PCA)\\ntoreducethedimensionalityofthevectors. Thisstepiscrucialforgeneratingcompactstringrepresentations\\nthat can be efficiently processed by LLMs. Next, we compute histograms of the reduced vectors, ensuring\\nconsistent binning intervals across all dimensions. Each bin is uniquely labeled with a distinct character,\\nand negative values are prefixed with a special character, following the approach of [9]. Finally, we count\\n22\\nthe occurrences of values within each bin, effectively converting the numerical vectors into strings. These\\nstring representations serve as the input for training a RoBERTa regression model.\\nFigure 9: Performance of an LLM in predicting total energies of benzene and ethanol structures, where the\\nmodel was trained on a large dataset of MD-generated configurations. Each point represents a different\\nmolecular structure sampled from MD simulations [7].\\nThe final step involves randomly partitioning the dataset into 80% training and 20% testing sets. The\\nstring-basedrepresentationsarethenemployedtotrainaRoBERTamodelaugmentedwitharegressionhead.\\nInFigure9,weshowcasescatterplotsillustratingthepredictedversustruetotalenergiesforbenzene(trained\\nfor4 epochs) andethanol (trainedfor20 epochs) usinga datasetof 80,000molecularconfigurations foreach\\nmolecule. Our results do not yet attain the accuracy levels of state-of-the-art equivariant neural networks\\nlike MACE, which reports a mean absolute error (MAE) of 0.009 kcal/mol for benzene [10]. Nonetheless, it\\nis important to underscore that this represents a novel capability for LLMs, which were previously unable\\nto process and predict properties of 3D molecular structures only\\ndiffering by bond rotations. This initial investigation paves the way for advancements through the\\nexploration of alternative string encoding schemes of numerical vectors in combination with larger LLMs.\\nAcknowledgements\\nA.K.G.andW.A.D.acknowledgefundingforthisprojectfromtheU.S.DepartmentofEnergy(DOE),Office\\nof Science, Office of Basic Energy Sciences, through the Rare Earth Project in the Separations Program at\\nLawrence Berkeley National Laboratory under Contract DE-AC02-05CH11231.\\nJ.W. thanks EPFL for computational resources and NCCR Catalysis (grant number 180544), a National\\nCentre of Competence in Research funded by the Swiss National Science Foundation for funding as well as\\nthe Laboratory for Computational Molecular Design.\\nReferences\\n[1] Jablonka, K. M., Ai, Q., Al-Feghali, A., Badhwar, S., Bocarsly, J. D., Bran, A. M., ... & Blaiszik, B.\\n(2023).14examplesofhowLLMscantransformmaterialsscienceandchemistry: areflectiononalarge\\nlanguage model hackathon. Digital Discovery, 2(5), 1233-1250.\\n23\\n[2] Alampara, N., Miret, S., & Jablonka, K. M. (2024). MatText: Do Language Models Need More than\\nText & Scale for Materials Modeling? arXiv preprint arXiv:2406.17295.\\n[3] Batzner, S., Musaelian, A., Sun, L., Geiger, M., Mailoa,J.P., Kornbluth, M., ...&Kozinsky, B.(2022).\\nE(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature\\nCommunications, 13(1), 2453.\\n[4] Gupta, A. K., & Raghavachari, K. (2022). Three-dimensional convolutional neural networks utilizing\\nmoleculartopologicalfeaturesforaccurateatomizationenergypredictions.JournalofChemicalTheory\\nand Computation, 18(4), 2132-2143.\\n[5] In-Context Learning of Physical Properties: Few-Shot Adaptation to Out-of-Distribution Molecular\\nGraphs. Grzegorz Kaszuba, Amirhossein D. Naghdi, Dario Massa, Stefanos Papanikolaou, Andrzej\\nJaszkiewicz, Piotr Sankowski, https://arxiv.org/abs/2406.01808.\\n[6] Khan, D., Heinen, S., &vonLilienfeld, O.A.(2023).Kernelbasedquantummachinelearningatrecord\\nrate: Many-body distribution functionals as compact representations. Journal of Chemical Physics,\\n159(034106).\\n[7] Chmiela,S.,Vassilev-Galindo,V.,Unke,O.T.,Kabylda,A.,Sauceda,H.E.,Tkatchenko,A.,&Mu¨ller,\\nK.R.(2023).Accurateglobalmachinelearningforcefieldsformoleculeswithhundredsofatoms.Science\\nAdvances, 9(2), https://doi.org/10.1126/sciadv.adf0873.\\n[8] Bowman, J. M., Qu, C., Conte, R., Nandi, A., Houston, P. L., & Yu, Q. (2022). The MD17 datasets\\nfrom the perspective of datasets for gas-phase “small” molecule potentials. The Journal of chemical\\nphysics, 156(24).\\n[9] Weinreich, J., & Probst, D. (2023). Parameter-Free Molecular Classification and Regression with Gzip.\\nChemRxiv.\\n[10] Batatia, I., Kovacs, D.P., Simm, G., Ortner, C., &Csa´nyi, G.(2022).MACE:Higherorderequivariant\\nmessage passing neural networks for fast and accurate force fields. Advances in Neural Information\\nProcessing Systems, 35, 11423-11436.\\n24\\n5 LLMSpectrometry\\nAuthors: Tyler Josephson, Fariha Agbere, Kevin Ishimwe, Colin Jones, Charishma Puli,\\nSamiha Sharlin, Hao Liu\\n5.1 Introduction\\nNuclear Magnetic Resonance (NMR) spectroscopy is a chemical characterization technique that uses oscil-\\nlating magnetic fields to characterize the structure of molecules. Different atoms in molecules resonate at\\ndifferent frequencies based on their local chemical environments. The resulting NMR spectrum can be used\\nto infer interactions between particular atoms in a molecule and determine the molecule’s entire structure.\\nSolving NMR spectral tasks is critical, as multiple aspects, such as the number, intensity, and shape of\\nsignals, as well as chemical shifts, need to be considered.\\nMachine learning tools, including the Molecular Transformer [1], have been used to learn a function to\\nmap spectrum to structure, but these require thousands to millions of labeled examples [2], far more than\\nwhat humans typically encounter when learning to assign spectra. In contrast, we recognize NMR spectral\\ntasks as being fundamentally about multi-step reasoning, and we aim to explore the reasoning capabilities\\nof large language models (LLMs) for solving this task.\\nThe project aims to investigate the capabilities of Large Language Models (LLMs), specifically GPT-4,\\nfor NMR structure determination. In particular, we were interested in evaluating whether GPT-4 could use\\nchain-of-thought reasoning [3] with a scratchpad to evaluate the components of the spectra and synthesize\\nan answer in the form of a molecule. Interpreting NMR data is crucial for organic chemists; an AI-assisted\\ntool could benefit the pharmaceutical or food industry, as well as forensic science, medicine, research, and\\nteaching.\\n5.2 Method\\nWe manually gathered 19 experimental 1H NMR data from the Spectral Database for Organic Compounds\\n(SDBS) website [4]. We then combined Python scripts, LangChain, and API calls to GPT-4 to automate\\nstructure elucidation. The components of the model are shown in Figure 10. First, NMR peak data and\\nthe chemical formula were formatted as text and inserted into a prompt. This prompt instructs the LLM to\\nreason about the data step-by-step while using a scratchpad to record its “thoughts,” then report its answer\\naccording to an output template. We then used Python to parse the output and compare the answer to the\\ntrue answer by matching with chemical names from the NIST Chemistry WebBook.\\n5.3 Results\\nOur 2024 LLM Hackathon for Applications in Materials and Chemistry (May 2024) results found GPT-4\\nsuccessful in just 3 of the 19 NMR datasets. The scratchpad reveals how GPT-4 follows the prompt and\\nsystematically approaches the problem. It analyzes the molecular formula and carefully reads peak values\\nand intensity to predict the molecule. The model correctly identified nonanal, ethanol, and acetic acid - 3\\nrelativelysimplemoleculeswithfewstructuralisomers. Incorrectanswersincludedmorecomplexmolecules,\\nwith significant branching, many functional groups, and aromatic rings, leading to more structural isomers\\nconsistent with the chemical formula.\\n5.4 Conclusions and Outlook\\nNMR spectral tasks challenge students to develop complex problem solving skills, and these also prove to\\nbe difficult in a zero-shot chain-of-thought prompting setting for GPT-4, with only 3/19 spectra solved\\ncorrectly. We noticed interesting patternsof (apparent)reasoning, and we speculate significantperformance\\nimprovements are possible. Prompting strategies can be explored more systematically (including few-shot\\nand many-shot approaches), as well as embedding the LLM’s answers inside an iterative loop, so it can\\nself-correct simple mistakes such as generating a molecule with an incorrect chemical formula.\\nSince the hackathon, we identified a better benchmark dataset: NMR-Challenge.com [3], an interactive\\nwebsitewithexercisescategorizedasEasy,Medium,andHard. Theyalsohavedataonhumanperformance,\\n25\\nFigure10: Schemeofthesystem. Dataisfirstconvertedintotextformat,withpeakpositionsandintensities\\nrepresented as (x,y) pairs. These are passed into an LLM prompt, which is tasked to use a scratchpad as it\\nreasons about the data and the formula, before providing a final answer.\\nwhich can enable comparison of GPT-4 to humans. Further analysis of proximity of incorrect answers to\\ncorrect answers would provide more granular information about the performance of the AI, for example, an\\naromatic molecule with correct substituents in incorrect locations is further from the right answer than a\\nmoleculewithincorrectsubstituents. WethinkthiscouldformausefuladditiontoexistingLLMbenchmarks,\\nfor evaluating chemistry knowledge intertwined with complex multistep reasoning.\\nDatasets, code, and results are available at: https://github.com/ATOMSLab/LLMSpectroscopy\\nReferences\\n[1] Schwaller, P., Laino, T., Gaudin, T., Bolgar, P., Hunter, C., Bekas, C., Lee, A. A., ACS Central Sci.,\\n2019, Vol. 5, No. 9, 1572-1583. https://pubs.acs.org/doi/10.1021/acscentsci.9b00576\\n[2] Alberts, M., Zipoli, F., Vaucher, A. C., ChemRxiv Preprint. https://doi.org/10.26434/\\nchemrxiv-2023-8wxcz\\n[3] Wei, J., Wang X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., Zhou, D. Chain-\\nof-Thought Prompting Elicits Reasoning in Large Language Models. NeurIPS 2022. https://arxiv.\\norg/abs/2201.11903\\n[4] Yamaji, T., Saito, T., Hayamizu, K., Yanagisawa, M., Yamamoto, O., Wasada, N., Someno, K., Kinu-\\ngasa, S., Tanabe, K., Tamura, T., Hiraishi, J., 2024. https://sdbs.db.aist.go.jp\\n[5] Socha, O., Osifova, Z., Dracinsky, M., J. Chem. Educ., 2023, Vol. 100, No. 2, 962-968. https://pubs.\\nacs.org/doi/10.1021/acs.jchemed.2c01067\\n[6] https://github.com/ATOMSLab/LLMSpectroscopy\\n26\\n6 MC-Peptide: An Agentic Workflow for Data-Driven Design of\\nMacrocyclic Peptides\\nAuthors: Andres M. Bran, Anna Borisova, Marcel M. Calderon, Mark Tropin, Rob Mills,\\nPhilippe Schwaller\\n6.1 Introduction\\nMacrocyclic peptides (MCPs) are a class of compounds composed of cyclized chains of amino acids forming\\na macrocyclic structure. They are promising for having improved binding affinity, specificity, proteolytic\\nstability, and enhanced membrane permeability compared to linear peptides [1]. Their unique properties\\nmake them highly suitable for drug development, enabling the creation of therapeutics that can address\\ncurrentlyunmetmedicalneeds[1]. Indeed,ithasbeenshownthatMCPsofupto12aminoacidsshowgreat\\npromise for permeating cellular membranes [2]. Despite the more constrained chemical space offered by this\\nclassofmolecules,theirdesignremainsachallengingissueduetothevastspaceofaminoacidcombinations.\\nOneimportantparameterofMCPsispermeability,whichdetermineshowwellthestructurecanpermeate\\ninto cells, making it a relevant factor in assessing the MCP’s pharmacological activity. However, data for\\nMCPs and their permeabilities is scattered across scientific papers that report data without consensus on\\nreporting form, making it challenging for systems to compile and use this raw data.\\nHere, we introduce MC-Peptide, an LLM-based agentic workflow created for tackling this issue in an\\nend-to-endfashion. AsshowninFigure1,MC-Peptide’smaingoalistoproducesuggestionsofnovelMCPs,\\nfollowing from a reasoning process involving: (i) understanding of the design objective, (ii) gathering of\\nrelevant scientific information, (iii) data extraction from papers, and (iv) inference based on the extracted\\ninformation. MC-Peptide leverages advances in LLMs such as semantic search, grammar-constrained gen-\\neration, and agentic architectures, ultimately yielding candidate MCP variants with enhanced predicted\\npermeability in comparison to reference designs.\\n6.2 Methods\\nWe implemented the basic building blocks for the pipeline described in Figure 11, with the most important\\ncomponents being document retrieval, structured data extraction, and in-context learning for design.\\nDocument Retrieval Animportantpartofthispipelineistheretrievalofrelevantdocuments. Asshown\\nin Figure 11, the pipeline developed here does not rely on a stand-alone predictive system, but rather\\naims to leverage existing knowledge in the scientific literature, in an on-demand fashion. This is known\\nas retrieval-augmented generation (RAG) [8], and one of its key components is a relevance-based retrieval\\nsystem. By integrating the Semantic Scholar API, a service that provides access to AI-based search of\\nscientific documents, the pipeline is able to create and refine a structured knowledge base from papers.\\nStructured Data Extraction To employ the unstructured data from the previous step, a retrieval-\\naugmentedsystemwasdesignedthatretrievessectionsofpapersandusesthemascontextforthegeneration\\nof structured data objects with predefined grammars, which can be used to constrain the decoding of LLMs\\n[3], ensuring that the resulting output follows the same structure and format. This technique also mitigates\\nhallucination,asitpreventstheLLMfromgeneratingunnecessaryandpotentiallymisleadinginformation[9].\\nIn-Context Learning The extracted data is then leveraged through the in-context learning capabilities\\nof LLMs [4], allowing the models to learn from few data points given as context in the prompt and generate\\noutput based on that. This capability has been extensively explored elsewhere [5]. Here we show that, for\\nthespecifictaskofmodifyingMCPstoimprovepermeability,LLMsperformwell,asassessedbyasurrogate\\nrandom forest model.\\n27\\n6.3 Conclusions\\nWe present MC-Peptide, a novel agentic workflow for designing macrocyclic peptides. The system is built\\nfrom a few main components: data collection, peptide extraction, and peptide generation. We evaluate the\\npeptide permeability of newly generated peptides with respect to the initial structures found in reference\\narticles.\\nThe resulting system shows that LLMs can be successfully leveraged for automating multiple aspects of\\npeptide design, yielding an end-to-end generative tool that researchers can utilize to accelerate and enhance\\ntheir experimental workflows. Furthermore, this workflow can be extended by adding more specialized\\nmodules (e.g., for increasing the diversity of information sources). The modular design of MC-Peptide\\nensures extendability to more design objectives and input sources, as well as other domains where data is\\nreported in an unstructured fashion in papers, such as materials science and organic chemistry.\\nThe code for this project has been made available at: https://github.com/doncamilom/mc-peptide.\\nFigure11: MC-Peptide: Pipelineimplementedinthiswork. Theexampleillustratesauserrequest,followed\\nbyretrievalfromtheSemanticScholarAPI,andthecreationofaknowledgebase. MCPsandpermeabilities\\nare extracted from [7]. The pipeline finishes with an LLM proposing modifications to an MCP, increasing\\nits permeability.\\nReferences\\n[1] Ji, X., Nielsen, A. L., Heinis, C., ”Cyclic peptides for drug development,” Angewandte Chemie Interna-\\ntional Edition, 2024, 63(3), e202308251.\\n[2] Merz, M.L., Habeshian, S., Li, B. et al., ”De novo development of small cyclic peptides that are orally\\nbioavailable,” Nat Chem Biol, 2024, 20, 624–633. https://doi.org/10.1038/s41589-023-01496-y.\\n[3] Beurer-Kellner,L.,etal.,”GuidingLLMsTheRightWay: Fast,Non-InvasiveConstrainedGeneration,”\\nArXiv, 2024, abs/2403.06988.\\n[4] Dong, Q., et al., ”A survey on in-context learning,” ArXiv, 2022, arXiv:2301.00234.\\n[5] Agarwal, R., et al., ”Many-shot in-context learning,” ArXiv, 2024, arXiv:2404.11018.\\n28\\n[6] Kristiadi, A., et al., ”A Sober Look at LLMs for Material Discovery: Are They Actually Good for\\nBayesian Optimization Over Molecules?,” ArXiv, 2024, arXiv:2402.05015.\\n[7] Lewis,I.,Schaefer,M.,Wagner,T.etal.,”ADetailedInvestigationonConformation,Permeabilityand\\nPK Properties of Two Related Cyclohexapeptides,” Int J Pept Res Ther, 2015, 21, 205–221. https:\\n//doi.org/10.1007/s10989-014-9447-3.\\n[8] Lewis, P., et al., ”Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” ArXiv, 2020,\\nabs/2005.11401.\\n[9] B´echard, P., Ayala, O. M., ”Reducing hallucination in structured outputs via Retrieval-Augmented\\nGeneration,” ArXiv, 2024, abs/2404.08189.\\n29\\n7 Leveraging AI Agents for Designing Low Band Gap Metal-\\nOrganic Frameworks\\nAuthors: Sartaaj Khan, Mahyar Rajabi, Amro Aswad, Seyed Mohamad Moosavi, Mehrad\\nAnsari\\n7.1 Introduction\\nMetal-organic frameworks (MOFs) are known to be excellent candidates for electrocatalysis due to their\\nlarge surface area, high adsorption capacity at low CO concentrations, and the ability to fine-tune the\\n2\\nspatial arrangement of active sites within their crystalline structure [1]. Low band gap MOFs are crucial\\nas they efficiently absorb visible light and exhibit higher electrical conductivity, making them suitable for\\nphotocatalysis,solarenergyconversion,sensors,andoptoelectronics. Inthiswork,weaimatusingchemistry-\\ninformed ReAct [2] AI Agents to optimize the band gap property of MOFs. The overview of the workflow\\nis presented in Figure 12a. The agent inputs a textual representation of the initial MOF structure as a\\nSMILES (Simplified Molecular Input Line-Entry System) string representation, and a short description of\\nthe property optimization task (i.e., reducing band gap), all in natural language text. This is followed by an\\niterativeclosed-loopsuggestionofnewMOFcandidateswithalowerbandgapwithuncertaintyassessment,\\nby making adjustments to the initial MOF given a set of design guidelines automatically obtained from the\\nscientific literature. Detailed analysis of this methodology applied to other materials and target properties\\ncan be found in reference [3].\\n7.2 Agent and Tools\\nThe agent, powered by a large language model (LLM), is augmented with a set of tools allowing for a more\\nchemistry-informed decision-making. These tools are as follows:\\n1. Retrieval-Augmented Generation (RAG): This tool allows the agent to obtain design guidelines\\non how to adapt the MOF structure from unstructured text. In specific, the agent has access to a\\nfixed set of seven MOF research papers (see Refs. [4]- [10]) as PDFs. This tool is designed to extract\\nthe most relevant sentences from papers in response to a given query. It works by embedding both the\\npaper and the query into numerical vectors, then identifying the top k passages within the document\\nthat either explicitly mention or implicitly suggest the adaptations to the band gap property for a\\nMOF. The embedding model is OpenAI’s text-ada-002 [11]. Inspired by our earlier work [12], k is\\nset to 9 but is dynamically adjusted based on the relevant context’s length to avoid OpenAI’s token\\nlimitation error.\\n2. Surrogate Band Gap Predictor: Thesurrogatemodelusedisatransformer(MOFormer[13])that\\ninputstheMOFasSMILES.Thismodelispre-trainedusingaself-supervisedlearningtechniqueknown\\nas Barlow-Twin [14], where representation learning is done against structure-based embeddings from\\na crystal graph convolutional neural network (CGCNN) [15]. This was done against 16,000 BW20K\\nentries[16]. Thepre-trainedweightsarethentransferredandfine-tunedtopredictthebandgaplabels\\ntaken from 7450 entries from the QMOF database [17]. From a 5-fold training, an ensemble of five\\ntransformers are trained to return the mean band gap and the standard deviation, which is used to\\nassess uncertainty for predictions. For comparison, our transformer’s mean absolute error (MAE) is\\napproximately 0.467, whereas MOFormer, which was pre-trained on 400,000 entries, achieves an MAE\\nof approximately 0.387.\\n3. Chemical Feasibility Evaluator: This tool primarily uses RDKit [18] to convert a SMILES string\\nintoanRDKitMolobject,andperformsseveralvalidationstepstoensurechemicalfeasibility. First,it\\nparses the SMILES string to confirm correct syntax. Next, it validates the atoms and bonds, ensuring\\nthey are chemically valid and recognized. It then checks atomic valences to ensure each atom forms\\na reasonable number of bonds. For ring structures, RDKit verifies the correct ring closure notation.\\nAdditionally, it adds implicit hydrogens to satisfy valence requirements and detects aromatic systems,\\n30\\nmarking relevant atoms and bonds as aromatic. These steps collectively ensure the molecule’s basic\\nchemical validity.\\nWeuseOpenAI’sGPT-4[19]withatemperatureof0.1asourLLMandLangChain[20]fortheapplication\\nframeworkdevelopment(notethechoiceofLLMisonlyahyperparameterandotherLLMscanbealsoused\\nwith the agent).\\nFigure 12: a) Workflow overview. The ReAct agent looks up guidelines for designing low band gap MOFs\\nfrom research papers and suggests a new MOF (likely with lower band gap). It then checks validity of the\\nnew SMILES candidate and predicts band gap with uncertainty estimation using an ensemble of surrogate\\nfine-tuned MOFormers. b) Band gap predictions for new MOF candidates as a function of agent iterations.\\nDetailed analysis of this methodology applied to other materials and target properties can be found in\\nreference [3].\\nThe new MOF candidates and their corresponding inferred band gap are represented in Figure 1.b. The\\nagent starts by retrieving the following design guidelines for low band gap MOFs from research papers: 1)\\nIncreasingtheconjugationinthelinker. 2)Selectingelectron-richmetalnodes. 3)Functionalizingthelinker\\nwith nitro and amino groups. 4) Altering linker length. 5) Substitute functional groups (i.e., substituting\\nhydrogen with electron-donating groups on the organic linker). Note that the metal node adaptations\\nwere restrained by simply changing the system input prompt. The agent iteratively implements the above\\nstrategiesandmakeschangestotheMOF.Aftereachmodification,thebandgapofthenewMOFisassessed\\nusingthefine-tunedsurrogateMOFormerstoensurealowerbandgap. Subsequently,thechemicalfeasibility\\nisevaluated. IfthenewMOFcandidatehasaninvalidSMILESstringorahigherbandgap,theagentreverts\\nto the most recent valid MOF candidate with the lowest band gap.\\n31\\n7.3 Data and Code Availability\\nAll code and data used to produce results in this study are publicly available in the following GitHub\\nrepository: https://github.com/mehradans92/PoreVoyant.\\nReferences\\n[1] Lirong Li, Han Sol Jung, Jae Won Lee, and Yong Tae Kang. Review on applications of metal–organic\\nframeworks for co2 capture and the performance enhancement mechanisms. Renewable and Sustainable\\nEnergy Reviews, 162: 112441, 2022.\\n[2] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. arXiv preprint, arXiv:2210.03629, 2022.\\n[3] Mehrad Ansari, Jeffrey Watchorn, Carla E. Brown, and Joseph S. Brown. dZiner: Rational Inverse\\nDesign of Materials with AI Agents. arXiv, 2410.03963, 2024. URL: https://arxiv.org/abs/2410.03963.\\n[4] MuhammadUsman,ShrutiMendiratta,andKuang-LiehLu.Semiconductormetal–organicframeworks:\\nfuture low- g”bandgap materials. Advanced Materials, 29(6):1605071, 2017.\\n[5] Espen Flage-Larsen, Arne Røyset, Jasmina Hafizovic Cavka, and Knut Thorshaug. Band gap modu-\\nlations in uio metal–organic frameworks. The Journal of Physical Chemistry C, 117(40):20610–20616,\\n2013.\\n[6] Li-Ming Yang, Guo-Yong Fang, Jing Ma, Eric Ganz, and Sang Soo Han. Band gap engineering of\\nparadigm mof-5. Crystal growth & design, 14(5):2532–2541, 2014.\\n[7] Li-Ming Yang, Ponniah Vajeeston, Ponniah Ravindran, Helmer Fjellvag, and Mats Tilset. Theoretical\\ninvestigationsonthechemicalbonding,electronicstructure,andopticalpropertiesofthemetal-organic\\nframework mof-5. Inorganic chemistry, 49(22):10283–10290, 2010.\\n[8] Maryum Ali, Erum Pervaiz, Tayyaba Noor, Osama Rabi, Rubab Zahra, and Minghui Yang. Recent\\nadvancementsinmof-basedcatalystsforapplicationsinelectrochemicalandphotoelectrochemicalwater\\nsplitting: A review. International Journal of Energy Research, 45(2):1190–1226, 2021.\\n[9] Yabin Yan, Chunyu Wang, Zhengqing Cai, Xiaoyuan Wang, and Fuzhen Xuan. Tuning electrical and\\nmechanical properties of metal–organic frameworks by metal substitution. ACS Applied Materials &\\nInterfaces, 15(36):42845–42853, 2023.\\n[10] Chi-KaiLin, DanZhao, Wen-YangGao, ZhenzhenYang, JingyunYe, TaoXu, QingfengGe, Shengqian\\nMa, and Di-Jia Liu. Tunability of band gaps in metal–organic frameworks. Inorganic chemistry,\\n51(16):9039–9044, 2012.\\n[11] RyanGreene,TedSanders,LilianWeng,andArvindNeelakantan.Newandimprovedembeddingmodel,\\n2022.\\n[12] MehradAnsariandSeyedMohamadMoosavi.Agent-basedlearningofmaterialsdatasetsfromscientific\\nliterature. arXiv preprint, arXiv:2312.11690, 2023.\\n[13] Zhonglin Cao, Rishikesh Magar, Yuyang Wang, and Amir Barati Farimani. Moformer: self-supervised\\ntransformermodelformetal–organicframeworkpropertyprediction.Journal of the American Chemical\\nSociety, 145(5): 2958–2967, 2023.\\n[14] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-\\nsupervisedlearningviaredundancyreduction.InInternational Conference on Machine Learning, pages\\n12310–12320. PMLR, 2021.\\n[15] Tian Xie and Jeffrey C. Grossman. Crystal graph convolutional neural networks for an accurate and\\ninterpretable prediction of material properties. Physical Review Letters, 120(14):145301, 2018.\\n32\\n[16] Seyed Mohamad Moosavi, Aditya Nandy, Kevin Maik Jablonka, Daniele Ongari, Jon Paul Janet, Peter\\nG Boyd, Yongjin Lee, Berend Smit, and Heather J Kulik. Understanding the diversity of the metal-\\norganic framework ecosystem. Nature communications, 11(1):1–10, 2020.\\n[17] AndrewS.Rosen,ShaelynM.Iyer,DebmalyaRay,ZhenpengYao,AlanAspuru-Guzik,LauraGagliardi,\\nJustin M. Notestein, and Randall Q. Snurr. Machine learning the quantum-chemical properties of\\nmetal–organic frameworks for accelerated materials discovery. Matter, 4(5):1578–1597, 2021.\\n[18] Greg Landrum. RDKit documentation. Release, 1(1-79):4, 2013.\\n[19] OpenAI. Gpt-4 technical report, 2023.\\n[20] Harrison Chase. LangChain, 10 2022. URL https://github.com/langchain-ai/langchain.\\n33\\n8 How Low Can You Go? Leveraging Small LLMs for Material\\nDesign\\nAuthors: Alessandro Canalicchio, Alexander Moßhammer, Tehseen Rug, Christoph V¨olker\\n8.1 Motivation - Leveraging LLMs for Sustainable Construction Material De-\\nsign\\nThe construction industry’s dependence on limited resources and the high emissions associated with tradi-\\ntionalbuildingmaterialssuchasPortlandcement-basedconcretenecessitateatransitiontomoresustainable\\nalternatives [1]. Geopolymers synthesized from industrial by-products such as fly ash and waste slag repre-\\nsentapromisingsolution[2–4]. However, scalingupthesehighlycomplexmaterialstomeetmarketdemand\\nis a major challenge due to the smaller volumes available at diverse sources. Traditionally, it takes years of\\nintensive scientific research to bring a new material to market. To meet the demand for sustainable alter-\\nnative materials, new resource streams need to be developed much more frequently, making the traditional\\nlengthy process of acquiring specialized material knowledge impractical.\\nThis is where LLMs offer a solution. LLMs are trained on web-scale data and contain deep domain ex-\\npertisethatwaspreviouslyhiddeninscientificinstitutionsbutisnowaccessibletoeveryone. Thisknowledge\\nis stored in the LLMs inner representations (hidden states) and is accessible through prompts making best\\npractices from specialized domains easily available. In addition, LLMs can provide information in any for-\\nmat, making it easy to convert complex scientific concepts into workable formulations for direct application\\nin the lab.\\nPrevious systematic benchmarks have shown that LLMs can keep up with conventional approaches such\\nas Bayesian Optimization (BO) and Sequential Learning (SL) when it comes to designing sustainable alter-\\nnativestoconcrete,so-calledalkali-activatedconcrete[5]. Whatsetsthesemodelsapartfromclassicaldesign\\nmethods is their ability to produce zero-shot designs, meaning they can propose relatively well-functioning\\nformulations without any initial training data. This capability suggests that LLMs can play a crucial role in\\nthe initial data collection phase. Essentially, it allows researchers entering a new field to begin new projects\\nat an expert level from the outset, leading to much faster development of viable solutions.\\n8.2 Research Gap\\nThe emergence of a new generation of high-performing, smaller-scale LLMs is expanding the boundaries\\nof feasible application scenarios. Deploying these compact LLMs enables applications in sensitive areas\\nlike R&D. Full control over a local model enhances quality control and repeatability by allowing precise\\nmanagement of versions and system prompts. Additionally, they increase security and data privacy through\\nin-houseprocessing,whichreducestheriskofconfidentialdataleakageandothersecuritythreats. Moreover,\\nsmall-scale LLMs improve cost efficiency and reliability by minimizing network dependency and operational\\ncosts. This development raises an intriguing research question:\\nCancurrentsmallLLMsretrieveandutilizevaluableknowledgefromtheirinnerrepresentations,making\\nthem effective tools for closed lab networks in material science?\\nInvestigating this could significantly advance LLM adoption in material science R&D, especially in sen-\\nsitive industry settings, potentially transforming experimental workflows and accelerating innovation.\\n8.3 Methodology\\nToinvestigatethisquestion,wedeployedtwosmall-scaleinstructionLLMsonconsumerhardware,specifically\\nLlama 3 8B and Phi-3 3.8B, both quantized to four bits. Our goal was to evaluate their ability to design\\nalkali-activated concrete with high compressive strength. The LLMs were tasked via system messages to\\ndetermine four mixture parameters from a predetermined grid of 240 recipes: 1) the blending ratio of fly\\nashtogroundgranulatedblastfurnaceslag, 2)thewater-to-powderratio, 3)thepowdercontent, and4)the\\ncuring method. We measured their performance by comparing the compressive strengths of the suggested\\nformulations to previously published lab results [6].\\n34\\nThe workflow and evaluation are shown in Figure 13. Each design run comprised 10 consecutive devel-\\nopment cycles, where the LLM suggested a formulation and the user provided feedback. This process was\\nrepeatedfivetimesforstatisticalanalysis. Additionally,thepromptwasrephrasedsynonymouslythreetimes\\ntoincreaselinguisticvariability. Twotypesofcontextswereprovided: onewithdetaileddesigninstructions,\\nsuch as “reducing the water-to-powder ratio leads to higher strengths,” and one without additional instruc-\\ntions. In total, 15 design runs were conducted per model and per context. Finally, we assessed the achieved\\n10% lower-bound strength, defined as the strength achieved in 90% of the design runs, and compared this\\nagainst a random draw as the statistical baseline and SL.\\n8.4 Results and Conclusion\\nTheresults,summarizedinTable2,showedthatallinvestigatedmodelsgenerateddesignsthatoutperformed\\nthe statistical baseline of 28 MPa in the first round and 52 MPa in the final round, demonstrating the\\nsurprising effectiveness of small-scale LLMs in materials design. An exception was noted with the Phi-3\\nmodel when provided with extensive design context: it produced the same output in each round and failed\\nto produce viable solutions completely after the fifth development cycle, indicating its failure to understand\\nthe task. As expected, Llama 3 outperformed the smaller Phi-3 model. Specifically, Llama 3 benefited from\\nadditional design context, showing an improvement of more than 10 MPa in the initial rounds.\\nTable 2: Achieved compressive strength of designs suggested by LLMs. Statistical assessment in terms of\\n10% lower bound strength, i.e., the 10% worst cases.\\nModel Development Cycle 1 Development cycle 10\\nRandom Baseline 28 MPa 52 MPa\\nPhi 3 (No design rules in context 50 MPa 58 MPa\\nPhi 3 (Design rules in context) 51 MPa –\\nLlama 3 (No design rules in context) 48 MPa 60 MPa\\nLlama 3 (Design rules in context) 59 MPa 60 MPa\\nIn conclusion, small-scale LLMs performed surprisingly well, with Phi-3 producing results significantly\\nabove a random guess, though it faced challenges with more complex prompts. The effectiveness of LLMs\\nin solving design tasks depends on how well material concepts are represented in their hidden states and\\nhow effectively these can be retrieved via prompts, giving larger models an advantage. Despite their smaller\\nparameter count and less training data, Phi-3 and Llama 3 demonstrated common sense for domain-specific\\ndesign tasks, making local deployment a viable option. While 100% reliability in retrieving sensible infor-\\nmation via LLMs is uncertain, small-scale LLMs can generate educated guesses that potentially accelerate\\nthe familiarization process with novel materials.\\n8.5 Code\\nThecodeusedtoconducttheexperimentsisopen-sourceandavailablehere: https://github.com/sandrocan/\\nLLMs-for-design-of-alkali-activated-concrete-formulations.\\nReferences\\n[1] U. Environment, K. L. Scrivener, V. M. John and E. M. Gartner, ”Eco-efficient cements: Potential\\neconomically viable solutions for a low-CO2 cement-based materials industry,” Cement and Concrete\\nResearch, vol. 114, pp. 2-26; DOI: https://doi.org/10.1016/j.cemconres.2018.03.015, 2018.\\n[2] J. L. Provis, A. Palomo and C. Shi, ”Advances in understanding alkali-activated materials,” Cement\\nand Concrete Research, pp. 110-125, 2015.\\n35\\nFigure 13: LLM-based material design workflow (left) and diagram showing evaluation metric (right).\\n[3] H. S. G¨okc¸e, M. Tuyan, K. Ramyar and M. L. Nehdi, ”Development of Eco-Efficient Fly Ash–Based\\nAlkali-Activated and Geopolymer Composites with Reduced Alkaline Activator Dosage,” Journal of\\nMaterials in Civil Engineering, vol. 32, no. 2, pp. 04019350; DOI: 10.1061/(ASCE)MT.1943-5533.\\n0003017, 2020.\\n[4] J. He, Y. Jie, J. Zhang, Y. Yu and G. Zhang, ”Synthesis and characterization of red mud and rice\\nhusk ash-based geopolymer composites,” Cement and Concrete Composites, vol. 37, pp. 108-118; DOI:\\nhttp://dx.doi.org/10.1016/j.cemconcomp.2012.11.010, 2013.\\n[5] C. V¨olker, T. Rug, K. M. Jablonbka and S. Kurschwitz, ”LLMs can Design Sustainable Concrete – a\\nSystematic Benchmark,” (Preprint), pp. 1-12; DOI: 10.21203/rs.3.rs-3913272/v1, 2023.\\n[6] G. M. Rao and T. D. G. Rao, ”A quantitative method of approach in designing the mix proportions of\\nfly ash and GGBS-based geopolymer concrete,” Australian Journal of Civil Engineering, vol. 16, no. 1,\\npp. 53-63; DOI: 10.1080/14488353.2018.1450716, 2018.\\n36\\n9 LangSim\\nAuthors: Yuan Chiang, Giuseppe Fisicaro, Greg Juhasz, Sarom Leang, Bernadette Mohr,\\nUtkarsh Pratiush, Francesco Ricci, Leopold Talirz, Pablo A. Unzueta, Trung Vo, Gabriel\\nVogel, Sebastian Pagel, Jan Janssen\\nThe complexity and non-intuitive user interface of scientific simulation software results in a high barrier\\nfor beginners and limits the usage to expert users. In the field of atomistic simulation, the simulation\\ncodes are developed by different communities (chemistry, physics, materials science) using different units,\\nfile names,and variable names. LangSim addresses this challenge, by providing a natural language interface\\nfor atomistic simulation in the field of computational materials science.\\nSince the introduction of ChatGPT, the application of large language models (LLM) in chemistry and\\nmaterialssciencehastransitionedfromsemanticliteraturesearchtoresearchagentscapableofautonomously\\nexecuting selected steps of the research process. In particular, in research domains with a high level of au-\\ntomation,likechemicalsynthesis,thelatestresearchagentsalreadycombineaccesstospecializeddatabases,\\nscientificsoftwareforanalysisaswellastorobotsforexecutingtheexperimentalmeasurements[1,2]. These\\nresearchagentsdividetheresearchquestionintoaseriesofindividualtasks, eachaddressingonetaskbefore\\ncombiningthemwithonecontrollingagent. Withthisapproach,theflexibilityoftheLLMisreduced,which\\nconsequently reduces the risk of hallucinations [8].\\nIn analogy, LangSim (Language + Simulation) is a research agent focused on simulation in the field of\\ncomputationalmaterialsscience. LangSimcancalculateaseriesofbulkpropertiesforelementalcrystals,like\\ntheequilibriumvolumeandequilibriumbulkmodulus. Internally,thisisachievedbyconstructingsimulation\\nprotocols consisting of multiple simulation steps to calculate one material property. For example, the bulk\\nmodulusiscalculatedbyqueryingtheatomisticsimulationenvironment(ASE)[3]fortheequilibriumcrystal\\nstructure, optimizing the crystal structure in dependence on the choice of simulation model, and finally\\nevaluating the change of energy over volume change around the equilibrium to calculate the bulk modulus\\nas the second derivative of the change in energy over volume change. The simulation protocols in LangSim\\nare independent of the selected level of theory and can be evaluated with either the effective medium theory\\nmodel [4] or the foundation machine-learned interatomic potential MACE [5]. Furthermore, to quantify the\\nuncertainty of these simulation results, LangSim also has access to databases with experimental references\\nfor these bulk properties.\\nFigure 14\\nThe LangSim research agent is based on the LangChain package. This has two advantages: On the one\\nhand, the LangChain package [6] simplifies the addition of new simulation agents and simulation workflows\\nfor LangSim. On the other hand, LangChain is compatible with a wide range of different LLM providers to\\nprevent vendor lock-in. LangSim extends the LangChain framework by providing data types for coupling\\nthesimulationcodeswiththeLLM,likeapydanticdataclass[7]representationoftheASEatomsclassanda\\nseriesofpre-definedsimulationworkflowstohighlighthowexistingsimulationworkflowscanbeimplemented\\n37\\nas LangChain agents. Once a simulation workflow is represented as a Python function compatible with a\\nsimulation framework like ASE, the interfacing with LangSim is as simple as changing the input arguments\\nto LLM-compatible data types indicated by type hints and adding a Docstring as context for the LLM.\\nAbstractly, these LangChain agents can be understood in analogy to the header files in C programming,\\nwhich define the interfaces for public functions. An example LangSim agent to calculate the bulk modulus\\nis provided below:\\nfrom ase.atoms import Atoms\\nfrom ase. calc import Calculator\\nfrom ase.eos import calculate eos\\nfrom ase.units import kJ\\nfrom langsim import (\\nAtomsDataClass,\\nget ase calculator from str ,\\n)\\nfrom langchain.agents import tool\\ndef get bulk modulus function(\\natoms: Atoms, calculator : Calculator\\n) −> float :\\natoms. calc = calculator\\neos = calculate eos(atoms)\\nv, e, B = eos. fit ()\\nreturn B / kJ ∗ 1.0e24\\n@tool\\ndef get bulk modulus agent(\\natom dict: AtomsDataClass, calculator str : str\\n) −> float :\\n”””\\nReturns the bulk modulus of chemical symbol\\nfor a given atoms dictionary and a selected\\nmodel specified by the calculator string in GPa.\\n”””\\nreturn get bulk modulus function(\\natoms=Atoms(∗∗atom dict. dict ()) ,\\ncalculator=get ase calculator from str(\\ncalculator str=calculator str\\n),\\n)\\nThe example workflow for calculating the bulk modulus highlights how existing simulation frameworks\\nlikeASEcanbeleveragedtoprovidetheLLMwiththeabilitytoconstructandexecutesimulationworkflows.\\nWhileforthisexample,inparticularforthecaseofelementalcrystals,itwouldbepossibletopre-computeall\\ncombinationsandrestrictthelargelanguagemodeltoadatabaseofpre-computedresults,thiswouldbecome\\nprohibitive for multi-component alloys and an increasing number of simulation models and workflows. At\\nthis stage the semantic capabilities of the LLM provide the capability to handle all possible combinations in\\na systematic way by allowing the LLM to construct and execute the specific simulation workflow when it is\\nrequested from the user.\\nIn summary, the LangSim package provides data classes and utility functions to interface LLMs with\\natomistic simulation workflows. This provides the LLM with the ability to execute simulations to answer\\nscientific questions like a computational material scientist. The functionality is demonstrated for the calcu-\\nlation of the bulk modulus for elemental crystals.\\n38\\n9.1 One sentence summaries\\n1. Problem/Task: Developanaturallanguageinterfaceforsimulationcodesinthefieldofcomputational\\nchemistry and materials science. Current LLMs, including ChatGPT 4, suffer from hallucination,\\nresultinginsimulationprotocolsthatcanbeexecutedbutfailtocalculatethecorrectphysicalproperty\\nwith the specified unit.\\n2. Approach: Develop a suite of LangChain agents to interface with the atomic simulation environment\\n(ASE) and corresponding data classes to represent objects used in atomistic simulation in the context\\nof large language models.\\n3. Results and Impact: Developed the LangSim package as a prototype for handling the calculation\\nof multiple material properties using predefined simulation workflows, independent of the theoretical\\nmodel, based on the ASE framework.\\n4. Challenges and Future Work: The current prototype enables the calculation of bulk properties for\\nunaries, the next step is to extend this functionality to multi-component alloys and more material\\nproperties.\\nReferences\\n[1] Boiko,D.A.,MacKnight,R.,Kline,B.etal.Autonomouschemicalresearchwithlargelanguagemodels.\\nNature 624, 570–578 (2023). https://doi.org/10.1038/s41586-023-06792-0.\\n[2] M. Bran, A., Cox, S., Schilter, O. et al. Augmenting large language models with chemistry tools. Nat\\nMach Intell 6, 525–535 (2024). https://doi.org/10.1038/s42256-024-00832-8.\\n[3] HjorthLarsen,A.,JørgenMortensen,J.,Blomqvist,J.,Castelli,I.E.,Christensen,R.,Dul ak,M.,Friis,\\nJ.,Groves,M.N.,Hammer,B.,Hargus,C.,Hermes,E.D.,Jennings,P.C.,BjerreJensen,P.,Kermode,\\nJ., Kitchin, J. R., Leonhard Kolsbjerg, E., Kubal, J., Kaasbjerg, K., Lysgaard, S., ... Jacobsen, K.\\nW. (2017). The atomic simulation environment—a python library for working with atoms. Journal of\\nPhysics: Condensed Matter, 29(27), 273002. https://doi.org/10.1088/1361-648x/aa680e.\\n[4] Jacobsen,K.W.,Stoltze,P.,Nørskov,J.K.(1996).Asemi-empiricaleffectivemediumtheoryformetals\\nand alloys. Surface Science, 366(2), 394–402. https://doi.org/10.1016/0039-6028(96)00816-3.\\n[5] Batatia, I., Benner, P., Chiang, Y., Elena, A. M., Kov´acs, D. P., Riebesell, J., Advincula, X. R., Asta,\\nM., Avaylon, M., Baldwin, W. J., Berger, F., Bernstein, N., Bhowmik, A., Blau, S. M., C˘arare, V.,\\nDarby, J. P., De, S., Della Pia, F., Deringer, V. L. et al. (2024). A foundation model for atomistic\\nmaterials chemistry. arXiv. https://arxiv.org/abs/2401.00096.\\n[6] https://github.com/langchain-ai/langchain.\\n[7] https://pydantic.dev/.\\n[8] Ye, H., Liu, T., Zhang, A., Hua, W., Jia, W. (2023). Cognitive mirage: A review of hallucinations in\\nlarge language models. arXiv. https://arxiv.org/abs/2309.06794.\\n39\\n10 LLMicroscopilot: assisting microscope operations through LLMs\\nAuthors: Marcel Schloz, Jose C. Gonzalez\\nThe operation of state-of-the-art microscopes in materials science research is often limited to a selected\\ngroup of operators due to their high complexity and significant cost of ownership. This exclusivity creates a\\nbarrier to broadening scientific progress and democratizing access to these powerful instruments. Presently,\\noperating these microscopes involves time-consuming tasks that demand substantial human expertise, such\\nas aligning the instrument for optimal performance and transitioning between different operational modes\\ntoaddressdifferentresearchquestions. Thesechallengeshighlighttheneedforimproveduserinterfacesthat\\nsimplify operation and increase the accessibility of microscopes in materials science.\\nRecent advancements in natural language processing software suggest that integrating large language\\nmodels (LLMs) into the user experience of modern microscopes could significantly enhance their usability.\\nJust as modern chatbots have enabled users without much programming background to create complex\\ncomputer programs, LLMs have the potential to simplify the operation of microscopes, thereby making\\nthem more accessible to non-expert users [1]. Early studies have demonstrated the potential of LLMs in\\nscanning probe microscopy, using microscope-specific external tools for remote access [2] and control [3].\\nParticularly promising is the application of LLMs as agents with access to specific external tools, providing\\noperators with a powerful assistant capable of reasoning based on observations and reducing the extensive\\nhallucinations common in LLM agents. This approach also enhances the accessibility of external tools,\\neliminating the need for users to learn tool-specific APIs.\\nThe LLMicroscopilot-team (Jose D. Cojal Gonzalez and Marcel Schloz) has shown that the operation of\\na scanning transmission electron microscope can be partially performed by the LLM-powered agent ”LLMi-\\ncroscopilot” through access to microscope-specific control tools. Figure 15 illustrates the interaction process\\nbetween the operator and the LLMicroscopilot. The LLMicroscopilot is built on a generally trained founda-\\ntion model that gains domain-specific knowledge and performance through the provided tools. The initial\\nprototypeusestheAPIofamicroscopeexperimentsimulationtool[4]toperformtaskssuchasexperimental\\nparameterestimationandexperimentexecution. Thisapproachreducestherelianceonhighlytrainedhuman\\noperators, fostering broader participation in materials science research. Future developments of LLMicro-\\nscopilot will integrate open-source microscope hardware control tools [5] and database-access tools, allowing\\nfor Retrieval-Augmented Generation possibilities to improve parameter estimation and data analysis.\\nReferences\\n[1] StefanBaueretal,Roadmapondata-centricmaterialsscience,ModellingSimul.Mater.Sci.Eng.,2024,\\n32, 063301.\\n[2] Diao, Zhuo, Hayato Yamashita, and Masayuki Abe. ”Leveraging Large Language Models and Social\\nMedia for Automation in Scanning Probe Microscopy.” arXiv preprint arXiv:2405.15490 (2024).\\n[3] Liu, Yongtao, Marti Checa, and Rama K. Vasudevan. ”Synergizing Human Expertise and AI Efficiency\\nwithLanguageModelforMicroscopyOperationandAutomatedExperimentDesign.”MachineLearning:\\nScience and Technology (2024).\\n[4] Madsen, Jacob, and Toma Susi. ”The abTEM code: transmission electron microscopy from first princi-\\nples.” Open Research Europe 1 (2021).\\n[5] Meyer, Chris, etal.”NionSwift: OpenSourceImageProcessingSoftwareforInstrumentControl, Data\\nAcquisition, Organization, Visualization, and Analysis Using Python.” Microscopy and Microanalysis\\n25.S2 (2019): 122-123.\\n40\\nFigure 15: Schematic overview of the LLMicroscopilot assistant. The microscope user interface allows the\\nusertoinputqueries, whicharethenprocessedbytheLLM.TheLLMexecutesappropriatetoolstoprovide\\ndomain-specific knowledge, support data analysis, or operate the microscope.\\n41\\n11 T2Dllama: Harnessing Language Model for Density Functional\\nTheory (DFT) Parameter Suggestion\\nAuthors: Chiku Parida, Martin H. Petersen\\n11.1 Introduction\\nLargelanguagemodelsarenowgainingtheattentionofmanyresearchersduetotheircapabilitiestoprocess\\nhumanlanguageandperformtasksonwhichtheyhavenotbeenexplicitlytrained,makingthemaninvaluable\\ntoolforresearchersinvariousfieldswheretheinformationisintheformoftext,likescientificjournals,blogs,\\nnews articles, and social media posts, etc. This is particularly applicable to the field of chemical sciences,\\nwhich encounters the challenge of dealing with limited and diverse datasets that are often presented in\\ntext format. LLMs have proven their potential in handling these challenges and are progressively being\\nutilised to predict chemical characteristics, optimise reactions, and even independently design and execute\\nexperiments [1].\\nHere, we used LLM to process published scientific articles to extract simulation parameters and other\\nrelevant information about different materials. This will help experimentalists get an idea of optimised\\nparameters for Density Functional Theory (DFT) calculations for the newly discovered material family.\\nNowadays, DFT is the most valuable tool to model atomistic materials. The idea behind DFT is to use\\nKohn-Sham’s equations to approximately solve Schr¨oding’s equations for the atomic material at hand. The\\napproximation is done by configuring the electron density for the material at each ionic step, where the ions\\nmoveinpositionbasedontheirenergyandforcesdeterminedbytheconfiguredelectrondensity. Thebiggest\\npartoftheapproximationistheexchangefunctional,anddependingonthecomplexityoftheexchangefunc-\\ntional, the approximation becomes more or less comparable with experimental results [6]. When performing\\na DFT calculation, the question is always what exchange functional and parameters to use, as well as what\\nk-space grid to use. This is material-dependent and will change for different materials. The unoptimized\\nparameters can lead to inaccurate results, resulting in a DFT calculation that fails to describe the relative\\nvalues and is therefore not comparable to the experimental results [2]. For that reason, experimentalists\\nnormally collaborate with computational chemists because of their expertise in computational modeling or\\nmake due without the atomistic model. Instead, our T2Dllama [talk-to-douments using Llama] framework\\ncan be an acceptable solution to give the necessary DFT parameters, and using additional tools on the top\\nof the LLM interface, it can create inputs for atomistic simulations [3,4] from the provided structure file by\\nthe user.\\n11.2 Retrieval Augmented Generation\\nThe retrieval augmented generation (RAG) technique [5] is a popular and efficient technique for generating\\nrelevant contextual responses using pre-trained models. It is also less complex as compared to fine-tuning\\nandtrainingLLMfromscratch. Figure16explainsourRAGworkflow. TheRAGworkflowhasthefollowing\\nblocks:\\nData Preparation: We collect open-access scientific journals using Arxiv-api and the necessary filters.\\nPre-existinglicensedscientificjournalsinthelocaldatabasecanbeusedforconfidentialpurposes. Thenthe\\ntext documents are processed to create chunks of text.\\nIndexing and Embedding: Inthiscrucialstep,weusellamaindexing[7]tostorethetokenizeddocuments\\nwith vector indexes and embeddings, transforming them into knowledge vector databases.\\nInformation Retrieval: WhentheLLMinterfacereceivesahumanprompt,themodelsearchestheknowl-\\nedge vector database and retrieves the relevant chunks of stored data.\\nLLM Interface: This is the last step where we communicate with the user. The LLM interface received\\nthe prompt from the user and got the information, as explained previously. The retrieved chunks during\\ninformation retrieval are processed by the pretrained model [Mistral 7B] [8], and the created context is\\ndelivered to the user as a response.\\n42\\nFigure 16: Retrieval Augmented Generation [RAG] architecture with LLM interface\\n11.3 Summary and outlooks\\nThe biggest issue here is training a GPT model to predict DFT exchange functional and parameters is\\nfiltering the data. There is no consensus about using a specific exchange functionals and parameters for a\\nmaterial,whichmakesthemodelconfused. Awaytoavoidthisistoonlyusepapers,whereDFTcalculations\\naredirectlycomparablewithexperimentalresults. Thiswilllimitthevariabilityinthedataandensurethat\\nthe model is trained on reliable information. To conclude through a pre-trained GPT model, we are able\\nto predict DFT exchange functionals and Hubbard-U parameters as well as k-point grid by using RAG\\ntechnique. Further, as we are retrieving information about simulation parameters and exchange-functionals\\nfor materials consistent with respect to experiments from the documents , it is important to prioritise\\nrelevant and reliable sources and special techniques to ensure the accuracy of the data being extracted.\\nOne approach that can be taken involves employing advanced tokenizer techniques specifically designed for\\nscientific notations and terms, so that we can get reliable embedding and vector indexing. This will help to\\nimprovethequalityoftheresponses. WestillneedtodomorethoroughtrainingaswellasaGUIapplication,\\nbut as an initial try in this LLM hackathon we showed that the idea can be realised.\\n11.4 Data and Code Availability\\nAll code and data used in this study are publicly available in the following GitHub repository: https:\\n//github.com/chiku-parida/T2Dllama\\nReferences\\n[1] AdrianMirzaetal.,“Arelargelanguagemodelssuperhumanchemists?”,https://doi.org/10.48550/\\narXiv.2404.01475.\\n[2] Hafner,Ju¨rgen,”Ab-initiosimulationsofmaterialsusingVASP:Density-functionaltheoryandbeyond.”\\nJournal of computational chemistry 29.13 (2008): 2044-2078.\\n[3] Kresse, Georg, and Ju¨rgen Hafner, ”Ab initio molecular dynamics for liquid metals.” Physical review B\\n47.1 (1993): 558.\\n43\\n[4] Mortensen, JensJørgen, LarsBrunoHansen,andKarstenWedelJacobsen, ”Real-spacegridimplemen-\\ntationoftheprojectoraugmentedwavemethod.”PhysicalReviewB—CondensedMatterandMaterials\\nPhysics 71.3 (2005): 035109.\\n[5] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bic, Yi Dai, Jiawei Sun, Meng\\nWang, and Haofen Wang, “Retrieval-Augmented Generation for Large Language Models: A Survey”,\\nhttps://doi.org/10.48550/arXiv.2312.10997.\\n[6] Giustino, Feliciano. Materials modelling using density functional theory: properties and predictions.\\nOxford University Press, 2014.\\n[7] LlamaIndex, https://docs.llamaindex.ai/en/stable/examples/llm/llama_2_llama_cpp/.\\n[8] Mistral 7B, the model used, https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF.\\n44\\n12 Materials Agent: An LLM-Based Agent with Tool-Calling Ca-\\npabilities for Cheminformatics\\nAuthors: Archit Datar, Kedar Dabhadkar\\n12.1 Introduction\\nOut-of-the box large language model (LLM) implementations such as ChatGPT, while offering interesting\\nresponses, generally provide little to no control over the workflow of the LLM by which the response is\\ngenerated. In other words, it is easy to get LLMs to say something in response to a prompt, but difficult to\\nget them to do something via an expected workflow. A solution to do this problem is to equip LLMs with\\ntool-calling capabilities; i.e., allow the LLM to generate the response via an available function(s) which is\\n(are) appropriate to answer the prompt. An LLM with tool-calling capabilities, when prompted, typically\\ndecides which tool(s) to call (execute) and the order in which to execute them, along with the arguments\\nto pass to them. It then executes these and returns the response. Such a system offers several powerful\\ncapabilities such as the ability to query databases to return the latest information and reliably perform\\nmathematical calculations. Such capabilities have been incorporated into ChatGPT via plugins such as\\nExpedia and Wolfram, among others [1]. In the chemistry literature, recent attempts have been made by\\nresearchers such as Smit and coworkers, Schwaller and coworkers, among others [2,3].\\nThroughMaterialsAgent,whichisanLLM-basedagentwithtool-callingcapabilitiesforcheminformatics,\\nwe seek to build on these attempts to provide a variety of important tool-calling capabilities and build a\\nframework to expand on these. We hope that this can serve to increase LLM adoption in the community\\nandlowerthebarriertoentryforcheminformatics. MaterialsAgentisbuiltusingtheLangChainlibrary[4],\\nGPT3.5-turbo[5]astheunderlyingLLM,andtheuserinterfaceisbasedontheFastDashproject[6]. Inthis\\ndemonstration,wehaveprovidedtoolsbasedonRDKit[7]—apopularcheminformaticslibrary,somecustom\\ntools,aswellasaRetrievalAugmentedGeneration(RAG)toallowLLMtointeractwithdocuments. Thefull\\ncode is available at https://github.com/dkedar7/materials-agent and the working application, hosted\\nonGoogleCloudPlatform(GCP),isavailableathttps://materials-agent-hpn4y2dvda-ue.a.run.app/.\\nThe demonstration video is uploaded on YouTube at https://www.youtube.com/watch?v=_5yCOg5Bi_Q&\\nab_channel=ArchitDatar. In the following section, we describe some key use cases.\\n12.2 Equipping LLMs with tools based on standard cheminformatics packages\\n(RDKit)\\nCommon cheminformatics workflows involve obtaining SMILES strings and key properties of molecules.\\nSearchingforthesedataindividuallycanbetimeconsuming. TheRDKitCalcMolDescriptorsmodulecomes\\npre-loadedwiththesedatabasedonSMILESstrings. Wecreatedafunctiontoselect19ofthemostcommon\\nproperties (for ease of visualization) and added it as a tool to the LLM. Not only can the LLM perform the\\nsimple task of providing these descriptions when a SMILES string is input, but it can also perform more\\ncomplicatedtasksinvolvingthisfunctionality. AsshowninFigure17below,givenacomplexquery,theLLM\\nbreaks it down into individual tasks, utilizes these tools in the correct sequence, and renders a response.\\n12.3 Tool-calling for custom tools: Radial Distribution Function calculation\\nCombining custom tools with LLMs lead to some interesting advantages. For one, they offer the makers of\\nthesecustomtoolsanabilitytoprovidetheiruserswithamoreintuitivenaturallanguage-baseduserinterface\\nwhichcanlowerthebarriertoentryandincreaseadoption. Otherbenefitscanbethattheycanbeintegrated\\ninto other workflows and automate more work. To demonstrate this, we have built a tool which computes\\nand plots the radial distribution function (RDF) and integrated it with an LLM. The RDF is an important\\nfunction commonly used in molecular dynamics and Monte Carlo simulations to understand the statistics of\\ndistances between two particles averaged over the simulation. By studying these, one can understand the\\nnature of interactions between these particles. Here, we constructed a custom tool to compute RDF for the\\ndistancebetweenawatermoleculeandtheframeworkatomsinametal-organicframework(MOF)inasingle\\nmolecule canonical ensemble Monte Carlo simulation. The inputs are a PDB file of the MOF structure and\\n45\\nFigure 17: Workflow of a response generated to a user prompt by Materials Agent using a tool based on\\nRDKit.\\na TXT file containing snapshots of the location of the water molecule during the simulation. The distance\\ncomputation also accounts for triclinic periodic boundary conditions which is the accurate way to quantify\\ndistances for crystalline systems such as this one. The inputs and outputs for this tool are shown below in\\nFigure 18(a).\\nFurthermore, we also stress that this approach is easily scalable and transferable, and adding new tools\\nis exceedingly easy. The reader is encouraged to clone our GitHub repository and experiment with adding\\nnewtoolstothissoftwarepackage. Newtoolscanbeeasilyaddedtothesrc/tools.pyfileintherepository\\nvia the format shown in the code snippet in Figure 18(b).\\nFigure 18: Niche purpose tools with LLM. (a) Illustration of the RDF computing tool. (b) Code snippet to\\nhighlight the ease of transferability of LLMs with tool-calling capabilities.\\n46\\n12.4 RAG capabilities\\nSummarizing and asking questions of documents is another common LLM use case. This capability is\\nprovided out-of-the box through the EmbedChain library [8]. and we have integrated that into Materials\\nAgent for convenience. We demonstrate the utility of this by supplying the LLM with a URL to a materials\\nand safety datasheet (MSDS) and asking questions of it (see Figure 19).\\nFigure 19: Illustration of RAG for interacting with a MSDS.\\nInfuture,weaimtoexpandthetoolkitthattheLLMisequippedwith. Forinstance,wecanaddfunctions\\nbuilt on the publicly available PubChem database [9]. as well as some functions built off of it [10]. We also\\naim to train it on user manuals of commonly used molecular simulations software such as GROMACS [11],\\nRASPA [12], and QuantumEspresso [13] to assist with setting up molecular simulations.\\nThrough the experience of building Materials Agent, we realized that, while convenient, such agents\\ncannot replace the need for human vigilance. At the same time, having the development of such an agent\\nwill make cheminformatics utilities easier to access for a broader range of users, lower the barrier to entry,\\nand ultimately, accelerate the pace of materials development.\\nReferences\\n[1] OpenAI plugins, https://openai.com/index/chatgpt-plugins/\\n[2] Jablonka,K.M.,Schwaller,P.,Ortega-Guerrero,A.etal.Leveraginglargelanguagemodelsforpredictive\\nchemistry. Nat Mach Intell 6, 161–169 (2024). https://doi.org/10.1038/s42256-023-00788-1.\\n[3] M. Bran, A., Cox, S., Schilter, O. et al. Augmenting large language models with chemistry tools. Nat\\nMach Intell 6, 525–535 (2024). https://doi.org/10.1038/s42256-024-00832-8.\\n[4] LangChain, https://www.langchain.com/.\\n47\\n[5] OpenAI models, https://platform.openai.com/docs/models.\\n[6] Fast Dash, https://docs.fastdash.app/.\\n[7] RDKit: Open-source cheminformatics; http://www.rdkit.org.\\n[8] Singh, Taranjeet. Embedchain, https://github.com/embedchain/embedchain.\\n[9] PubChem programmatic access, https://pubchem.ncbi.nlm.nih.gov/docs/programmatic-access\\n[10] PubChemPy, https://pubchempy.readthedocs.io/en/latest/guide/introduction.html\\n[11] Abraham, M., Alekseenko, A., Basov, V., Bergh, C., Briand, E., Brown, A., Doijade, M., Fiorin, G.,\\nFleischmann, S., Gorelov, S., Gouaillardet, G., Grey, A., Irrgang, M. E., Jalalypour, F., Jordan, J.,\\nKutzner,C.,Lemkul,J.A.,Lundborg,M.,Merz,P.,... Lindahl,E.(2024).GROMACS2024.2Manual\\n(2024.2). Zenodo. https://doi.org/10.5281/zenodo.11148638.\\n[12] Dubbeldam, D., Calero, S., Ellis, D. E., & Snurr, R. Q. (2015). RASPA: molecular simulation software\\nfor adsorption and diffusion in flexible nanoporous materials. Molecular Simulation, 42(2), 81–101.\\nhttps://doi.org/10.1080/08927022.2015.1010082.\\n[13] Giannozzi, P., Baroni, S., Bonini, N., Calandra, M., Car, R., Cavazzoni, C., Ceresoli, D., Chiarotti,\\nG. L., Cococcioni, M., Dabo, I., Dal Corso, A., de Gironcoli, S., Fabris, S., Fratesi, G., Gebauer, R.,\\nGerstmann, U., Gougoussis, C., Kokalj, A., Lazzeri, M., ... Wentzcovitch, R. M. (2009). Quantum\\nEspresso: Amodularandopen-sourcesoftwareprojectforquantumsimulationsofmaterials.Journalof\\nPhysics: Condensed Matter, 21(39), 395502. https://doi.org/10.1088/0953-8984/21/39/395502.\\n48\\n13 LLM with Molecular Augmented Token\\nAuthors: Luis Pinto, Xuan Vu Nguyen, Tirtha Vinchurkar, Pradip Si, Suneel Kuman\\n13.1 Objective\\nOurprimaryobjectiveistoexplorehowchemicalencoderssuchasmolecularfingerprintsorembeddingsfrom\\n2D/3Ddeeplearningmodels(e.g., ChemBERTa[1], UniMol[2])canenhancelargelanguagemodels(LLMs)\\nfor zero-shot tasks such as property prediction, molecule editing, and generation. We aim to benchmark our\\napproachagainststate-of-the-artmodelslikeLlaSmol[3]andChatDrug[4],demonstratingthetransformative\\npotential of LLMs in the field of chemistry.\\nFigure 20: Workflow for integrating chemical encoders with large language models. Molecular data from\\nSMILESistransformedintomoleculartokensandcombinedwithtextembeddingsfortaskssuchasproperty\\nprediction, molecule editing, and generation.\\n13.2 Methodology\\nWe identified two key benchmarks to evaluate our approach:\\n• LlaSmol: Thisbenchmarkinvolvesfine-tuningaMistral7Bmodel[5]on14differentchemistrytasks,\\nincluding 6 property prediction tasks. The LlaSmol project demonstrated significant performance\\nimprovements over baseline models, both open-source and proprietary, by using the SMolInstruct\\ndataset, which contains over three million samples.\\n• ChatDrug: This framework leverages ChatGPT for molecule generation and editing tasks. It in-\\ncludes a prompt module, a retrieval and domain feedback module, and a conversation module to\\nfacilitate effective drug editing. ChatDrug showed superior performance across 39 drug editing tasks,\\nencompassingsmallmolecules,peptides,andproteins,andprovidedinsightfulexplanationstoenhance\\ninterpretability.\\n13.3 Our approach\\nWeproposetofine-tuneaMistral7Binstructmodeltocompeteagainstthesebenchmarks. Duetocompute\\nconstraints, we were unable to complete the training. Training the model using QLoRA 4-bit [6] on 50k\\nproperty prediction samples requires approximately 5 hours per epoch on a 24GB GPU.\\n49\\nSteps Taken:\\n• Data Preparation: We utilized chemical encoders to transform molecular structures into suitable\\nembeddings for the LLM.\\n• Model Modification: We integrated the embeddings into the LLM’s forward function to enrich its\\ninput.\\n• Fine-Tuning: We applied QLoRA for efficient training on limited computational resources.\\n• Preliminary Results and Ongoing Work: Although we are still fine-tuning the LLMs, initial\\nresults are promising.\\n• Code Snippets: Screenshots in Figures 21 and 22 demonstrate the modifications made to the model\\ncode to extract embeddings and implement the forward function of the LLM.\\n13.4 Conclusion\\nOur project underscores the potential of using LLMs enhanced with chemical encoders in materials science\\nand chemistry. By fine-tuning these models, we aim to improve property prediction and facilitate molecule\\nediting and generation, paving the way for future research and applications in this space. The code is\\navailable at https://github.com/luispintoc/LLM-mol-encoder.\\nFigure 21: Modified code of the function get embeddings.\\nReferences\\n[1] S.Chithrananda,G.Grand,andB.Ramsundar,“ChemBERTa: Large-ScaleSelf-SupervisedPretraining\\nforMolecularPropertyPrediction,”arXivpreprintarXiv:2010.09885,2020.Available: https://arxiv.\\norg/abs/2010.09885\\n[2] G. Zhou, Z. Gao, Q. Ding, H. Zheng, H. Xu, Z. Wei, et al., “Uni-Mol: A Universal 3D Molecular\\nRepresentation Learning Framework,” ChemRxiv, 2022, doi:10.26434/chemrxiv-2022-jjm0j.\\n[3] B. Yu, F. N. Baker, Z. Chen, X. Ning, and H. Sun, “LlaSMol: Advancing Large Language Models\\nfor Chemistry with a Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset,” arXiv\\npreprint arXiv:2402.09391, 2024. Available: https://arxiv.org/abs/2402.09391\\n[4] S. Liu, J. Wang, Y. Yang, C. Wang, L. Liu, H. Guo, and C. Xiao, “ChatGPT-powered Conversational\\nDrugEditingUsingRetrievalandDomainFeedback,”arXivpreprintarXiv:2305.18090,2023.Available:\\nhttps://arxiv.org/abs/2305.18090\\n50\\nFigure 22: Modified forward function which allows for the molecular token to be added.\\n[5] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de las Casas, F. Bressand,\\nG. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. Le Scao, T. Lavril, T.\\nWang, T. Lacroix, and W. El Sayed, “Mistral 7B,” arXiv preprint arXiv:2310.06825, 2023. Available:\\nhttps://arxiv.org/abs/2310.06825\\n[6] T.Dettmers,A.Pagnoni,A.Holtzman,andL.Zettlemoyer,“QLoRA:EfficientFinetuningofQuantized\\nLLMs,” arXiv preprint arXiv:2305.14314, 2023. Available: https://arxiv.org/abs/2305.14314\\n51\\n14 MaSTeA: Materials Science Teaching Assistant\\nAuthors: Defne Circi, Abhijeet S. Gangan, Mohd Zaki\\n14.1 Dataset Details\\nWe take 650 questions from materials science question answering dataset (MaScQA) [1], which required\\nundergraduate-levelunderstandingtosolvethem. Theauthorsclassifiedthemintofourtypesbasedontheir\\nstructure: Multiple choice questions (MCQs), Match the following type questions (MATCH), Numerical\\nquestions where options are given (MCQN), and numerical questions (NUM): see Table 3. MCQs are\\ngenerally conceptual, given four options, out of which mostly one is correct, and sometimes more than one\\noption is also correct. In MATCH, two lists of entities are given, which are to be matched with each other.\\nThesequestionsarealsoprovidedwithfouroptions,outofwhichonehasthecorrectsetofmatchedentities.\\nInMCQN,thequestionhasfourchoices,outofwhichthecorrectoneisidentifiedaftersolvingthenumerical\\nproblem stated in the question. The NUM type questions have numerical answers, rounded to the nearest\\ninteger or floating-point number as specified in the questions.\\nTo understand the performance of LLMs from a domain perspective, the questions were classified into\\n14 topical categories [1]. The database can be accessed at https://github.com/M3RG-IITD/MaScQA.\\n14.2 Methodology\\nOurobjectivewastoautomatetheevaluationofbothopen-sourceandproprietaryLLMsonmaterialsscience\\nquestionsfromtheMaScQAdatasetandprovideaninteractiveinterfaceforstudentstosolvethesequestions.\\nWeevaluatedtheperformanceofseverallanguagemodels,includingLLAMA3-8B,HAIKU,SONNET,GPT-\\n4,andOPUS,acrossthe14variouscategoriessuchascharacterization,applications,properties,andbehavior.\\nThe evaluation involved:\\n• Extracting corresponding values: For multiple-choice questions, correct answer options were ex-\\ntracted using regular expressions to compare model predictions against the correct choices.\\n• Prediction verification: Fornumericalquestions,thepredictedvaluewascheckedagainstaspecified\\nrangeorexactvalue. Formultiple-choicequestions,thepredictedanswerwasverifiedagainstthecorrect\\noption or the extracted corresponding value.\\n• Calculating accuracy: Accuracy was calculated for each question type and topic, and the overall\\naccuracy across all questions was computed.\\nTheresultsoftheevaluationaresummarizedinTable4,whichpresentstheaccuracyofthevariousmodels\\nfor different question types and topics. The opus variant of Claude consistently outperformed the others,\\nachieving the highest accuracy in most categories. GPT-4 also showed strong performance, particularly in\\ntopics related to material processing and fluid mechanics.\\nOur interactive web app, MaSTeA (Materials Science Teaching Assistant), developed using Streamlit,\\nallowseasymodeltestingtoidentifyLLMs‘strengthsandweaknessesindifferentmaterialssciencesubfields.\\nTheresultssuggestthatthereissignificantroomforimprovementtoenhancetheaccuracyoflanguagemodels\\ninansweringscientificquestions. Oncethesemodelsbecomemorereliable,MaSTeAcouldbeavaluabletool\\nfor students to practice answering questions and learn the steps to get to the answer. By analyzing LLM\\nperformance, we aimed to guide future model development and pinpoint areas for improvement.\\nOur code and application can be found at:\\n• https://github.com/abhijeetgangan/MaSTeA\\n• https://mastea-nhwpzz8fehvc9b3n5bhzya.streamlit.app/\\nReferences\\n[1] Zaki, M., & Krishnan, N. A. (2024). MaScQA: investigating materials science knowledge of large lan-\\nguage models. Digital Discovery, 3(2), 313-327.\\n52\\nTable 3: Sample questions from each category: (a) multiple choice question (MCQ), (b) matching type\\nquestion (MATCH), (c) numerical question with multiple choices (MCQN), and (d) numerical question\\n(NUM). Correct answers are in bold.\\nA peak in the X-ray diffraction pattern is ob-\\nservedat2θ =78◦,correspondingto{311}planes\\nFloatation beneficiation is based on the principle\\nof an fcc metal, when the incident beam has a\\nof\\nwavelength of 0.154 nm. The lattice parameter\\nof the metal is approximately\\n(A) Mineral surface hydrophobicity (A) 0.6 nm\\n(B) Gravity difference (B) 0.4 nm\\n(C) Chemical reactivity (C) 0.3 nm\\n(D) Particle size difference (D) 0.2 nm\\n(c) Numerical question with multiple choices\\n(a) Multiple choice question (MCQ)\\n(MCQN)\\nThethirdpeakintheX-raydiffractionpatternof\\naface-centeredcubiccrystalisat2θ valueof45◦,\\nwhere 2θ is the angle between the incident and\\nMatch the composite in Column I with the most\\nreflected rays. The wavelength of the monochro-\\nsuitable application in Column II.\\nmatic X-ray beam is 1.54 ˚A. Considering first-\\norder reflection, the lattice parameter (in ˚A) of\\nthe crystal is? (Round off to two decimal places)\\nColumn I: (P) Glass fibre reinforced plastic, (Q)\\nSiC particle reinforced Al alloy, (R) Carbon-\\nAns. 5.64 to 5.73\\ncarboncomposite,(S)Metalfibrereinforcedrub-\\nber:\\nColumn II: (1) Missile cone heads, (2) Commer-\\ncial automobile chassis, (3) Airplane wheel tyres,\\n(4) Car piston rings, (5) High performance skate\\nboards:\\n(A) P-4, Q-5, R-1, S-2 (B) P-3, Q-5, R-2, S-4\\n(C) P-5, Q-4, R-1, S-3 (D) P-4, Q-2, R-3, S-1\\n(b) Matching type question (MATCH) (d) Numerical question (NUM)\\n53\\nTable 4: Accuracy of Language Models by Topic\\nTopic # Questions LLaMA-3-8b Haiku Sonnet OPUS GPT4\\nThermodynamics 114 37.72 47.37 55.26 73.68 57.02\\nAtomic structure 100 32 40 49 64 59\\nMechanical behavior 96 22.92 41.67 52.08 71.88 43.75\\nMaterial manufacturing 91 43.96 57.14 56.04 80.22 68.13\\nMaterial applications 53 52.83 64.15 77.36 92.45 86.79\\nPhase transition 41 31.71 46.34 65.85 70.73 63.41\\nElectrical properties 36 33.33 25 55.56 72.22 44.44\\nMaterial processing 35 48.57 54.29 74.29 88.57 88.57\\nTransport phenomena 24 37.5 70.83 58.33 87.5 62.5\\nMagnetic properties 15 26.67 46.67 46.67 66.67 60\\nMaterial characterization 14 78.57 57.14 85.71 92.86 71.43\\nFluid mechanics 14 21.43 50 57.14 78.57 85.71\\nMaterial testing 9 77.78 66.67 100 100 100\\nMiscellaneous 8 62.5 62.5 62.5 75 62.5\\n54\\n15 LLMy-Way\\nAuthors: Ruijie Zhu, Faradawn Yang, Andrew Qin, Suraj Sudhakar, Jaehee Park, Victor\\nChen\\n15.1 Introduction\\nIn the academic realm, researchers frequently present their work and that of others to colleagues and lab\\nmembers. This task, while essential, is fraught with difficulties. For example, below are three challenges:\\n1. Reading and understanding research papers: Comprehending the intricacies of a research paper can\\nbe daunting, particularly for interdisciplinary subjects like materials science.\\n2. Creating presentation slides: Designing slides that effectively communicate the content requires sig-\\nnificant effort, including remaking slides, sourcing images, and determining optimal text and image\\nplacement.\\n3. Tailoring to the audience: Deciding on the appropriate level of technical vocabulary and the number\\nof slides needed to fit within a given time limit adds another layer of complexity.\\nFigure 23\\nThese challenges can be effectively addressed using large language models, which can streamline and\\nautomate the text summarization and slide creation process. LLMy Way leverages the power of GPT-3.5-\\nturbotoautomatethecreationofacademicslidesfromresearcharticles. Themethodologyinvolvesprimarily\\nthree steps:\\n15.2 Structured Summarization\\nWe prompt the large language model to generate a structured summary of the research paper, adhering to\\nthetypicalsectionsofanacademicpaper: background,challenge,currentstatus,method,result,conclusion,\\nand outlook. Each section is summarized in under a certain number of words to ensure brevity and clarity.\\nExamplePrompt: Readthepaper,andsummarizein{num words}wordseachaboutthefollowing:\\\\n\\\\n1.\\nBackground\\\\n2. Challenge\\\\n3. Current status\\\\n4. Method\\\\n5. Result\\\\n6. Conclusion\\\\n7. Outlook\\\\n\\n55\\n15.3 Slide Generation\\nTo create slides, we format the language model’s output in a specific manner, using symbols to denote slide\\nbreaks. This output is then parsed and converted into a Markdown file, where additional images and text\\nformattingareappliedasneeded. TheformattedMarkdownfileissubsequentlytransformedintoPDFslides\\nusing Marp. Example output formatting:\\n# Background\\nSummary of the background here.\\n−−−\\n# Challenge\\nSummary of the challenge here.\\n−−−\\n15.4 Customization for Audience and Time Limit\\nLLMy Way allows customization based on the target audience’s expertise level (expert or non-expert) and\\nthe presentation time limit (e.g., 10 minutes, 15 minutes). This information is incorporated into the initial\\ngeneration phase, ensuring that the content and slide count are appropriately tailored. This feature is to be\\nimplemented.\\n15.5 Conclusion\\nLLMy Way represents a significant step forward towards the automation of academic presentation prepara-\\ntion. Byleveragingthestructurednatureofscientificpapersandthecapabilitiesofadvancedlargelanguage\\nmodels, our tool addresses common pain points faced by researchers. The current implementation of our\\nframework can be summarized into three consecutive steps. First, the research paper is parsed by LLM,\\nwhich is summarized into predefined sections. Next, the summarized texts are converted into Markdown.\\nFinally, MarpisusedtogeneratethefinalslidedeckinthePDFformat. Thecurrentimplementationofour\\nframeworkusesGPT-3.5-turbo, butitcanbeadaptedtootherlanguagemodelsasneeded. Wealsosupport\\nthe output format of LaTex to fit the needs of many researchers. Future work will focus on further refining\\nthe tool, incorporating user feedback, and exploring additional customization options.\\nReferences\\n[1] OpenAI. (2024). GPT-3.5-turbo. https://openai.com/api/.\\n[2] Marp. (2024). Markdown Presentation Ecosystem. https://marp.app/\\n56\\n16 WaterLLM: Creating a Custom ChatGPT for Water Purifica-\\ntion Using Prompt-Engineering Techniques\\nAuthors: Viktoriia Baibakova, Maryam G. Fard, Teslim Olayiwola, Olga Taran\\n16.1 Introduction\\nDrinking water pollution is a growing environmental problem that, ironically, comes from an increase in\\nindustrial production of new materials and chemicals. Common pollutants include heavy metals, perfluo-\\nrinated compounds, microplastic particles, excreted medicinal drugs from hospitals, agricultural runoff and\\nmany others [1]. The communities that suffer the most from water contamination often lack the plumbing\\ninfrastructure necessary for centralized water analysis and treatment. Decentralized and localized water\\ntreatments, based on resources available to the communities, can alleviate the problem. Since the resources\\ncan vary greatly, from well equipped analytical facilities to low-cost DIY solutions, a knowledge base that\\ncan rapidly provide information relevant for specific situations is needed. Here we show a prototype Large\\nLanguage Model (LLM) chatbot that can take a variety of inputs about possible contaminants (ranging\\nfrom detailed LC/MS analysis to general description of the situation) and propose the best solution to the\\nwater treatment for the particular case based on contaminant composition, cost and resources availability.\\nWe employed a Retrieval Augmented Generation (RAG) capability of ChatGPT to answer questions about\\npossible water treatments based on the latest scientific literature .\\nFor this project, we focused on advanced oxidation procedures (AOPs) for water purification from mi-\\ncroplastics (MPs). In recent times, MPs have received significant attention globally due to their widespread\\npresence in various species’ bodies, environmental media, and even bottled drinking water, as frequently\\ndocumented [2]. Numerous trials have been conducted and reported on the use of AOPs for breaking down\\ndiverse persistent microplastics as wastewater treatment methodologies. However, there remains a lack of\\nguidelines on selecting the most suitable and cost effective treatment method based on the characteristics of\\nthe contaminant, maximum removal percentage of MP.\\nThe complexity of existing research on AOPs for MPs can be tackled with LLMs enhanced with RAG.\\nRAG allows to augment LLM’s knowledge and achieve state of the art results on knowledge-intensive tasks.\\nOne straightforward modern way to implement LLM with RAG is through configuring a custom chatGPT.\\nWe uploaded current scientific papers under the “Knowledge” for RAG and tailored chatbot performance\\nwith prompt-engineering techniques such as grounding, context, and chain-of-thought reasoning to ensure\\nthat it delivers accurate, detailed, and useful information.\\n16.2 Grounding\\nTomakesurethatchatbotprovidesaccurateandscientificallyvalidanswers,weloadedthelatestresearchon\\nmicroplasticpollutionremediationforRAGandimplementedgroundinginchatprompt. Tocollectthedata,\\nwegatheredtheinitialsetof10reviewarticlesfromtheexpertinthefieldthattalkaboutwaterpurification\\nusing Advanced Oxidation Process. Then, we manually found 112 scientific articles discussing the specific\\ntreatmentprocedure. Thisway, ourdatasethasstudiesondifferentpollutionsourceslikelaundry, hospitals,\\nindustry, different pollutant types and their descriptive characteristics like size, shape, color, and different\\ntreatment methods like Ozonation, Fenton Reaction, UV, Heat-Activated Persulfate. We merged all papers\\ninto 8 pdfs to meet chatGPT “Knowledge” restrictions in files number, size and length. With grounding, we\\naim to anchor the chatbot’s responses in concrete and factual knowledge. We explicitly asked chatGPT to\\navoidgivingwordybroadgeneralizedanswersandtoprovideconcisescientificdetailsusingthefilesuploaded\\nunder Knowledge.\\n16.3 Context\\nWe provided context for the chatbot to understand and respond appropriately to user queries. We specified\\nthat Chatbot has expert-level knowledge on MPs and water purification strategies from MPs and other\\ncontaminants. Wedefinedauserasatechnicianwithbasicknowledgeonchemicalengineeringthatneedsto\\nchoose and apply a purification method. We set that the communication between Chatbot and User should\\n57\\nbe in the form of interactive dialog. It means that Chatbot should ask follow-up questions from the user\\nandshouldfinallyreturnanaccuratepurificationprotocolwithallthedetailsthatcanbereproducedinthe\\nexperiment.\\n16.4 Chain-of-Thought Reasoning\\nTo further ensure that the chatbot considers all necessary aspects before providing a solution, we employed\\nthe chain-of-thought reasoning by breaking down the problem-solving process into sequential steps: Get the\\nsource of contamination. Ask what the pollutant particle is and suggest evaluation tests if it is unknown.\\nAsk if the characteristics of pollutants are known: size, shape, type. Suggest the most effective purification\\napproach that will get rid of the largest percentage of the pollutants and estimate the price. Adjust if it is\\ntoo expensive. Inquiry about the post-treatment analysis. If unsatisfactory, go to the previous step. End of\\nconversation: Chatbot should provide a table with all used purification methods and their parameters for\\nestablished contaminants.\\nThese techniques allowed to boost the custom GPT performance, and WaterLLM demonstrated perfor-\\nmance approved by the expert in the field.\\nFigure 24: WaterLLM approach: custom chatGPT with RAG from scientific papers, context and chain-of-\\nthought allowed for interactive dialog with the user anchored to science.\\nReferences\\n[1] Mishra, R. K.; Mentha, S. S.; Misra, Y.; Dwivedi, N. Emerging Pollutants of Severe Environmental\\nConcern in Water and Wastewater: A Comprehensive Review on Current Developments and Future\\nResearch. Water-Energy Nexus 2023, 6, 74–95. https://doi.org/10.1016/j.wen.2023.08.002.\\n[2] Li, Y.; Peng, L.; Fu, J.; Dai, X.; Wang, G. A Microscopic Survey on Microplastics in Beverages: The\\nCase of Beer, Mineral Water and Tea. Analyst 2022, 147 (6), 1099–1105. https://doi.org/10.1039/\\nD2AN00083K.\\n58\\nFigure 25: Sample of the WaterLLM communication with the User.\\n59\\n17 yeLLowhaMMer: A Multi-modal Tool-calling Agent for Accel-\\nerated Research Data Management\\nAuthors: Matthew L. Evans, Benjamin Charmes, Vraj Patel, Joshua D. Bocarsly\\nAs scientific data continues to grow in volume and complexity, there is a great need for tools that can\\nsimplify the job of managing this data to draw insights, increase reproducibility, and accelerate discovery.\\nDigital systems of record, such as electronic lab notebooks (ELN) or laboratory information management\\nsystems (LIMS), have been a great advancement in this area. However, complex tasks are still often too\\nlaborious, or simply impossible, to accomplish using graphical user interfaces alone, and any barriers to\\nstreamlined data management often lead to lapses in data recording.\\nAsdevelopersoftheopen-sourcedatalab [1]ELN/LIMS,weexploredhowlargelanguagemodels(LLMs)\\ncan be used to simplify and accelerate data handling tasks in order to generate new insights, improve\\nreproducibility,andsavetimeforresearchers. Previously,wemadeprogresstowardthisgoalbydevelopinga\\nconversational assistant, named Whinchat [2], that allows users to ask questions about their data. However,\\nthisassistantwasunabletotakeactionwithauser’sdata. Here,wedevelopedyeLLowhaMmer,amultimodal\\nlarge language model (MLLM)-based data management agent capable of taking free-form text and image\\ninstructions from users and executing a variety of complex scientific data management tasks.\\nOur agent is powered by a low-cost commercial MLLM (Anthropic’s Claude 3 Haiku) used within a\\ncustom agentic infrastructure that allows it to write and execute Python code that interacts with datalab\\ninstances via the datalab-api package. In typical usage, a yeLLowhaMmer user might instruct the agent:\\n“Pull up my 10 most recent sample entries and summarize the synthetic approaches used.” In this case, the\\nagent will attempt to write and execute Python code using the datalab API to query for the user’s samples\\ninthedatalab instanceandwriteahuman-readablesummary. Ifthecodeitgeneratesgivesanerror(ordoes\\nnot give sufficient information), the agent can iteratively rewrite the program until the task is accomplished\\nsuccessfully.\\nFurthermore,weleveragedthepowerfulmultimodalcapabilitiesofthelatestMLLMstoallowforprompts\\nthat include visual cues. For example, a user may upload an image of a handwritten lab notebook page and\\nask that a new sample entry be added to the datalab instance. The agent uses its multimodal capabilities to\\n“read” the lab notebook page (even if it is a messy/unstructured page), adds structure to the information\\nit finds by massaging it into the form requested by the datalab JSON schema, then writes a Python snippet\\nto ingest the new sample into the datalab instance. Notably, we found that even the inexpensive, fast model\\nwe used (Claude 3 Haiku) was able to perform sufficiently well at this task, while larger models may be\\nexplored in the future to allow for more advanced applications (though with slower speed and greater cost).\\nWe believe the capabilities demonstrated by yeLLowhaMmer show that MLLM agents have the potential to\\ngreatly lower the barrier to advanced data handling in experimental materials and chemistry laboratories.\\nThis proof-of-concept work is accessible on GitHub at bocarsly-group/llm-hackathon-2024, with ongoing\\nwork at datalab-org/yellowhammer.\\nyeLLowhaMmerwasbuiltuponseveralopen-sourcesoftwarepackages. Thecodebox-apiPythonpackage\\nwas used to set up a local code sandbox that the agent has access to in order to safely read and save files,\\ninstall Python packages, and run the code generated by the model. The datalab-api Python package was\\nused to interact with datalab instances. An MLLM-compatible tool was designed to allow the model to\\nuse function-calling capabilities, write, and execute code within the sandbox. LangChain was used as a\\nframework to interact with the commercial MLLM APIs and build the agentic loop. Streamlit was used\\nto build a responsive GUI to show the conversation and upload/download the files from the codebox. A\\ncustomized Streamlit callback was written to display the code and files generated by the agent in a user-\\nfriendly manner.\\nAn interesting challenge in the development of yeLLowhaMmer was the creation of a system prompt\\nthat would enable the agent to reliably generate robust code using the datalab-api package, which is a\\nrecent library not included in the training of the commercial models at the time of writing. Initially, we\\ncopiedtheexistingdocumentationforthedatalab-apiintothesystemprompt,butwefoundthatthecode\\ngenerated by the model was not working very well. Instead, it was helpful to produce a simplified version\\nof the documentation that removed extraneous information and gave a few concrete examples of scripts.\\nAdditionally, we provided an abridged version of the datalab schemas in the JSON Schema format in the\\nsystem prompt, which was necessary for the generation of compliant data to be inserted into datalab.\\n60\\nFigure26: TheyeLLowhaMmermultimodalagentcanbeusedforavarietyofdatamanagementtasks. Here,\\nit is shown automatically adding an entry into the datalab lab data management system based on an image\\nof a handwritten lab notebook page.\\nOverall, the yeLLowhaMmer system prompt amounts to around 12,000 characters (corresponding to\\nabout 3200 tokens using Claude’s tokenizer). Given the large context windows of the current generation of\\nMLLMs(e.g.,200ktokensforClaude3Haiku),thissizeofpromptisfeasibleforfast,extendedconversations\\ninvolving text, generated code, and images. In the future, we envision that library maintainers may wish\\nto increase the utility of their libraries by maintaining two parallel sets of documentation: the standard\\nhuman-readable documentation, and an abridged agents.txt (or llms.txt – https://llmstxt.org/) file\\nthat can be used by ML agents to write high-quality code using that library.\\nGoing forward, we will undertake further prototyping to incorporate MLLM-based agents more tightly\\ninto our data management workflows, and to ensure that data curated or modified by such an agent will\\nbe appropriately ‘credited’ by, for example, visually demarcating AI-generated content, and providing UI\\npathways to verify or ‘relabel’ such data in an efficient manner. Finally, we emphasize the great progress\\nmade within the last year in MLLMs themselves, which are now able to handle audio and video content in\\naddition to text and images. These will allow MLLM agents to use audiovisual data in real-time to provide\\nnew user interfaces. Based on these promising developments, we believe that data management platforms\\nare well-placed to help bridge the divide from physical to digital data recording.\\nReferences\\n[1] M. L. Evans and J. D. Bocarsly. datalab, July 2024. URL https://github.com/datalab-org\\ndoi:10.5281/zenodo.12545475.\\n[2] Jablonka et al. 14 examples of how LLMs can transform materials science and chemistry: a reflection\\non a large language model hackathon. Digital Discovery, 2023. doi:10.1039/D3DD00113J.\\n61\\n18 LLMads\\nAuthors: Sarthak Kapoor, Jos´e M. Pizarro, Ahmed Ilyas, Alvin N. Ladines, Vikrant Chaud-\\nhary\\nParsingrawdataintoastructured(meta)datastandardorschemaisamajorResearchDataManagement\\n(RDM)topic. WhiledefiningF.A.I.R.(Findable,Accessible,Interoperable,andReusable)metadataschemas\\nare the key to RDM, these empty fields must be populated. This is typically done in two ways:\\n• Fill a schema manually using electronic or physical lab notebooks, or\\n• Create scripts that read the input/output raw files and parse them into the data schema.\\nThe first option is used in a lab setting where data is entered in real-time as it is generated. The second\\noptionisusedwhendatafilesareavailable,albeitinanincompatibleformattofilltheschemadirectly. These\\ncan be measurement files coming from instruments or files generated from simulations. Specific parsers for\\neach raw file type can transfer large amounts of data into schemas, making them essential for automation\\nand big-data management. However, implementing parsers for all the possible raw files to a fill schema can\\nbe laborious and time-consuming. It requires expert knowledge of the structure of the raw files and regular\\nmaintenancetokeepupwithnewversionsofrawfiles. Inthiswork,weattemptedtosubstituteparserswith\\nLarge Language Models (LLMs).\\nWe investigated whether LLMs can be used to parse data into structured schemas, thus relieving the\\nneed forcoding parsers. As an example, we usedrawfiles from X-ray diffraction (XRD)measurements from\\nthree different instrument vendors (Bruker, Rigaku, and Panalytical). We defined a Pydantic model for our\\nstructured data schema and used the pre-trained Mixtral-8x7b from Groq. The data schemas are provided\\nto the LLM using the function-calling mechanism. The schema is constructed by defining a Pydantic base\\nmodel class and their fields or attributes with well-defined types and descriptions. The LLM tries to extract\\ndata for each variable from the raw files that matches these descriptions. Considering the size of the raw\\nfiles and the token limitation of LLMs, we decided to create the following workflow:\\n• Break the raw file contents into chunks.\\n• Prompt the LLM with the initial chunk.\\n• Generate a response from the LLM and populate the schema.\\n• Prompt the LLM with the next chunk along with the previous response.\\nWe found that when populating the schema, the LLM was correctly extracting the values in cases where\\nthe data types were float and str. This was the case for the XRDSettings class. However, when parsing\\nvalueswithadatatypelist[float],theLLMwasoftenunabletoextractthedataintheexpectedformat.\\nThis occurred when populating XRDResults class with the intensities data. The LLM output included non-\\nnumeric characters like \\\\n or \\\\t, along with hallucinated data values. By providing the previous response\\nalong with the new chunk of data in the prompts, we incorporated some degree of context. We found that\\nusing smaller chunk sizes led to the rapid replacement of the populated data. Sometimes, the correct data\\nwas replaced by hallucinated values.\\nWeusedLangChaintobuildourmodelsandpromptgenerators. ThecodeisopenlyavailableonGithub:\\nhttps://github.com/ka-sarthak/llmads. Ourworkusespromptengineeringandfunction-calling. Future\\nwork into tuning the model temperature and fine-tuning could be explored to combat hallucination. Our\\nworkalsoindicatesaneedforhumaninterventiontoverifyiftheschemawasfilledcorrectlyandtocorrectit\\nwhennecessary. Nevertheless,ourpromptingstrategyprovestobeavaluabletoolasitmanagestoinitialize\\nthe schema properly for non-vectorial fields, all at the minimal effort of providing a structured schema and\\nthe raw files.\\n62\\n19 NOMAD Query Reporter: Automating Research Data Narra-\\ntives\\nAuthors: Nathan Daelman, Fabian Sch¨oppach, Carla Terboven, Sascha Klawohn, Bernadette\\nMohr\\nMaterials science research data management (RDM) platforms and structured data repositories contain\\nlargenumbersofentries,eachcomposedofproperty-valuepairs. Usersquerytheserepositoriesbyspecifying\\nproperty-valuepairstofindentriesmatchingspecificcriteria. Whilethisguaranteesthatallreturnedentries\\nhave at least the queried properties, they do not provide context or insights into the structure and variety\\nof other data present in them.\\nTraditionally, it is up to the data scientist to examine the returned properties and interpret the overall\\nresponse. To assist with this task, we use a LLM to create context-aware reports based on the properties\\nandtheirmeanings. Webuildandtestedour“QueryReporter”[1]prototypeontheNOMAD[2]repository,\\nwhichstoresheterogeneousmaterialssciencedata,includingmetadataonscientificmethodologies,atomistic\\nstructures, and materials properties.\\nWe developed the NOMAD Query Reporter [1] as a proof-of-concept. It fetches and analyzes entries\\nand produces a summary of the used methodology and standout results. It does so in a scientific style,\\nlending itself as the basis for a “methods” section in a journal article. Our agent uses a retrieval-augmented\\ngeneration(RAG)approach[3],whichenrichesanLLM’sknowledgeofexternalDBdatawithoutperforming\\nretraining or fine-tuning. To allow its safe application on private and unpublished data, we use a self-hosted\\nOllama instance running Meta’s Llama3 70B [4] model.\\nWe tested the agent on publicly available data from NOMAD. To manage Llama3’s context window of\\n8,000tokens,theentriesarecollectedasrowsintoaPandasdataframe. Eachrow(e.g.,entry)isindividually\\npassed on to the LLM via the chat-completion API. Instead of a single message, it accepts a multi-turn\\nconversation that simulates several demarcated roles. We use the “system” and “user” roles of the chat to\\nreinforce the retention of parts of the previous summary. This approach generally conforms to the Naive\\nRAG category in Gao et al.’s classification [3]. For a step-by-step overview, see Figure 27.\\nWe used two kinds of data for testing: (a) homogeneous, property-value pairs of computational data;\\nand (b) heterogeneous text typed properties of solar cell experiments, often formatted as in-text tables.\\nWe engineered different prompts for each kind. The agent performed better on the homogeneous than the\\nheterogeneous data. Here, summaries would often suffer from irrelevant threads, or even hallucinations. We\\ntheorize that homogeneous data maps more consistently onto our predefined dataframe columns, which aids\\nthe LLM in interpreting follow-up messages. Still, we could not improve the performance for heterogeneous\\ndata within the hackathon.\\nInshort,theNOMADQueryReporterdemonstratesthatthecombinedapproachoffilteringandRAGcan\\neffectivelysummarizecollectionsoflimited-size,structureddatadirectlystoredinresearchdatarepositories,\\nallowing for automated drafting of methods and setups for publications at a consistent level of quality\\nand style. These results suggest applicability for other well-defined materials science APIs, such as the\\nOPTIMADEstandard[5]. Follow-upworkincludesinvestigatingtheimpactofAdvancedRAGstrategies[3].\\n19.1 Acknowledgements\\nN. D., S. K., and B. M. are members of the NFDI consortium FAIRmat, funded by the German Research\\nFoundation (DFG, Deutsche Forschungsgemeinschaft) in project 460197019. F. S. acknowledges funding\\nreceived from the SolMates project, which has been supported by the European Union’s Horizon Europe\\nresearch and innovation program under grant agreement No 101122288. C. T. is supported by the German\\nFederalMinistryofEducationandResearch(BMBF,Bundesministeriumfu¨rBildungundForschung)inthe\\nframework of the project Catlab 03EW0015A.\\nReferences\\n[1] NOMAD:Adistributedweb-basedplatformformanagingmaterialsscienceresearchdata.M.Scheidgen,\\net al., JOSS, 8(90), 5388 (2024), doi.org/10.21105/joss.05388.\\n63\\nFigure27: FlowchartoftheQueryReporterusage,includingtheback-endinteractionwithexternalresources,\\ni.e., NOMAD and Llama. Intermediate steps managing hallucinations or token limits are marked in red and\\norange, respectively.\\n[2] https://github.com/ndaelman-hu/nomad_query_reporter.\\n[3] Retrieval-Augmented Generation for Large Language Models: A Survey. Gao, Y., et al., arXiv (2024),\\ndoi.org/10.48550/arXiv.2312.10997.\\n[4] Llama: open and efficient foundation language models. H. Touvron, et al., arXiv (2023), doi.org/10.\\n48550/arXiv.2302.13971.\\n[5] Development and applications of the OPTIMADE API for materials discovery, design, and data ex-\\nchange. Evans, M. L., et al., Digital Discovery (2024), DOI: doi.org/10.1039/D4DD00039K.\\n64\\n20 Speech-schema-filling: Creating Structured Data Directly from\\nSpeech\\nAuthors: Hampus N¨asstr¨om, Julia Schumann, Michael Go¨tte, Jos´e A. M´arquez\\n20.1 Introduction\\nAstheamountofmaterialssciencedatabeingcreatedincreases,sodotheeffortstomakethisdataFindable,\\nAccessible, Reusable, and Interoperable (FAIR) [1]. One pragmatic approach to creating FAIR data is by\\ndefiningso-calleddataschemasforthevarioustypesofdatabeingrecorded. Theseschemascanthenbeused\\ninbothinputtoolslikeelectroniclabnotebooks(ELNs)andstoragesolutionslikedatarepositoriestocreate\\nstructured data. One widely adopted standard for writing data schemas is the so-called JSON Schema [2].\\nJSON Schema allows us to define objects, such as, for example, a solution preparation experiment in the\\nlab, with properties such as temperature and a list of solutes and solvents (see Figure 28a). These schemas\\ncan then be used to create forms in an ELN like NOMAD [3] (see Figure 28b). However, in a lot of lab\\nsituations, such as when working inside a glovebox, it is difficult to i) navigate the ELN and select the right\\nform and ii) actually fill in the form with experimental data. In our experience, this usually results in users\\nhaving to record their data later from memory or even in data not being recorded at all.\\nWe propose a solution for this using LLMs to:\\n• Converting spoken language in the lab into text using advanced speech recognition technologies, such\\nas OpenAI’s Whisper.\\n• Based on the text select and fill the appropriate schema, enabling accurate data capture without\\nmanual text entry.\\n20.2 Speech recognition\\nThe first step of converting speech into structured data is to record the speech and transcribe it into text.\\nTherearemultipleoptionsforrecordingaudio, andthebestsolutiondependsonboththehardwareandthe\\noperating system used. We use the Recognizer class from the Python package SpeechRecognition [4] to\\nperform the recording only, while leaving the actual speech recognition to OpenAIs Whisper. There is an\\nopen version of this available for download through the Python package openai-whisper [5].\\n20.3 Text to structured data\\nOnce the speech has been converted to text we need to use this text to i) select the appropriate schema\\nand ii) fill the schema. For this, we make use of a common feature in LLMs called “function calling”, “API\\ncalling”,or“tooluse”. ThishasbeendevelopedtoallowLLMstomakevalidAPIcallsand,sinceOpenAPIis\\nvalidatedbyit,usesJSONSchematodefinethepossible“functions”,or“tools”,thatcanbeoutputted. Since\\nweuseJSONSchematodefineourdataschemaswecansimplysupplytheseasthetoolsfortheLLMtouse.\\nOne way to do this in Python is to write a Pydantic schema for each of the data schemas and then convert\\nthis to a JSON Schema. For the Llama model we used, the python package langchain-experimental [6]\\nhas an OllamaFunctions class that can be imported from the llms.ollama functions sub-package. This\\nclass can then be used to instantiate the model and add the valid data schemas. Here is an example using\\nLangChain and Llama3:\\nfrom langchain experimental.llms.ollama functions import OllamaFunctions\\nmodel = OllamaFunctions(model=”llama3:70b”, base url = ’... ’ , format=’json ’)\\nmodel = model. bind tools(\\ntools=[\\n{\\n”name”: ”solution preparation”,\\n”description ”: ”Schema for solution preparation”,\\n”parameters”: SolutionPreparation.schema(),\\n65\\n},\\n{\\n”name”: ”powder scaling”,\\n”description ”: ”Schema for powder scaling”,\\n”parameters”: Scaling.schema(),\\n}\\n] ,\\n)\\nWhere SolutionPreparation and Scaling are Pydantic models for our desired data schemas. Finally,\\nthiscanbechainedtogetherwitha prompt templateandusedto process thetranscribed audio frombefore:\\nprompt = PromptTemplate.from template (...)\\nchain with tools = prompt | model\\nresponse = chain with tools .invoke(transcribed audio)\\nFor langchain the selected schema can be retrieved from:\\nschema = response.additional kwargs[ ’ function call ’][ ’name’]\\nAnd the filled instance from:\\ninstance = json.loads(\\nresponse.additional kwargs[ ’ function call ’][ ’arguments ’])\\nFor LangChain the selected schema can be retrieved from:\\nschema = response.additional kwargs[ ’ function call ’][ ’name’]\\nAnd the filled instance from:\\ninstance = json.loads(\\nresponse.additional kwargs[ ’ function call ’][ ’arguments ’])\\nAdetailedexampleofthetext-to-structureddatacanbefoundasaniPythonnotebookongithub.com/\\nhampusnasstrom/speech-schema-filling together with an implementation of the audio recording and\\ntranscribing using Whisper. In conclusion, we believe that LLMs can be useful in labs where traditional\\nELNs are hard to operate by transcribing speech, selecting appropriate schemas, and filling the schemas to\\nultimately create structured data directly from speech.\\n20.4 Acknowledgements\\nH.N., J. S., and J. A. M. are part of the NFDI consortium FAIRmat funded by the Deutsche Forschungsge-\\nmeinschaft (DFG, German Research Foundation) – project 460197019.\\nReferences\\n[1] Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. The FAIR Guiding Principles for scientific data\\nmanagement and stewardship. Sci Data 3, 160018 (2016) https://doi.org/10.1038/sdata.2016.18.\\n[2] https://json-schema.org/\\n[3] Scheidgen et al., (2023). NOMAD: A distributed web-based platform for managing materials science\\nresearchdata.JournalofOpenSourceSoftware,8(90),5388,https://doi.org/10.21105/joss.05388.\\n[4] https://pypi.org/project/SpeechRecognition/.\\n[5] https://pypi.org/project/openai-whisper/.\\n[6] https://pypi.org/project/langchain-experimental/.\\n66\\nFigure 28: a) Part of a JSON Schema defining a data structure for a solution preparation. b) The schema\\nconverted to an ELN form in NOMAD [3].\\n67\\n21 Leveraging LLMs for Bayesian Temporal Evaluation of Scien-\\ntific Hypotheses\\nAuthors: Marcus Schwarting\\n21.1 Introduction\\nScience is predicated on empiricism, and a scientist uses the tools at their disposal to gather observations\\nthat support the veracity of their claims. When one cannot gather firsthand evidence for a claim, they must\\nrely on their own assessment of evidence presented by others. However, the scientific literature on claim\\nis often large and dense (particularly for those without domain expertise), and the scientific consensus on\\nthe veracity of a claim may drift over time. In this work we consider how large language models (LLMs),\\nin conjunction with temporal Bayesian statistics, can rapidly provide a more holistic view of a scientific\\ninquiry. WedemonstrateourapproachonthehypothesisthatthematerialLK-99, whichwentviralafterits\\ndiscovery in April 2023, is in fact a room-temperature superconductor.\\n21.2 Background\\nScientific progress requires a researcher to iteratively update their prior understanding based on new obser-\\nvations. The process of updating a statistical prior based on new information is the backbone of Bayesian\\nstatistics and is routinely used in scientific workflows [1]. Under such a model, a hypothesis has an inferred\\nprobability P ∈(0,1) of being true and will never be completely discarded (P =0) or accepted (P =1)\\nH H H\\nbut may draw arbitrarily close to these extrema. This inferred probability from a dataset is also a feature\\nof assessing the power of a claim using statistical hypothesis tests [2], which are commonly used across most\\nscientific disciplines.\\nModelling the veracity of a scientific claim as a probability also stems from the work of philosopher\\nKarl Popper [3]. Popper posits that a scientific claim should be falsifiable, such that it can be refuted by\\nempirical evidence. If a scientific claim cannot be refuted, Popper argues that it is not proven, but gains\\nfurther credibility from the scientific community. Scientific progress is not made by proving claims, but\\ninstead by hewing away false claims through observation until only those that withstand repeated scrutiny\\nremain.\\nThe history of science is littered with discarded hypotheses. Some of these claims were believed to be\\ntrue for centuries before being dismissed (including geocentrism, phrenology, energeticism, and spontaneous\\ngeneration). Inthiswork,wefocusonaclaimbyLee,Kim,andKwonthattheyhadsuccessfullysynthesized\\naroom-temperaturesuperconductor,whichtheycalledLK-99[4]. Suchamaterialwouldnecessitatealtering\\ntheexistingtheoryofsuperconductorsatafundamentallevelandwouldenableinnovationsthatarecurrently\\nbeyond reach. LK-99 went viral in summer 2023 [5], but replication efforts quickly ran into issues [6]. Since\\ntheinitialclaimwasmadeinApril2023,roughly160workshavebeenpublished,andthescientificconsensus\\nnow appears established: LK-99 is not a room-temperature superconductor.\\n21.3 Methods\\nOur dataset consists of the 160 papers on Google Scholar, ordered by publication date, that are deemed\\nrelevant to the hypothesis “LK-99 is a room-temperature superconductor.” For each paper abstract in\\nour dataset, we construct an LLM prompt as follows to perform a zero-shot natural language inference\\noperation [7]:\\nGivenaHypothesisandanAbstract,determinewhethertheHypothesisisan‘entailed’,‘neutral’,\\nor ‘contradicted’ by the Abstract. \\\\n Hypothesis: {claim} \\\\n Abstract: {abstract}\\nWethenwrotearegularexpressiontochecktheLLMresponsetomakeanassertionforeachpublication:\\n“entailment,” “neutrality,” or “contradiction.” We use Llama2 to make these assertions on all 160 papers,\\nwhich complete in under five minutes (on a desktop with an Nvidia GTX-1070 GPU).\\nNext,weconstructedatemporalBayesianmodelwhere,startingfromaninitialGaussianpriorN(µ ,σ2)\\nP P\\nthatmodelsthelikelihoodofacceptingthehypothesis,wecanupdatewithaGaussianlikelihoodN(µ ,σ2).\\nL L\\n68\\nOur likelihood is a Gaussian designed so that for a given publication, “entailment” acceptance probability\\nhigher, “contradiction” pushes the acceptance probability lower, and “neutrality” leaves the acceptance\\nprobability the same. Our likelihood probability is further weighted according to the impact factor of the\\njournal, with an impact factor floor set to accommodate publications that have not passed peer review.\\nRetroactively, we could also weight by the number of citations, however we omit this feature in our analysis\\nsincethisisinherentlyapost-hocmetricandwouldviolateourtemporalassessment. WeupdateourGaussian\\nprior using the equations [8]:\\n(cid:18) 1 1 (cid:19)−1(cid:18) µ µ (cid:19) (cid:18) 1 1 (cid:19)−1\\nµ ← + P + L ; σ2 ← +\\nP σ2 σ2 σ2 σ2 P σ2 σ2\\nP L P L P L\\nWe are also able to specify the initial probability associated with the hypothesis (µ ), as well as how\\nP\\nflexible we are in changing our perspective based on new evidence (σ2). We select two initial probabilities:\\nP\\n50% and 20%, and fix standard deviations σ2 and σ2. Assuming either “contradiction” or “entailment”,\\nP L\\nthe update due to µ then scales linearly with the publication impact factor. Finally, we can compare our\\nL\\ntemporal probability assessment with probabilities provided by the betting platform Manifold Markets [9],\\nwhere players bet on the outcome of a successful replication of the LK-99 publication results.\\n21.4 Results\\nWe find that our temporal Bayesian probabilities, with an adjusted initial prior, can mirror the Manifold\\nMarketsprobabilitieswithtwointerestingdivergences. Whiletheinitialprobabilitystartsataround20%for\\nboth,thetemporalBayesianapproachnevergoesabove30%. Bycontrast,thebettingmarket,followingthe\\nhype and virality of the LK-99 publication, reaches a peak at around 60%. Furthermore, while the betting\\nmarket has a long tail of low probability starting in mid-August 2023, our approach more quickly disregards\\nthe hypothesis based on a continuing accumulation of studies showing that the LK-99 findings could not be\\nreplicated. Our temporal Bayesian model with the adjusted initial prior reaches a probability below 1% by\\nmid-September2023,butneverentirelydismissesthechancethatthehypothesisistrue. Figure29showcases\\nthese results.\\nWhile we specifically select initial probabilities or 20% and 50%, both trajectories end with a steadily\\nshrinking probability of accepting the hypothesis. Our initial prior probability of 50% mimics an unbiased\\nobserverwithnoknowledgeaboutwhetherthehypothesisshouldbeacceptedorrejected. Apriorprobability\\nof 20% could be considered a reasonable guess for an observer biased by a baseline understanding and\\nsuspicion of the claims and their corresponding evidence. Such an initial guess is admittedly subjective,\\nas is the degree to which new information affects one’s inherent biases. We treat these settings as presets,\\nhowever these are trivial for others to configure and assess based on their background. Finally, for claims\\nwith established scientific consensus, our approach is guaranteed to asymptotically approach that consensus\\nwhere the rate of convergence varies according to these initial presets.\\nFigure 29: Likelihood of accepting the hypothesis “LK-99 is a room-temperature superconductor” via three\\napproaches, from April 15, 2023 to April 15, 2024. The unadjusted initial probability (set to 50%) is shown\\nin gray, the adjusted initial probability (set to 20%) is shown in black, and the probability according to the\\nManifold Markets online betting platform is shown in red.\\n69\\n21.5 Conclusion\\nCarefully validating a claim based a body of scientific literature can be a time-consuming and challenging\\nprospect,especiallywithoutdomainexpertise. Inthiswork,wedemonstratehowaclaimmightbeevaluated\\nusing a temporal Bayesian model based on a literature evaluation using natural language inference.\\nWeshowhowouraggregatedliteraturepredictionsallowustoquicklyrejectthehypothesisthatLK-99is\\naroom-temperaturesuperconductor. Inthefuture,wehopetoapplythisapproachtootherscientificclaims,\\nincluding those with debates that are ongoing and as well as claims have an established scientific consensus.\\nIn general, we hope this approach will allow a researcher to quickly measure the scientific community’s\\nconfidence in a claim, as well as aid the public in assessing both the veracity of a claim and the change in\\nconfidence driven by continued experimentation and observation.\\nReferences\\n[1] Settles, Burr. ”Active learning literature survey.” (2009).\\n[2] Lehmann,ErichLeo,JosephP.Romano,andGeorgeCasella.Testing statistical hypotheses.Vol.3.New\\nYork: springer, 1986.\\n[3] Popper, Karl. The logic of scientific discovery. Routledge, 2005.\\n[4] Lee, Sukbae, Ji-Hoon Kim, and Young-Wan Kwon. ”The first room-temperature ambient-pressure su-\\nperconductor.” arXiv preprint arXiv:2307.12008 (2023).\\n[5] Chang, Kenneth. “LK-99 Is the Superconductor of the Summer.” New York Times (2023).\\n[6] Garisto,Dan.”ClaimedsuperconductorLK-99isanonlinesensation—Butreplicationeffortsfallshort.”\\nNature 620, no. 7973 (2023): 253-253.\\n[7] Liu, Hanmeng, Leyang Cui, Jian Liu, and Yue Zhang. ”Natural language inference in context-\\ninvestigating contextual reasoning over long texts.” In Proceedings of the AAAI conference on artificial\\nintelligence, vol. 35, no. 15, pp. 13388-13396. 2021.\\n[8] Murphy, Kevin P. ”Conjugate Bayesian analysis of the Gaussian distribution.” def 1, no. 2σ 2 (2007):\\n16.\\n[9] “Will the LK-99 room temp superconductivity pre-print replicate in 2023”. Manifold Markets (2024).\\nhttps://manifold.markets/Ernie/will-the-lk99-room-temp-ambient-pre-17fc7cb7a2a0.\\n70\\n22 Multi-Agent Hypothesis Generation and Verification through\\nTree of Thoughts and Retrieval Augmented Generation\\nAuthors: Aleyna Beste Ozhan, Soroush Mahjoubi\\n22.1 Introduction\\nOur project, developed during the “LLM Hackathon for Applications in Materials and Chemistry,” aims to\\naccelerate scientific hypotheses and enhance the creativity of scientific inquiry. We propose using a multi-\\nagent system of specialized large language models (LLMs) to streamline and enrich hypothesis generation\\nand verification in materials science. This approach leverages diverse, fine-tuned LLM agents collaborating\\ntogenerateandvalidatenovelhypothesesmoreeffectively. Whilesimilarpipelineshavebeenprovenusefulin\\nthesocialsciences[1],tothebestofourknowledge,thisworkmarksthefirstadaptationofsuchanapproach\\ntohypothesisgenerationinmaterialsscience. AsillustratedinFigure30,Thesystemincludesagentssuchas\\na background provider, an inspiration generator, a hypothesis generator, and three evaluators. Each agent\\nplays a crucial role in formulating and assessing hypotheses, ensuring only the most viable and compelling\\nideas are developed. This innovative approach fosters an environment conducive to scientific inquiries.\\nFigure 30: Multi-Agent Hypothesis Generation and Verification Pipeline\\n22.2 Methodology\\nBackgroundExtraction Thebackgroundextractionmoduleisdesignedtosearchthroughavastdatabase\\nforrelevantinformationdirectlyrelatedtotheuser’squery. Thismoduleemploysadvancedembedding-based\\nretrieval techniques to identify and extract pertinent corpus. As new papers and findings are added to the\\nrepository, the system dynamically updates, ensuring the use of the most current and relevant information.\\nInspiration Generator Agent The inspiration generator agent leverages extensive background data to\\neffectively formulate inspirations using a Retrieval Augmented Generation (RAG) mechanism. Serving as\\n71\\nthestrategiccoreofthehypothesisgenerationprocess,itdrawsinspirationfromabroadspectrumofsources\\ntospawndiversehypotheses,similartothebranchingstructureofa”TreeofThoughts(ToT)”[2]. Theagent\\nsamples ”k” candidates as possible solutions, evaluates their effectiveness through self-feedback, and votes\\non the most promising candidates. The selection is narrowed down to ”b” promising options per step, with\\nthis structured approach helping the agent systematically refine its solutions.\\nHypothesisGenerator Basedonthebackgroundinformationandtheinspirations,thismodulegenerates\\nmeaningful research hypotheses. It is fine-tuned on reasoning datasets, such as Atlas, which encompasses\\nvarioustypesofreasoningincludingdeductiveandinductivereasoning,cognitivebiases,decisiontheory,and\\nargumentative strategies [3].\\n22.3 Evaluator Agents\\nOnce a hypothesis is generated, a RAG mechanism is used to fetch relevant abstracts from the dataset to\\nevaluate the hypothesis. The evaluation is based on three critical aspects adopted from existing literature\\nfor the materials science domain:\\nFeasibility: Assesses whether the hypothesis is realistically grounded and achievable based on current\\nscientific knowledge and technology.\\nUtility: Evaluates the practical value of the hypothesis, considering its potential to solve problems,\\nenhance experimental design, or lead to beneficial exploratory paths.\\nNovelty: Measures the uniqueness and originality of the hypothesis, encouraging the generation of inno-\\nvative ideas that advance scientific understanding.\\n22.4 Case Study\\nOurcasestudyfocusesonsustainablepracticesinconcretedesignatthematerialsupplylevel. Weprocessed\\n66,000 abstracts related to cement and concrete, converting them into a Chroma vector database using\\nthe sentence transformer all-MiniLM-L6-v2 [4]. This model maps abstracts to a 384-dimensional dense\\nvectorspace. Wethenqueriedtheembedding-basedretrievalsystemwithquestionsrelatedtomaterial-level\\nsolutions for sustainability in concrete, retrieving 10,000 relevant abstracts.\\nExample Query: ”How can we incorporate industrial wastes and byproducts as supplementary cemen-\\ntitious materials to promote sustainability while maintaining its fresh and hardened properties such as\\nstrength?”\\n22.5 Results\\nIn the initial phase of our “Tree of Thoughts” structure, we generated approximately 5,000 inspirations.\\nTheseinspirationswererefinedtoaround1,000throughadistillationstep. Thehypothesisgenerator, GPT-\\n3.5 Turbo, fine-tuned on 13,000 data points from the AtlasUnified/Atlas-Reasoning dataset, produced one\\nhypothesis per inspiration. The evaluation process involved three agents assessing feasibility, utility, and\\nnovelty(FUN)usingembedding-basedretrievaltoidentifyrelevantabstracts. Forenhancedprecision,GPT-\\n4 was employed during the evaluation stages. Ultimately, hypotheses that withstood all evaluation stages\\nwere included in the hypothesis pool. Out of the initial 1,000 hypotheses, 243 passed the feasibility filter,\\n175 were deemed useful, and only 12 were found to be highly novel. The 12 hypotheses deemed feasible,\\nnovel, and useful are listed in Table 5.\\n22.6 Future Directions: Adaptability to Other Material Systems and Cross-\\nDomain Applications\\nTo apply it to another material system, the first background query would be modified to target the new\\nmaterialofinterest,andadatabaserelevanttothatmaterialsystemwouldbeemployed. Also,thisframework\\nis not limited to materials science; it can be applied across various domains. For example, ideas generated\\nfrom civil engineering could inspire hypotheses in materials science. A background provider querying civil\\nengineering databases might produce inspirations that, when evaluated by our multi-agent system, lead to\\n72\\ninnovative hypotheses in materials science. Similarly, within the domain of materials science, inspirations\\ngenerated based on concrete research could be used to develop hypotheses for other materials, such as\\nceramics or composites. This cross-pollination of ideas can foster creativity and drive breakthroughs by\\napplying concepts from one domain to another.\\nTable 5: Final hypothesis pool for the study of Section 22\\nNo. Hypothesis\\n1 Incorporating Stainless Steel (SS) micropowder from additive manufacturing into cement paste\\nmixtures can improve the mechanical strength and durability of the mixture, with an optimal\\naddition of 5% SS micropowder by volume.\\n2 Theuseofsynthesizedzeolitesinself-healingconcretecansignificantlyimprovethedurabilityand\\nlongevity of concrete structures.\\n3 The utilization of municipal solid waste (MSW) in cement production by integrating anaerobic\\ndigestion and mechanical-biological treatment to produce refuse-derived fuel (RDF) for cement\\nkilnscanreduceenvironmentalimpacts,establishasustainablewaste-to-energysolution,andcreate\\naclosed-loopprocessthatalignswastemanagementwithcementproductionforamoresustainable\\nfuture.\\n4 The use of smart fiber-reinforced concrete systems with embedded sensing capabilities can revolu-\\ntionize infrastructure monitoring and maintenance by providing real-time feedback on structural\\nhealth, leading to safer and more resilient built environments.\\n5 The use of advanced additives or nanomaterials in geopolymer well cement can enhance its me-\\nchanical properties and durability, leading to more reliable CO2 sequestration projects.\\n6 The use of carbonated steel slag as an aggregate in concrete can enhance the self-healing perfor-\\nmance of concrete, leading to improved durability and longevity.\\n7 The synergistic effect of combining different pozzolanic materials with varying particle sizes and\\nreactivities can lead to the development of novel high-performance concrete formulations with\\nsuperior properties.\\n8 Smart bio concrete incorporating bacterial silica leaching exhibits superior strength, durability,\\nand reduced water absorption capacity compared to traditional concrete.\\n9 Noveleco-concreteformulationdevelopedbycombiningcarbonated-aggregateswithothersustain-\\nable materials like volcanic ash or limestone powder can create a carbon-negative concrete with\\nsuperior mechanical strength, durability, and thermal conductivity.\\n10 The use of nano-enhanced steel fiber reinforced concrete (NSFRC) will result in a significant\\nimprovement in the mechanical properties, durability, and crack resistance of concrete structures\\ncompared to traditional steel fiber reinforced concrete.\\n11 The combined addition of silica fume (SF) and nano-silica (NS) can further enhance the sulphate\\nand chloride resistance to higher than possible with the single addition of SF or NS.\\n12 The utilization of oil shale fly ash (OSFA) in concrete production can be optimized to develop\\nsustainable and high-performance construction materials.\\nReferences\\n[1] Yang, Zonglin, et al. ”Large Language Models for Automated Open-domain Scientific Hypotheses Dis-\\ncovery.” arXiv preprint arXiv:2309.02726 (2023). https://arxiv.org/pdf/2309.02726\\n73\\n[2] Yao,Shunyu,etal.”Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels.”Advances\\nin Neural Information Processing Systems 36 (2024).\\n[3] https://huggingface.co/datasets/AtlasUnified/Atlas-Reasoning/commits/main.\\n[4] https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2.\\n74\\n23 ActiveScience\\nAuthors: Min-Hsueh Chiu\\n23.1 Introduction\\nHumans have conducted material research for thousands of years, yet the vast chemical space, estimated\\nto encompass up to 1060 compounds, remains unexplored. Traditional research methods often focus on\\nincremental improvements, making the discovery of new materials a slow process. As data infrastructure\\nhas developed, data mining techniques have increasingly accelerated material discovery. However, three\\nsignificantobstacleshinderthisprocess: First,theavailabilityandsparsityofdatapresentamajorchallenge.\\nComprehensive and high-quality datasets are essential for effective data mining, yet materials science suffer\\nfrom limited data availability. Second, each database typically consists of specific types of quantitative\\nproperties, which may not fully meet researchers’ needs. This fragmentation and specialization of databases\\ncan impede the holistic analysis necessary for breakthrough discoveries. Third, scientists usually focus on\\ncertain materials and related articles, decreasing the likelihood of deeply exploring diverse literature that\\nreportspotentialmaterialsorapplicationsnotyetutilizedintheirspecificfield. Thissiloedapproachfurther\\nlimits the scope of discovery and innovation.\\nUnliketheintrinsicpropertiesfoundindatabases,scientificarticlesprovideunstructuredbuthigher-level\\ninformation, such as applications, material categories, and properties that might not be explicitly recorded\\nin databases. Additionally, these texts include inferences and theories proposed by domain experts, which\\nare crucial for guiding research directions. The challenge lies in automatically extracting and digesting this\\nunstructured text into actionable insights. This process typically requires experienced experts, creativity,\\nand a measure of luck to identify the next desirable candidate. These challenges motivate the potential\\nof utilizing large language models to parse scientific reports and integrate the extracted information into\\nknowledge graphs, thereby constructing high-level insights.\\n23.2 Approach\\nThe Python-based ActiveScience framework consists of three key functionalities: data source API, large\\nlanguage model, and graph database. LangChain is employed for downstream applications within this\\nframework. The schematic pipeline is illustrated in Figure 1.Notably, this framework is not restricted to the\\nspecificpackagesorAPIsusedinthisdemonstration;alternativetoolsthatprovidetherequiredfunctionality\\nand input/output can also be integrated.\\nArXivAPIswereusedtoretrievescientificreporttitles,abstracts,andURLs. Thisdemonstrationfocused\\non reports related to alloys. Consequently, the string “cat:cond-mat.mes-hall AND ti:alloy” was queried in\\nthe ArXiv APIs, which returned the relevant articles. GPT-3.5 Turbo models was used to access the large\\nlanguagemodel. Thesystem’srolewasdefinedas: “Youareamaterialscienceprofessorandwanttoextract\\ninformation from the paper’s abstract.” The provided prompt was: “Given the abstract of a paper, can you\\ngenerate a Cypher code to construct a knowledge graph in Neo4j? ...” along with the designated ontology\\nschema. ThegeneratedCyphercodeistheninputintoNeo4j,whichisusedtoingesttheentityrelationships,\\nstore the knowledge graph, and provide robust visualization and querying interfaces.\\n23.3 Results\\nWith the implemented knowledge graph, GraphCypherQAChain module from LangChain was emploied to\\nperformretrieval-augmentedgeneration. Forinstance,whenasked,“Givemethetop3referencesURLswhere\\nthe Property contains ’opti’?” GraphCypherQAChain automatically generates a Cypher query according to\\nthe designated schema, executes it in Neo4j, and ultimately returns the relevant answer, as shown in the\\nright bottom box in Figure 31. Although this demonstration used a simple question, more complex queries\\ncan be processed using this framework. However, handling such queries effectively will require more refined\\nprompting techniques.\\n75\\nFigure31: ASchematicillustrationofActiveSciencearchitectureanditspotentialapplications. Codesnippet\\ndemonstrating the use of LangChain.\\n23.4 Conclusion and future works\\nThe work demonstrates the pipeline of integrating large language model, knowledge graph, and a question-\\nanswering interface. Several aspects of this framework can be enhanced. Firstly, using domain-specific\\nlanguage models in the materials domain could improve entity and relationship recognition. Secondly,\\nenhancing entity resolution and data normalization could lead to a more concise and informative knowledge\\ngraph, thereby improving the quality of answers. Thirdly, designing more effective prompting strategies,\\nsuch as chain-of-thought prompting, could enhance the quality of both answers and code generation.\\n23.5 Data and Code Availability\\nAll code and data used in this study are publicly available in the following GitHub repository: https:\\n//github.com/minhsueh/ActiveScience\\n76\\n24 G-Peer-T: LLM Probabilities For Assessing Scientific Novelty\\nand Nonsense\\nAuthors: Alexander Al-Feghali, Sylvester Zhang\\nLarge language models (LLMs) and foundation models have garnered significant attention lately due to\\ntheir natural language programmability and potential to parse high-dimensional data from reactions to the\\nscientific literature [1]. While these models have demonstrated utility in various chemical and materials\\nscience applications, we propose leveraging their designed strength in language processing to develop a first\\npass peer review system for materials science research papers [2].\\nTraditional-gram tests such as BLEU or ROUGE, as well as X-of-thought LLM-based evaluations, are\\nnot sensitive enough for creativity or diversity in scientific writing [3]. Our approach utilizes the learned\\nprobabilistic features to establish a baseline for typical scientific language in materials science, based on\\nfine-tuning on materials science abstracts through a historical train-test split. New abstracts are scored by\\ntheir weighted-average probabilities, identifying those that deviate from the expected norms, flagging both\\npossibly innovative or potentially nonsensical works.\\nAs a proof-of-concept in this direction, we fine-tuned two models: OPT (6.7B) and TinyLLama (1.1B)\\nusing the Huggingface PEFT library’s Low Rank Adapters (LoRA) to access the log probabilities of the\\nabstracts, which is not typically accessible for modern API services [4–6]. Our results come with the usual\\ncaveats for small models with small computational costs.\\nWecuratedadatasetof6000abstractsfromPubMed,publishedbetween2017–2020,focusingonMaterials\\nScience and Chemistry [7]. The models were fine-tuned over 200 steps using this dataset. We compared\\nhighly cited papers (>200 citations) with those of average citation counts. Our preliminary findings suggest\\nthat higher-cited papers exhibit less “typical” language use, with mean log probabilities of -2.24 ± 0.32 for\\nhighly cited works compared to -1.79 ± 0.3 for average papers. However, the calculated p-value of 0.07\\nindicates that these results are not statistically significant at the conventional 0.05 level.\\nFulltrainingwithmorestepsonlargermodels,aswellasmoreexperimentationandmethodoptimization,\\nwould yield more reliable results and be of modern relevance. Our documented code with step-by-step\\ninstructions is available in the repository [8].\\nReferences\\n[1] K. M. Jablonka, P. Schwaller, A. Ortega-Guerrero, B. Smit, Nat. Mach. Intell., 2024, 6, 161–169.\\n[2] D. A. Boiko, R. MacKnight, B. Kline, G. Gomes, Nature, 2023, 624, 570–578.\\n[3] Y. Liu, D. Iter, Y. Xu, S. Wang, R. Xu, C. Zhu, Proc. 2023 Conf. Empir. Methods Nat. Lang. Process.,\\n2023, 2511–2522.\\n[4] S.Mangrulkar,S.Gugger,L.Debut,Y.Belkada,S.Paul,B.Bossan,PEFT:State-of-the-artParameter-\\nEfficient Fine-Tuning Methods; GitHub: https://github.com/huggingface/peft, 2022.\\n[5] P. Zhang, G. Zeng, T. Wang, W. Lu, TinyLlama: An Open-Source Small Language Model;\\narXiv:2401.02385.\\n[6] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, T.\\nMihaylov, M. Ott, S. Shleifer, K. Shuster, D. Simig, P. S. Koura, A. Sridhar, T. Wang, L. Zettlemoyer,\\nOPT: Open Pre-trained Transformer Language Models; arXiv:2205.01068.\\n[7] National Center for Biotechnology Information (NCBI) [Internet]. Bethesda (MD): National Library\\nof Medicine (US), National Center for Biotechnology Information; https://www.ncbi.nlm.nih.gov/,\\n1998.\\n[8] A. Al-Feghali, S. Zhang, G-Peer-T; GitHub: https://github.com/alxfgh/G-Peer-T, 2024.\\n77\\n25 ChemQA: Evaluating Chemistry Reasoning Capabilities of Multi-\\nModal Foundation Models\\nAuthors: Ghazal Khalighinejad, Shang Zhu, Xuefeng Liu\\n25.1 Introduction\\nCurrentfoundationmodelsexhibitimpressivecapabilitieswhenpromptedwithtextandimageinputsinthe\\nchemistry domain. However, it is essential to evaluate their performance on text alone, image alone, and a\\ncombinationofbothtofullyunderstandtheirstrengthsandlimitations. Inchemistry,visualrepresentations\\noften enhance comprehension. For instance, determining the number of carbons in a molecule is easier for\\nhumans when provided with an image rather than SMILES annotations. This visual advantage underscores\\nthe need for models to effectively interpret both types of data. To address this, we propose ChemQA—a\\nbenchmark dataset containing problems across five question-and-answering (QA) tasks. Each example is\\npresented with isomorphic representations: visual (images of molecules) and textual (SMILES). ChemQA\\nenables a detailed analysis of how different representations impact model performance.\\nWeobservefromourresultsthatmodelsperformbetterwhengivenbothtextandvisualinputscompared\\nto when they are prompted with image-only inputs. Their accuracy significantly decreases when provided\\nwith only visual information, highlighting the importance of multimodal inputs for complex reasoning tasks\\nin chemistry.\\n25.2 ChemQA: A Benchmark Dataset for Multi-Modal Chemical Understand-\\ning\\nInspired by the existing work of IsoBench [2] and ChemLLMBench [3], we create a multimodal question-\\nand-answering dataset on chemistry reasoning, ChemQA [5] containing five QA tasks:\\n• Counting Numbers of Carbons and Hydrogens in Organic Molecules: adapted from the 600\\nPubChem molecules created by [3], evenly divided into validation and evaluation datasets.\\n– Example: Given a molecule image or its SMILES notation, identify the number of carbons and\\nhydrogens.\\n• CalculatingMolecularWeightsinOrganicMolecules: adaptedfromthe600PubChemmolecules\\ncreated by [3], evenly divided into validation and evaluation datasets.\\n– Example: Given a molecule image or its SMILES notation, calculate its molecular weight.\\n• Name Conversion: From SMILES to IUPAC:adaptedfromthe600PubChemmoleculescreated\\nby [3], evenly divided into validation and evaluation datasets.\\n– Example: Convert a given SMILES string or a molecule image to its IUPAC name.\\n• Molecule Captioning and Editing: inspired by [3], adapted from the dataset provided in [1],\\nfollowing the same training, validation, and evaluation splits.\\n– Example: Given a molecule image or its SMILES notation, find the most relevant description of\\nthe molecule.\\n• Retro-synthesis Planning: inspired by [3], adapted from the dataset provided in [4], following the\\nsame training, validation, and evaluation splits.\\n– Example: GivenamoleculeimageoritsSMILESnotation,findthemostlikelyreactantsthatcan\\nproduce the molecule.\\n78\\nFigure 32: Performance of Gemini Pro, GPT-4 Turbo, and Claude3 Opus on text, visual, and text+visual\\nrepresentations. The plot shows that models achieve higher accuracy with combined text and visual inputs\\ncompared to visual-only inputs.\\n25.3 Evaluating Foundation Model Performances on ChemQA\\nThe model evaluation results are shown in Figure 32. According to the plot, models perform better with\\ntextandvisualinputscombined,whiletheiraccuracydropswhengivenonlyvisualinputs. Claudeperforms\\nwell in text-only tasks, whereas Gemini and GPT-4 Turbo perform better with visual or combined inputs.\\nThis highlights the importance of evaluating models with different input modalities to understand their full\\ncapabilities and limitations.\\n25.4 Conclusion and Future Work\\nThe evaluation of multimodal language models in the chemistry domain demonstrates that the integration\\nof both text and visual inputs significantly enhances model performance. This suggests that for complex\\nreasoning tasks in chemistry, multimodal approaches are more effective than relying on a single type of\\ninput. Future work should continue to explore and refine these multimodal strategies to further improve the\\naccuracy and applicability of AI models in specialized fields such as chemistry.\\nReferences\\n[1] CarlEdwards,TuanLai,KevinRos,GarrettHonke,KyunghyunCho,andHengJi.Translationbetween\\nmolecules and natural language. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Pro-\\nceedingsofthe2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages375–413,\\nAbu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi:\\n10.18653/v1/2022.emnlp-main.26. URL https://aclanthology.org/2022.emnlp-main.26.\\n[2] Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie\\nNeiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations,\\n2024. URL https://arxiv.org/abs/2404.01266.\\n79\\n[3] Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V. Chawla, Olaf Wiest,\\nand Xiangliang Zhang. What can large language models do in chemistry? a comprehensive benchmark\\non eight tasks, 2023. URL https://arxiv.org/abs/2305.18365.\\n[4] Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-trained\\ntransformerforcomputationalchemistry.MachineLearning: ScienceandTechnology,3(1):015022,Jan-\\nuary 2022. doi: 10.1088/2632-2153/ac3ffb. URL https://dx.doi.org/10.1088/2632-2153/ac3ffb.\\n[5] Shang Zhu, Xuefeng Liu, and Ghazal Khalighinejad. ChemQA: a multimodal question-and-answering\\ndataset on chemistry reasoning. https://huggingface.co/datasets/shangzhu/ChemQA, 2024.\\n80\\n26 LithiumMind - Leveraging Language Models for Understand-\\ning Battery Performance\\nAuthors: Xinyi Ni, Zizhang Chen, Rongda Kang, Sheng-Lun Liao, Pengyu Hong, Sandeep\\nMadireddy\\n26.1 Introduction\\nIn this project, we explore multiple applications of Large Language Models (LLMs) in analyzing the perfor-\\nmance of lithium metal batteries. Central to our investigation is the concept of Coulombic efficiency (CE),\\na critical measure in battery technology that assesses the efficiency of electron transfer during a battery’s\\nchargeanddischargecycles. ImprovingtheCEisessentialforadvancingtheadoptionofhigh-energy-density\\nlithium metal batteries, which are crucial for next-generation energy storage solutions. A key focus of our\\nanalysis is the role of the liquid electrolyte engineering, which is instrumental in determining the battery’s\\ncyclability and improving the CE.\\nWeexploredtwomethodsofintegratingLLMasasupplementaltoolinbatteryresearch. First,weutilize\\nLLMs as information extractors to distill structural knowledge essential for the design of electrolytes from\\nthevastamountofpaper. Second, weintroduceanLLM-poweredchatbotspecificallytailoredtorespondto\\ninquiries related to lithium metal batteries. We believe these applications of LLMs may potentially enhance\\nour capabilities in battery innovation, streamlining research processes and increasing the accessibility of\\nspecialized knowledge. Our code is available at: https://github.com/KKbeckang/LiGPT-Beta.\\n26.2 Investigation 1: Structural Knowledge Extraction\\nMotivation In this project, we utilize LLM to extract CE-electrolyte pairs from over 70 papers. A recent\\nstudy [1] (Dataset S01) provides a reliable database about the relationship between Coulombic Efficiency\\n(CE) and battery electrolytes, along with a list of relevant papers. The data was filtered and cleaned by\\nhuman experts. We managed to automate the data collection procedure through LithiumMind and aimed\\nto discover more relationships from the paper list. The pipeline is describe in the left column of Figure 1.\\nMethod The pipeline consists of the following steps:\\n• Parse PDF: The raw papers were saved as PDF files and loaded into text using a PDF parser.\\n• Retrieve Relevant Context: Instead of extracting information directly from the whole paper, we\\ningest the documents into a vector datastore. We combined three domain-specific embedding models\\n- MaterialBERT, ChemBERT, and OpenAI - together as a powerful retriever through the LangChain\\nLOTR module. The retriever finds 10 text chunks for each paper that are most relevant to Coulombic\\nEfficiency.\\n• CEExtraction: Wedefinedtheschemaforextractingkeyinformation,includingCoulombicEfficiency\\nandthesolventandsaltoftheelectrolyte. Weprovidedfew-shotin-contextinstructiontoGPT-4Turbo\\nJSON mode. The extracted output is saved in JSON format.\\nPreliminary Results TheLLMextracted334CE-electrolytepairsin71papers,whiletheoriginalpaper\\nfound 152 pairs. Since it is difficult to verify our results without the help of human experts, we filtered\\nthe extracted pairs through the Coulombic Efficiency and found 46 matches to the original dataset. We\\nclassified these verifiable data into three categories: correct (the extracted data exactly matches the human-\\nlabeled data); incorrect (the types or amounts of the solvent/salt do not match the labels); and unknown\\n(the extracted data provides more details than the human-labeled data). For example, the label only shows\\nthe types of the salts, but the extracted data contains not only the correct types but also the mix ratio of\\ndifferent salts. The results are shown in Table 6:\\n81\\nTable 6: Results\\nCorrect Incorrect Unknown\\n72.4% 23.4% 4.2%\\nFigure 33: Summary of our LLM hackathon project.\\nFuture Work One major challenge is the low recall of the extracted information, as only 46 out of 152\\nlabeled pieces of information were retrieved. Upon investigating the papers, we found that much of the\\nCoulombic Efficiency was recorded in tables and figures, which were dropped during PDF parsing. It is\\nnecessary to introduce multimodal LLMs to further investigate those papers.\\n26.3 Investigation 2: Advanced Chatbot for Lithium Battery Q/A\\nIn this exploration, we utilized a curated collection of research papers focused on lithium metal batteries to\\nconstruct a vector database and developed a chatbot employing a Retrieval-Augmented Generation (RAG)\\nframework. Our Q/A pipeline includes two types of answer strategies: General Q/A and Knowledge-graph-\\nbased Q/A. The pipeline is described in Figure 1.\\nGeneral Q/A Building In this section, we detail our comprehensive Q/A pipeline designed for the\\nexploration of lithium metal battery-related research. We began by sourcing and downloading 71 research\\npapers pertinent to lithium metal batteries. The information extracted from these papers was encoded into\\nvectorsandstoredinChromadatabases. Tobestreflectthespecializedlanguageofthefield,wecreatedtwo\\ndistinctdatabases: oneutilizingMaterialsBERTformaterialssciencecontent,andanotherusingChemBERT\\nfor chemical context.\\nDuring the retrieval phase, we employ LOTR (MergerRetriever) to enhance the precision of document\\nretrieval. Uponreceivingauserquery,thesystemretrievesrelevantdocumentsegmentsfromeachdatabase.\\nIt then removes any redundant results from the merged outputs and selects the top 10 most pertinent\\ndocument chunks. Finally, both the selected context and the user query are processed by GPT-4 Turbo to\\ngenerateaninformedandaccurateresponse. Thispipelineexemplifiesarobustintegrationofstate-of-the-art\\ntechnologies to facilitate advanced research interrogation and knowledge discovery in the domain of lithium\\nmetal batteries.\\n82\\nKnowledge Graph-based Q/A Building We built a knowledge graph with the extracted information\\nusing Neo4j. The knowledge graph consists of four node types:\\n• Solvent node: properties include name, SMILES, density, and weight.\\n• Salt node: properties include name, SMILES, and weight.\\n• Electrolyte node: properties include name and Coulombic Efficiency.\\n• Reference node: properties include title, content, and page number and,\\nthree edge types:\\n• (electrolyte)-[:VOLUME] -¿(solvent)\\n• (electrolyte)-[:CONCENTRATION] -¿(salt)\\n• (electrolyte)-[:CITED] -¿(reference)\\nThe built knowledge graph can be accessed and visualized using the Neo4j web application.\\nKnowledge Graph Enhanced Question Answering By using GraphCypherQAChain in LangChain,\\nLLMs can generate Cypher queries to solve users’ questions. This capability allows the LLMs to address\\nuser queries by filtering and leveraging relationships between data points, which is particularly valuable\\nin complex domains such as lithium battery technology. This integration ensures that our RAG pipeline is\\nadeptathandlingdomain-specificquestionsandexcelsinscenarioswhereunderstandingtheinterconnections\\nwithin data is crucial.\\nReferences\\n[1] Kim, S.C., et al. ”Data-driven electrolyte design for lithium metal anodes.” Proceedings of the National\\nAcademy of Sciences 120.10 (2023): e2214357120. https://www.pnas.org/doi/full/10.1073/pnas.\\n2214357120.\\n83\\n27 KnowMat: Transforming Unstructured Material Science Lit-\\nerature into Structured Knowledge\\nAuthors: Hasan M. Sayeed, Ramsey Issa, Trupti Mohanty, Taylor Sparks\\n27.1 Introduction\\nThe rapid growth of materials science has led to an explosion of scientific literature, often presented in\\nunstructuredformatsthatposesignificantchallengesforsystematicdataextractionandanalysis. Toaddress\\nthis, we developed KnowMat, a novel tool designed to transform complex, unstructured material science\\nliteratureintostructuredknowledge. LeveragingadvancedLargeLanguageModels(LLMs)suchasGPT-3.5,\\nGPT-4, Llama3, KnowMatautomatestheextractionofcriticalinformationfromscientifictexts, converting\\nthem into structured JSON formats. This tool not only enhances the efficiency of data processing but also\\nfacilitates deeper insights and innovation in material science research. KnowMat’s user-friendly interface\\nallows researchers to input material science papers, which are then parsed and analyzed to extract key\\ninsights. Thetool’sversatilityinhandlingmultipleinputfilesanditscapacityforcustomizationthroughsub-\\nfield specific prompts make it an indispensable resource for researchers aiming to streamline their workflow.\\nAdditionally, KnowMat’s ability to integrate with other tools and platforms, along with its support for\\nvarious LLMs, ensures that it remains adaptable to the evolving needs of the research community.\\n27.2 Method\\nData Parsing and Extraction The KnowMat workflow begins with parsing unstructured text from\\nmaterial science literature using a tool called Unstructured [1]. This tool reads the input file, separates\\nout the sections, and stores everything in a machine-readable format. This initial step involves identifying\\nrelevant sections and extracting pertinent information related to material compositions, properties, and\\nexperimental conditions.\\nCustomizable Prompts KnowMat provides field-specific prompts for several fields, and it offers the\\nflexibility for users to customize these prompts further or create their own prompts for new fields. This\\nfeature ensures that the extracted data is both relevant and comprehensive, tailored to the specific needs of\\nthe researcher. The interface allows users to define the scope and focus of the extraction process effectively.\\nIntegration and Interoperability To enhance usability and interoperability, KnowMat supports seam-\\nless integration with other tools and platforms. Extracted results can be easily exported in CSV format,\\nenablingstraightforwarddatasharingandfurtheranalysis. Thetool’sflexibilityextendstoitscompatibility\\nwith various LLMs, including both subscription-based models like GPT-3.5 and GPT-4 [2], and open-source\\nmodels like Llama 3 [3]. This ensures that researchers can select the most suitable LLM for their specific\\nrequirements.\\n27.3 Preliminary Results\\nIn initial tests, KnowMat demonstrated promising results in the efficiency and accuracy of data extraction\\nfrom material science literature. The tool successfully parsed and structured information from multiple\\npapers, converting complex textual data into actionable insights. For instance, in a study involving super-\\nconductors,KnowMataccuratelyextracteddetailedinformationoncompositions,criticaltemperatures,and\\nexperimental conditions, presenting them in a structured JSON format.\\n27.4 Future Development\\nLookingahead,futuredevelopmentsforKnowMatincludeenhancingtheeditingcapabilitiesforfield-specific\\nprompts, improving the handling of multiple input files in a single operation, and expanding output options\\ntofurtherenhanceflexibilityandusability. Continuousimprovementandexpansionoffield-specificprompts\\nwillensurethatKnowMatremainsavaluabletoolforresearchersacrossvariousdomainsofmaterialscience.\\n84\\nFigure 34: KnowMat Workflow. The graphical abstract illustrates the KnowMat workflow, which begins\\nwithparsinginputfilesinXMLorPDFformats. TheLargeLanguageModel(LLM)poweredbyengineered\\nprompts processes the reference text to extract key information, such as material composition, properties,\\nand measurement conditions, and converts it into a structured JSON output.\\nIn conclusion, KnowMat represents a significant advancement in the field of knowledge extraction from\\nscientific literature. By converting unstructured material science texts into structured formats, it provides\\nresearchers with powerful tools to unlock insights and drive innovation in their fields.\\n27.5 Data and code availability\\nhttps://github.com/sparks-sayeed/LLMs_for_Materials_and_Chemistry_Hackathon\\nReferences\\n[1] https://unstructured.io/.\\n[2] https://platform.openai.com/docs/models\\n[3] https://llama.meta.com/llama3/\\n85\\n28 Ontosynthesis\\nAuthors: Qianxiang Ai, Jiaru Bai, Kevin Shen, Jennifer D’Souza, Elliot Risch\\n28.1 Introduction\\nOrganic synthesis is often described in unstructured text without a standard taxonomy, which makes syn-\\nthesis data less searchable and less compatible with downstream data-driven tasks (e.g., retrosynthesis,\\ncondition recommendation) compared to structured records. The specificities and writing styles of these\\ndescriptions also vary, ranging from simple sentences about chemical transformations to long paragraphs\\nthat include procedure details. This leads to ambiguities, unidentified missing information, challenges in\\ncomparing different syntheses, and can impede proper experimental reproduction.\\nIn last year’s hackathon, we fine-tuned an open-source LLM to extract data structured in the Open\\nReaction Database schema from synthesis procedures [1]. While this method has proved to be successful\\nfor patent text, it relies on existing unstructured-structured data pairs and does not generalize well to non-\\npatent writing styles. The dependency of fine-tuning on existing data makes it less useful, especially when\\nconsidering new writing styles, or newly developed data structures or ontologies are preferred.\\nIn this project, we explore the potential of LLMs in structured data extraction without fine-tuning.\\nSpecifically, given an ontology (formally defined concepts and relationships) for organic synthesis, we aim to\\nextract structured information as Resource Description Framework (RDF) graphs from unstructured text\\nusing LLMs with zero-shot prompting. RDF is a World Wide Web Consortium (W3C) standard that serves\\nasafoundationfortheSemanticWebandexpressingmeaningandrelationshipsbetweendata. WhileLLMs\\ncan create ontologies on the fly for a given piece of text, “grounding” to a pre-specified ontology allows\\nstandardizingtheextracteddataandreasoningwithexistingaxioms. Theextractionworkflowiswrappedin\\nawebapplicationwhichalsoallowsvisualizationoftheextractedresults. Weshowcasedthecapabilityofour\\napplicationwithcasestudieswhereRDFswereextractedfromreactiondescriptionsofdifferentcomplexities\\nand specificities.\\n28.2 Information extraction workflow\\nOpenAI’s GPT model (gpt-4-turbo-2024-04-09) is used for structured information extraction through its\\nChatComplete API. The prompt for an individual extraction task consists of two parts:\\n1. The System prompt: The given ontology in OWL format based on which the RDF graph is defined,\\nalong with supporting instructions on the task. The full prompt template is included in the project\\nGitHub repository [2]. Two ontologies were used in this study:\\n(a) OntoReaction: A reaction ontology previously used in distributed self-driving laboratories [3];\\n(b) Synthesis Operation Ontology: A process chemistry ontology designed for common operations\\nused in organic synthesis [4].\\n2. The User prompt: The unstructured text from which the RDF graph is extracted.\\nThe final OpenAI API request is a combination of the User and System prompts:\\nfrom openai import OpenAI\\nclient = OpenAI(api key=api key)\\ncompletion = client .chat.completions. create(\\nmodel=”gpt−4−turbo”,\\nmessages=[\\n{”role ”: ”system”, ”content”: system prompt ,},\\n{”role ”: ”user”, ”content”: unstructured data.text}\\n] ,\\n)\\nAlternatively, one can use GPT assistants and provide the System prompt as a base Instruction through\\nOpenAI’s web user interface.\\n86\\n28.3 Application\\nWecollectedasetofeightreactiondescriptionstakenfrompatents,journalarticles(maintextorsupporting\\ninformation), and lab notebooks. Each of them is assigned a complexity rating and a specificity rating\\nusing three-point scales. Based on these test cases, we found our workflow was able to produce valid RDF\\ngraphs representing the chemical reactions, even for multi-step reactions including many elements. Expert\\ninspections indicate the resulting RDF graphs better represent the unstructured text when OntoReaction is\\nused as the target ontology compared to the larger Synthesis Operation Ontology (the latter contains more\\nclasses and object properties).\\nSincetheextracteddataisinRDFformat,theycanbereadilyvisualizedusinginteractivegraphlibraries.\\nUsing dash-cytoscape [5], we created an interface application to the extraction workflow. The interface\\nallows submitting unstructured text as input to the extraction workflow with a user-provided OpenAI API\\nkey,retrievingandinteractivelyvisualizingtheextractedknowledgegraph,aswellasdisplayingtheextracted\\nRDF text. A file-based caching routine is used to store previous extraction results. All code and test cases\\nare available in the project GitHub repository [2].\\nFigure 35\\n28.4 Acknowledgements\\nQ.A. acknowledges support by the National Institutes of Health under award number U18TR004149. The\\ncontent is solely the responsibility of the authors and does not necessarily represent the official views of the\\nNational Institutes of Health. J. D. acknowledges the SCINEXT project (BMBF, German Federal Ministry\\nof Education and Research, Grant ID: 01lS22070).\\nReferences\\n[1] Ai, Q.; Meng, F.; Shi, J.; Pelkie, B.; Coley, C. W. Extracting Structured Data from Organic Synthesis\\nProcedures Using a Fine-Tuned Large Language Model. ChemRxiv April 8, 2024. https://doi.org/\\n10.26434/chemrxiv-2024-979fz.\\n[2] Ontosynthesis, 2024. https://github.com/qai222/ontosynthesis (accessed 2024-07-07).\\n[3] Bai,J.;Mosbach,S.;Taylor,C.J.;Karan,D.;Lee,K.F.;Rihm,S.D.;Akroyd,J.;Lapkin,A.A.;Kraft,\\nM. A Dynamic Knowledge Graph Approach to Distributed Self-Driving Laboratories. Nat. Commun.\\n2024, 15 (1), 462. https://doi.org/10.1038/s41467-023-44599-9.\\n87\\n[4] Ai, Q.; Klein, C. Synthesis Operation Ontology. GitHub. https://github.com/qai222/\\nontosynthesis/blob/main/ontologies/soo/soo.md (accessed 2024-07-07).\\n[5] Dash-Cytoscape: A Component Library for Dash Aimed at Facilitating Network Visualization in\\nPython,WrappedaroundCytoscape.Js.https://dash.plotly.com/cytoscape(accessed2024-07-06).\\n88\\n29 Knowledge Graph RAG for Polymer Simulation\\nAuthors: Jiale Shi, Weijie Zhang, Dandan Tang, Chi Zhang\\nFigure36: CreatingKnowledgeGraphRetrieval-AugmentedGeneration(KGRAG)forPolymerSimulation.\\nMolecular modeling and simulations have become essential tools in polymer science and engineering,\\noffering predictive insights into macromolecular structure, dynamics, and material properties. However, the\\ncomplexity of polymer simulations poses challenges in model development/selection, computational method\\nchoices, advanced sampling techniques, and data analysis. While literature [1] provides guidelines to en-\\nsure the validity and reproducibility of these simulations, these resources are often presented in massive,\\nunstructured text formats, making it difficult for new learners to systematically and comprehensively un-\\nderstand the correct approaches for model selection, simulation execution, and post-processing analysis.\\nTherefore, this study presents a novel approach to address these challenges by implementing a Knowledge\\nGraph Retrieval-Augmented Generation (KGRAG) system for building an AI chatbot focused on polymer\\nsimulation guidance and education.\\nOur team utilized the large language model GPT-3.5 Turbo [2] and Microsoft’s GraphRAG [3,4] frame-\\nwork to extract polymer simulation-related entities and relationships from unstructured documents, con-\\nstructing a comprehensive KGRAG, as shown in Figure 36, where the nodes are colored by their degrees.\\nThose nodes with high degrees include “Polymer Simulation”, “Atomistic Model,” “CG Model,” “Force\\nField,” and “Polymer Informatics,” which are all the keywords about polymer simulation and modeling,\\nillustrating the effective performance of entity extraction. We run the query engineering of KGRAG to\\nask questions. For comparative analysis, we also implemented a baseline Retrieval-Augmented Generation\\n(RAG)systemusingLlamaIndex[5]. UponcomparingtheresponsesfromthebaselineRAGandKGRAGby\\nhuman experts, we found that the KGRAG demonstrates substantial improvements in question-answering\\nperformance when analyzing complex and high-level information about polymer simulation. This improve-\\nmentisattributedtoKGRAG’sabilitytocapturesemanticconceptsanduncoverhiddenconnectionswithin\\nthe data, providing more accurate, logical, and insightful responses compared to traditional vector-based\\nRAG methods.\\nThis study contributes to the growing field of data-driven approaches in polymer science by offering a\\n89\\npowerful tool for knowledge extraction and synthesis. Our KGRAG system shows promise in enhancing the\\nunderstanding of massive unstructured polymer simulation guidance in the relevant literature, potentially\\nimproving the validity and reproducibility of these polymer simulations, and accelerating the development\\nof new polymer simulation methods. We found that the quality of prompts is crucial for effective entity\\nextraction and knowledge graph construction. Therefore, our future work will focus on optimizing prompts\\nforentityextraction,relationshipbuilding,andknowledgegraphconstructiontofurtherimprovethesystem’s\\nperformance and applicability in polymer simulation research.\\n29.1 Data availability\\nRepository: https://github.com/shijiale0609/KG-RAG-LLM-Polymers\\n29.2 Author\\n• Jiale Shi, Department of Chemical Engineering, Massachusetts Institute of Technology, Cambridge,\\nMassachusetts 02139, United States (jialeshi@mit.edu)\\n• WeijieZhang,DepartmentofChemistry,UniversityofVirginia,Charlottesville,Virginia22904,United\\nStates (shv9px@virginia.edu)\\n• Dandan Tang, Department of Psychology, University of Virginia, Charlottesville, Virginia 22904,\\nUnited States (gux8df@virginia.edu)\\n• ChiZhang,DepartmentofAutomationEngineering,RWTHAachenUniversity,Aachen,NorthRhine-\\nWestphalia, 52062, Germany (chi.zhang2@rwth-aachen.de)\\nReferences\\n[1] Gartner,T.E.,III;Jayaraman,A.ModelingandSimulationsofPolymers: ARoadmap.Macromolecules\\n2019, 52 (3), 755-786. DOI: 10.1021/acs.macromol.8b01836.\\n[2] GPT-3.5 Turbo. 2024. https://platform.openai.com/docs/models/gpt-3-5-turbo (accessed\\n2024/07/10).\\n[3] Welcome to GraphRAG. 2024. https://microsoft.github.io/graphrag/ (accessed 2024/07/10).\\n[4] Edge, D.; Trinh, H.; Cheng, N.; Bradley, J.; Chao, A.; Mody, A.; Truitt, S.; Larson, J. From Local to\\nGlobal: A Graph RAG Approach to Query-Focused Summarization. 2024. (acccessed 2024/07/10).\\n[5] LlamaIndex. 2024. https://docs.llamaindex.ai/en/stable/ (accessed 2024/07/10).\\n90\\n30 Synthetic Data Generation and Insightful Machine Learning\\nfor High Entropy Alloy Hydrides\\nAuthors: Tapashree Pradhan, Devi Dutta Biswajeet\\nThegenerationofsyntheticdatainmaterialsscienceandchemistryistraditionallyperformedbymachine\\nlearninginteratomicpotentials(MLIPs)thatapproximatethefirstprinciplefunctionalformusedtocompute\\nwave functions of known electronic configurations [1]. Synthetic data is a component of the active-learning\\nfeedback loop that is utilized to retrain these potentials. The purpose of this active-learning component is\\nto incorporate a wider range of physical conditions into the potential’s application domain. However, the\\ninitial cost of data generation to train the MLIPs is a major setback for complex chemistries like in the case\\nof high entropy alloys (HEAs).\\nThe potential application of high entropy alloys in hydrogen storage [2] demands acceleration in the\\ncomputation of surface and mechanical properties of the alloys by better approximation of the potential\\nlandscape. The use of Large Language Models (LLMs) in the generation of synthetic data to tackle this\\nbottleneck problem poses an alternative to the traditional MLIPs. LLMs like ChatGPT, Llama3, Claude,\\nandGemini[3]learnfromtext-embeddingsofthetrainingdata,capturinginherenttrendsbetweensemantic\\nor numerical relationships of the text and making them suitable for learning certain complex relationships\\nin materials physics that might be present in the language itself.\\nThe current work aims to build LLM applications working in conjunction with an external database of\\nhigh entropy alloy hydrides via Retrieval-Augmented Generation (RAG) [4] to populate synthetic data for\\npredictive modeling later. The inbuilt RAG feature of GPT-4 enables us to write a few prompts to make\\na synthetic data generator utilizing a custom dataset of HEA hydrides. The work also utilizes OpenAI’s\\nAPI [5] and the text-embedding-3-large model [6] to configure custom generators that can be fine-tuned via\\nprompts for synthetic data generation.\\nThe development of the entire product is aimed at a web-based application that allows users to upload\\ntheir datasets and instruct the GPT model to generate more entries that can serve as training data for\\npredictive ML models like hydrogen capacity predictors. The term “Insightful Machine Learning” refers to\\na sequential pipeline starting with (a) a reference database that serves as the retrieval component of an\\nLLM, (b) the generation of synthetic data and features, and (c) getting insights from a chatbot on physics\\nof the problem having multiple retrieval components inclusive of the predictive model. Figure 37 shows the\\nflowchart of the pipeline which is currently at the prototype stage under development. The current code to\\ngenerate synthetic data is available for use and modification.\\n30.1 Future Directions\\nTheproposedpipelineshowninFigure1requiresavalidationstagewhichisessentialforactivelearning. The\\nongoing work involves the search for novel validation techniques taking inspiration from recently published\\nworks on information theory.\\nThe future direction of the work is to complete the validation phase and have a product utilizing the\\npipeline for high entropy hydrides that can accelerate the search and discovery process without performing\\ncomplex first principle calculations for data generation and training.\\nReferences\\n[1] Focassio, B., Freitas, M., Schleder, G.R. (2024). Performance assessment of universal machine learning\\ninteratomic potentials: Challenges and directions for materials’ surfaces. ACS Applied Materials &\\nInterfaces.\\n[2] Marques, F., Balcerzak, M., Winkelmann, F., Zepon, G., Felderhoff, M. (2021). Review and outlook on\\nhigh-entropy alloys for hydrogen storage. Energy & Environmental Science, 14(10), 5191-5227.\\n[3] Jain,S.M.(2022).Hugging face: Introduction to transformers for NLP with the hugging face library and\\nmodels to solve problems. Berkeley, CA: Apress.\\n91\\nFigure 37: Insightful machine learning for HEA hydrides\\n[4] Lewis, P., et al. (2020). Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances\\nin Neural Information Processing Systems, 33, 9459-9474.\\n[5] OpenAI. (2020). OpenAI GPT-3: Language models are few-shot learners. Retrieved from https://\\nopenai.com/blog/openai-api.\\n[6] OpenAI. (2023). New embedding models and API updates. Retrieved from https://openai.com/\\nindex/new-embedding-models-and-api-updates.\\n92\\n31 Chemsense: Are large language models aligned with human\\nchemical preference?\\nAuthors: Martin˜o R´ıos-Garc´ıa, Nawaf Alampara, Mara Schilling-Wilhelmi, Abdelrahman\\nIbrahim, Kevin Maik Jablonka\\n31.1 Introduction\\nGenerativeAImodelsarerevolutionizingmoleculardesignbylearningfromvastchemicaldatasetstocreate\\nnovelcompounds[1]. Thechallengeinmoleculardesigngoesbeyondjustcreatingnewstructures. Akeyissue\\nis ensuring the designed molecules have specific useful properties, e.g., high solubility, high synthesizability,\\netc. LLMs that can be conditioned on the desired property seem to be a promising solution [4]. However,\\ncurrent frontier models often lack an innate chemical understanding, which can lead to impractical or even\\ndangerous molecular designs [5].\\nA factor that distinguishes many successful chemists is their chemical intuition. This intuition, for\\ninstance, describes the preference for certain compounds that are not grounded in knowledge that can be\\neasilyconveyedbutratherbytacitknowledgeaccumulatedoveryearsofexperience. Ifmodelscouldpossess\\nthis chemical intuition, they would be more useful for real-world scientific applications.\\nIn this work, we introduce ChemSense, in which we explore how well frontier models are aligned with\\nhuman experts in chemistry. By aligning AI with human knowledge and preferences, one can aim to create\\ntoolsthatcanproposefeasible,safe,anddesirablemolecularstructures,bridgingthegapbetweentheoretical\\ncapabilitiesandpracticalscientificneeds. Moreover,ChemSensewouldhelpusinunderstandingtheemergent\\nalignment of frontier models with dataset scale and size.\\n31.2 Related work\\nYang et al. (2024) investigated the ability of LLMs to predict human characteristics in their social life and\\nshowedthatLLMsencounteredgreatdifficultiesinpredictingthese. Mirzaetal.(2024)benchmarkedLLMs\\non various chemistry tasks; however, good performance in that benchmark does not guarantee alignment\\nwith human-expert intuitions. Chennakesavalu et al. (2024) aligned LLMs to generate low energy stable\\nmolecules with externally specified properties, they noticed huge improvement upon alignment compared to\\nthe base model.\\nFigure 38: Comparison of the alignment of the different LLMs with the SMILES (left) and IUPAC (right)\\nmolecular representations. For both representations, note that the random baseline is at a fixed value of 0.5\\nsince all the questions were binary and the datasets are balanced.\\n31.3 Materials and Methods\\nChemical preference dataset The test set used during all the experimentation was constructed with\\n900samplesfromthedatasetpublishedbyChoungetal. (2023). Thisdatasetcontainsthepreferencesof35\\nNovartis medical chemists, resulting in more than 5000 question-answer pairs about organic synthesis. To\\n93\\nchoose the 900 samples, only questions in which both molecules could be converted into the IUPAC names\\n(using the chemistry name resolver) were selected. In that way, we ensure some homogeneity in the test\\ndataset.\\nLLM evaluation To study the chemical preference of the actual LLMs, some of the models performing\\nbest on the ChemBench benchmark [5] were prompted with a simple instruction prompt inspired by the\\nquestion that was asked to collect the original data from the scientists of Novartis. Additionally, to study\\nhowmolecularrepresentationscouldaffectthemodel,eachofthe900questionswasgiventothemodelusing\\ndifferent molecular representations.\\nTo ensure the correct format of the answers, OpenAI models as well as Claude-3 were constrained to\\nanswer only “A” or “B” using the Instructor package. On the other hand, the Llama models and Mixtral\\n8Bx7used(inanunconstrainedsetup)throughGroqAPIserviceandinsteadfurtherpromptedtoencourage\\nthem only to answer “A” or “B”.\\n31.4 Results\\nComparison of chemical preference of LLMs We compare the accuracy of the preference prediction\\nof the different models and representations (Figure 1). The accuracy of all models ranges from 49% to\\n61% where 50% is the accuracy one would obtain for a random choice. The GPT-4o model achieves the\\nhighest accuracy of all models and performs best with the SMILES and IUPAC representations. This might\\nbe explained by the widespread use of both of the representations and, therefore, a high occurrence in the\\ntraining data of the model. We observe the same trend in the GPT-4 and GPT-3.5 Turbo predictions. For\\ntheotherLLMs,representationseemstohaveasmallerimpactontheaccuracywithvaluesthatpresumably\\nare random.\\n31.5 Discussion and Conclusions\\nOur work shows that while some LLMs might show sparks of chemical intuition, they are mostly unaligned\\nwith expert intuitions, as their performance currently is only marginally better than random guessing. In-\\nterestingly,similartopreviouswork[4],wefindthatoneobtainsdifferentperformanceindifferentmolecular\\nrepresentations, which might be attributable to different prevalences in the training data. Our research\\ndemonstrates that preference tuning is a promising and underexplored approach to enhance models for\\nchemists. Addressing model biases is crucial for ensuring fair and accurate predictions. By tackling these\\nchallenges, we can develop large language models (LLMs) that are both powerful and practical for chemists\\nand materials scientists.\\nReferences\\n[1] Bilodeau, Camille et al. (2022). “Generative models for molecular discovery: Recent advances and\\nchallenges”. In: Wiley Interdisciplinary Reviews: Computational Molecular Science 12.5, e1608.\\n[2] Chennakesavalu, Shriram et al. (2024). Energy Rank Alignment: Using Preference Optimization to\\nSearch Chemical Space at Scale. DOI: 10.48550/ARXIV.2405.12961. URL: https://arxiv.org/abs/\\n2405.12961.\\n[3] Choung,Oh-Hyeonetal.(Oct.2023).“Extractingmedicinalchemistryintuitionviapreferencemachine\\nlearning”.In: NatureCommunications14.1.ISSN:2041-1723.DOI:10.1038/s41467-023-42242-1.URL:\\nhttp://dx.doi.org/10.1038/s41467-023-42242-1.\\n[4] Jablonka, Kevin Maik et al. (Feb. 2024). “Leveraging large language models for predictive chemistry”.\\nIn: Nature Machine Intelligence 6.2, pp. 161–169. ISSN: 2522-5839. DOI: 10.1038/s42256-023-00788-1.\\nURL: http://dx.doi.org/10.1038/s42256-023-00788-1.\\n[5] Mirza, Adrian et al. (2024). “Are large language models superhuman chemists?” In: arXiv preprint.\\nDOI: 10.48550/arXiv.2404.01475. arXiv: 2404.01475 [cs.LG].\\n94\\n[6] Yang, Kaiqi et al. (2024). Are Large Language Models (LLMs) Good Social Predictors? DOI:\\n10.48550/ARXIV.2402.12620. URL: https://arxiv.org/abs/2402.12620.\\n95\\n32 GlossaGen\\nAuthors: Magdalena Lederbauer, Dieter Plessers, Philippe Schwaller\\nAcademicarticles,particularlyreviews,andgrantproposalswouldgreatlybenefitfromaglossaryexplain-\\ningcomplexjargonandterminology. However,themanualcreationofsuchglossariesisatime-consumingand\\nrepetitive task. To address this challenge, we developed GlossaGen, an innovative framework that leverages\\nlarge language models to automatically generate glossaries from PDF or TeX files, streamlining the process\\nfor academics. The generated glossary is not only a list of terms and definitions but also visualized as a\\nknowledge graph, illustrating the intricate relationships between various technical concepts (see Figure 39).\\nFigure 39: Overview of (left) the graphical user interface (GUI) protoype and (right) the generated Neo4J\\nknowledge graph (right). Our results demonstrate that LLMs can greatly accelerate glossary creation,\\nincreasing the likelihood that authors will include a helpful glossary without the need for tedious manual\\neffort. Additionally, an analysis of our test case by a zeolite domain expert revealed that LLMs produce\\ngood results, with about 70% - 80% of explanations requiring little to no manual changes.\\nTheproject’scodebasewasdevelopedasaPythonpackageonGitHubusingatemplate[1]andDSPy[2]\\nas an LLM framework. This modular approach facilitates seamless collaboration and easy incorporation of\\nnew features.\\nTo overcome the limitations of LLMs in directly processing PDFs, a prevalent format for scientific pub-\\nlications, we implemented a pre-processing step that converts papers into manageable text sequences. This\\nstepinvolvesextractingtextualinformationusingPyMuPDF[3],automaticallyobtainingthetitleandDOI,\\nand chunking the text into smaller sections. This chunking preserves context and makes it easier for the\\nmodels to handle the input.\\nWe used GPT-3.5-Turbo [4] and GPT-4-Turbo [5] to extract scientific terms and their definitions from\\nthe text chunks. Employing Typed Predictors [6] and Chain-of-Thought prompting [7] ensures the outputs\\nare well-structured and contextually accurate, guiding the model to produce precise definitions through a\\nsimulated reasoning process. Post-processing involved identifying and removing duplicate entries, ensuring\\neach term appears only once in the final glossary. Figure 40 shows details about the GlossaryGenerator\\nclass that was used to process documents into the corresponding glossaries. We selected a review article on\\nzeolites [8] (shown in Figure 39) as a test publication to manually tune and evaluate the pipeline’s output.\\nTheobtainedglossaryistransformedintoanontologythatdefinesnodesandrelationshipsfortheknowl-\\nedge graph. For instance, relationships like ’MATERIAL – exhibits → PROPERTY’ illustrate how different\\nterms are interconnected. The knowledge graph is constructed using the library Neo4J [9] and Graph\\n96\\nFigure 40: Overview of the GlossaryGenerator class, responsible for processing text chunks and extracting\\nrelevant, correctly formatted terms and definitions.\\nMaker [10] on the processed text chunks. We developed a user-friendly front-end interface with Gradio [11],\\nas shown in Figure 39. This interface allows users to interact with the glossary, making it easier to navigate\\nand customize the information.\\nThe quick prototyping provided us with several ideas for future work. We can improve the glossary\\noutput by fine-tuning the LLM, incorporating retrieval-augmented generation, and parsing article images.\\nAdditionally, the user experience can be enhanced by allowing users to input specific terms for glossary\\nexplanations as a backup when the LLM omits certain terms. Integration with LaTeX would broaden\\nusability, envisioning commands like \\\\glossary similar to \\\\bibliography. We also consider connecting\\nthe knowledge graph directly to the user interface and enhance its ontology creation feature. Overall, this\\nrapidlydevelopedprototype,withnumerousfuturepossibilities,demonstratesthepotentialofLLMstoassist\\nresearchers in their scientific outreach.\\nReferences\\n[1] Copier template: Available at https://github.com/copier-org/copier.\\n[2] DSPy: Khattab,O.;Singhvi,A.;Maheshwari,P.;Zhang,Z.;Santhanam,K.;Vardhamanan,S.;Haq,S.;\\nSharma, A.; Joshi, T. T.; Moazam, H.; Miller, H.; Zaharia, M.; Potts, C. DSPy: Compiling Declarative\\nLanguage Model Calls into Self-Improving Pipelines. preprint arXiv:2310.03714. 2023.\\n[3] PyMuPDF: Available at ttps://github.com/pymupdf/PyMuPDF.\\n[4] GPT-3.5-Turbo: OpenAI.Availableathttps://platform.openai.com/docs/models/gpt-3-5-turbo.\\n97\\n[5] GPT-4-Turbo: OpenAI. Available at\\nhttps://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4.\\n[6] DSPy Typed Predictors: Documentation at https://dspy.ai/learn/8-typed_predictors.\\n[7] Chain-of-Thought prompting: Wei, J.; Wang X.; Schuurmans, D.; Bosma, M.; Ichter, B.; Xia, F.; Chi,\\nE.;Le,Q.;Zhou,D.Chain-of-ThoughtPromptingElicitsReasoninginLargeLanguageModels.preprint\\narXiv: 2201.11903. 2023.\\n[8] Test review article on zeolites: Rhoda, H. M.; Heyer, A. J.; Snyder, B. E. R.; Plessers, D.; Bols, M.\\nL.; Schoonheydt, R. A.; Sels, B. F.; Solomon, E. I. Second-Sphere Lattice Effects in Copper and Iron\\nZeolite Catalysis. Chem. Rev. 2022, 122, 12207–12243.\\n[9] Neo4J: Documentation at https://neo4j.com/.\\n[10] Graph Maker: Available at https://github.com/rahulnyk/graph_maker.\\n[11] Gradio: Available at https://github.com/gradio-app/gradio.\\n98\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526b048",
   "metadata": {},
   "source": [
    "# Browser-Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa21201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [browser_use] BrowserUse logging setup complete with level info\n",
      "INFO     [root] Anonymized telemetry enabled. See https://docs.browser-use.com/development/telemetry for more information.\n",
      "INFO     [agent] 🚀 Starting task: \n",
      "        1. Go to Google.com.\n",
      "        2. Search 'Kaylx Jang email address' and enter.\n",
      "        3. Output only the emails or 'None'—no additional explanations.\n",
      "        \n",
      "INFO     [agent] 📍 Step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/arxiv-and-scholar-scraping/.venv/lib/python3.11/site-packages/browser_use/agent/message_manager/views.py:59: LangChainBetaWarning: The function `load` is in beta. It is actively being worked on, so the API may change.\n",
      "  value['message'] = load(value['message'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [agent] 🤷 Eval: Unknown - Starting a new task from scratch.\n",
      "INFO     [agent] 🧠 Memory: Task 0 of 3 started. Go to Google.com to start searching for 'Kaylx Jang email address'.\n",
      "INFO     [agent] 🎯 Next goal: Open Google.com in the current tab.\n",
      "INFO     [agent] 🛠️  Action 1/1: {\"go_to_url\":{\"url\":\"https://www.google.com\"}}\n",
      "INFO     [controller] 🔗  Navigated to https://www.google.com\n",
      "INFO     [agent] 📍 Step 2\n",
      "INFO     [agent] 👍 Eval: Success - Navigated to Google successfully.\n",
      "INFO     [agent] 🧠 Memory: Task step 1 of 3 completed: Navigated to Google.com. Need to accept cookies and perform the search for 'Kaylx Jang email address'.\n",
      "INFO     [agent] 🎯 Next goal: Accept cookies to proceed with the search.\n",
      "INFO     [agent] 🛠️  Action 1/1: {\"click_element\":{\"index\":4}}\n",
      "INFO     [controller] 🖱️  Clicked button with index 4: Alle ablehnen\n",
      "INFO     [agent] 📍 Step 3\n",
      "INFO     [agent] 👍 Eval: Success - Cookies accepted, search page ready for input.\n",
      "INFO     [agent] 🧠 Memory: Task step 2 of 3: Input 'Kaylx Jang email address' into the search bar and then execute the search.\n",
      "INFO     [agent] 🎯 Next goal: Input the search query and perform the search on Google.\n",
      "INFO     [agent] 🛠️  Action 1/2: {\"input_text\":{\"index\":7,\"text\":\"Kaylx Jang email address\"}}\n",
      "INFO     [agent] 🛠️  Action 2/2: {\"click_element\":{\"index\":10}}\n",
      "INFO     [controller] ⌨️  Input Kaylx Jang email address into index 7\n",
      "INFO     [agent] Something new appeared after action 1 / 2\n",
      "INFO     [agent] 📍 Step 4\n",
      "INFO     [agent] ⚠ Eval: Failed - Autocomplete suggestions appeared, preventing search execution.\n",
      "INFO     [agent] 🧠 Memory: Task 2 of 3 not fully completed: Autocomplete suggestion needs to be accepted before executing the search for 'Kaylx Jang email address'.\n",
      "INFO     [agent] 🎯 Next goal: Select the correct autocomplete suggestion to execute the search.\n",
      "INFO     [agent] 🛠️  Action 1/1: {\"click_element\":{\"index\":12}}\n",
      "WARNING  [controller] Element not clickable with index 12 - most likely the page changed\n",
      "INFO     [agent] 📍 Step 5\n",
      "INFO     [agent] ⚠ Eval: Failed - Unable to select the autocomplete suggestion which interrupted the search process.\n",
      "INFO     [agent] 🧠 Memory: Autocomplete suggestion could not be selected. Retry selecting the correct suggestion to proceed with the search.\n",
      "INFO     [agent] 🎯 Next goal: Attempt to click the correct autocomplete suggestion for 'Kaylx Jang email address' and execute the search.\n",
      "INFO     [agent] 🛠️  Action 1/2: {\"click_element\":{\"index\":12}}\n",
      "INFO     [agent] 🛠️  Action 2/2: {\"send_keys\":{\"keys\":\"Enter\"}}\n",
      "WARNING  [controller] Element not clickable with index 12 - most likely the page changed\n",
      "INFO     [agent] 📍 Step 6\n",
      "INFO     [agent] ⚠ Eval: Failed - Couldn't select the autocomplete option, error with element identification.\n",
      "INFO     [agent] 🧠 Memory: Retry selecting autocomplete suggestion for 'Kaylx Jang email address'.\n",
      "INFO     [agent] 🎯 Next goal: Select the correct autocomplete suggestion for 'Kaylx Jang email address' again and execute the search.\n",
      "INFO     [agent] 🛠️  Action 1/2: {\"click_element\":{\"index\":12}}\n",
      "INFO     [agent] 🛠️  Action 2/2: {\"send_keys\":{\"keys\":\"Enter\"}}\n",
      "WARNING  [controller] Element not clickable with index 12 - most likely the page changed\n",
      "INFO     [agent] 📍 Step 7\n",
      "INFO     [agent] ⚠ Eval: Failed - Previous attempt to select an autocomplete option failed due to incorrect element interaction.\n",
      "INFO     [agent] 🧠 Memory: Retrying to select the correct autocomplete suggestion for 'Kaylx Jang email address' to proceed with the search.\n",
      "INFO     [agent] 🎯 Next goal: Click on the first autocomplete suggestion to execute the search.\n",
      "INFO     [agent] 🛠️  Action 1/1: {\"click_element\":{\"index\":12}}\n",
      "WARNING  [controller] Element not clickable with index 12 - most likely the page changed\n",
      "INFO     [agent] 📍 Step 8\n",
      "INFO     [agent] ⚠ Eval: Failed - Incorrect interaction with the element to select autocomplete option.\n",
      "INFO     [agent] 🧠 Memory: The autocomplete suggestion for 'Kaylx Jang email address' needs to be selected correctly. Attempt to execute search after selecting suggestion.\n",
      "INFO     [agent] 🎯 Next goal: Select the 'address' autocomplete suggestion to proceed with the search.\n",
      "INFO     [agent] 🛠️  Action 1/2: {\"click_element\":{\"index\":12}}\n",
      "INFO     [agent] 🛠️  Action 2/2: {\"send_keys\":{\"keys\":\"Enter\"}}\n",
      "WARNING  [controller] Element not clickable with index 12 - most likely the page changed\n",
      "INFO     [agent] 📍 Step 9\n",
      "INFO     [agent] ⚠ Eval: Failed repeatedly due to an incorrect click on the autocomplete suggestion.\n",
      "INFO     [agent] 🧠 Memory: Task 2 of 3 stalled at selecting autocomplete suggestion for 'Kaylx Jang email address'. Attempted multiple times without success.\n",
      "INFO     [agent] 🎯 Next goal: Manually send Enter key to bypass autocomplete and execute search.\n",
      "INFO     [agent] 🛠️  Action 1/1: {\"send_keys\":{\"keys\":\"Enter\"}}\n",
      "INFO     [controller] ⌨️  Sent keys: Enter\n",
      "INFO     [agent] 📍 Step 10\n",
      "INFO     [agent] 👍 Eval: Success - Successfully executed the search.\n",
      "INFO     [agent] 🧠 Memory: Search results list includes email addresses for Kaylx Jang. Extract the email addresses: 'kaylx.jang@columbia.edu' and 'kyj2110@columbia.edu'.\n",
      "INFO     [agent] 🎯 Next goal: Extract mentioned emails and complete the task.\n",
      "INFO     [agent] 🛠️  Action 1/1: {\"done\":{\"text\":\"kaylx.jang@columbia.edu, kyj2110@columbia.edu\"}}\n",
      "INFO     [agent] 📄 Result: kaylx.jang@columbia.edu, kyj2110@columbia.edu\n",
      "INFO     [agent] ✅ Task completed successfully\n",
      "['kaylx.jang@columbia.edu, kyj2110@columbia.edu']\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from browser_use import Agent, BrowserConfig, Browser\n",
    "\n",
    "researcher = \"Kaylx Jang\"\n",
    "\n",
    "def browser_use_sync() -> str:\n",
    "    \"\"\"Search for email addresses synchronously using browser-use.\"\"\"\n",
    "    browser = Browser(\n",
    "        config=BrowserConfig(\n",
    "            headless=True,\n",
    "            disable_security=True\n",
    "        )\n",
    "    )\n",
    "\n",
    "    agent = Agent(\n",
    "        task=f\"\"\"\n",
    "        1. Go to Google.com.\n",
    "        2. Search '{researcher} email address' and enter.\n",
    "        3. Output only the emails or 'None'—no additional explanations.\n",
    "        \"\"\",\n",
    "        llm=ChatOpenAI(model=\"gpt-4o\"),\n",
    "        browser=browser,\n",
    "    )\n",
    "\n",
    "    loop = asyncio.get_event_loop()\n",
    "    result = loop.run_until_complete(agent.run())\n",
    "    return result.final_result().split(\"\\n\")\n",
    "\n",
    "result = browser_use_sync()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2e373",
   "metadata": {},
   "source": [
    "# With Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b2717f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shuosun@colorado.edu', 'shuosun@colorado.edu', 'shuosun@colorado.edu']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_email(pdf_path):\n",
    "    import re\n",
    "    return re.findall(r\"[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+\", extract_text_from_pdf(pdf_path))\n",
    "\n",
    "extract_email(\"pdfs/2307.13309.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4069df6c",
   "metadata": {},
   "source": [
    "# ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f3b16f2323ab2e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T09:28:41.904317Z",
     "start_time": "2025-02-17T09:28:40.287115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blaiszik@uchicago.edu']\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    store=True,\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a data extraction assistant. The user will provide the text from a PDF. \"\n",
    "            \"Identify and list all email addresses that appear in the text (i.e., strings containing '@'). \"\n",
    "            \"If you find no email addresses, return 'None'. \"\n",
    "            \"Output only the emails or 'None'—no additional explanations.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": pdf_texts\n",
    "    }\n",
    "    ],\n",
    "    temperature=0.8,\n",
    ")\n",
    "\n",
    "# Extract and print the message content\n",
    "list_of_emails = completion.choices[0].message.content.split(\"\\n\")\n",
    "print(list_of_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e6bd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv_scraped_data_backup.xlsx'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # FOR TEST # # #\n",
    "import shutil\n",
    "# Backup the original file before modifications\n",
    "shutil.copy(\"arxiv_scraped_data.xlsx\", \"arxiv_scraped_data_backup.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7832b41ae5b4082",
   "metadata": {},
   "source": [
    "# WEB SEARCH PERPLEXITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48900ca33bd67889",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:14:49.161300Z",
     "start_time": "2025-02-17T13:14:46.879780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matthew.zimmerman@hmh-cdi.org\n",
      "mzimmerman@theretailconnection.net\n",
      "mzimmerman@carleton.edu\n",
      "    ✅ [INFO] Email addresses listed: ['matthew.zimmerman@hmh-cdi.org', 'mzimmerman@theretailconnection.net', 'mzimmerman@carleton.edu']\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from openpyxl import load_workbook\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Optional\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "def _load_excel(file_name: str) -> object:\n",
    "    \"\"\"Load the excel file.\"\"\"\n",
    "    wb = load_workbook(filename=file_name)\n",
    "    ws = wb.active\n",
    "    return wb, ws\n",
    "\n",
    "class FindSimilarity:\n",
    "    \"\"\"Find similarity between the author names and emails.\"\"\"\n",
    "    def preprocess_emails(self, email_list: List) -> Optional[str]:\n",
    "        \"\"\"Preprocess the email address to extract email author name.\"\"\"\n",
    "        return \" \".join([email.split(\"@\")[0].lower() for email in email_list])\n",
    "\n",
    "    def find_email_author_and_save(self, list_of_emails: List[str]) -> object:\n",
    "        \"\"\"Find the email address and author name using Cosine Similarity.\"\"\"\n",
    "        print(\"\\n📍 Step 9: Finding Similarity and Saving to Excel!\")\n",
    "        wb, ws = _load_excel(\"arxiv_scraped_data_backup.xlsx\")\n",
    "        headers = [str(cell.value).strip().lower() if cell.value else None for cell in ws[1]]\n",
    "\n",
    "        if \"email\" in headers:\n",
    "            email_column = headers.index(\"email\") + 1\n",
    "        else:\n",
    "            email_column = ws.max_column + 1\n",
    "            ws.cell(row=1, column=email_column, value=\"Email\")\n",
    "\n",
    "        for email in list_of_emails:\n",
    "            if email == \"None\":\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing email: {email}\")\n",
    "            doc1 = self.preprocess_emails([email])\n",
    "            match_found = False\n",
    "\n",
    "            for row_index, row in enumerate(ws.iter_rows(min_row=2, max_row=ws.max_row, values_only=True), start=2):\n",
    "                author_name = row[0]\n",
    "                if not author_name:\n",
    "                    continue\n",
    "\n",
    "                doc2 = str(author_name)\n",
    "\n",
    "                vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 2), lowercase=True)\n",
    "                tfidf_matrix = vectorizer.fit_transform([doc1, doc2])\n",
    "\n",
    "                cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "                if cosine_sim[0][0] > 0.6:\n",
    "                    print(f\"Match Found: {author_name} | {cosine_sim[0][0]}\")\n",
    "                    current_email = ws.cell(row=row_index, column=email_column).value\n",
    "                    # If there is already an email in the cell, append the new one separated by a comma\n",
    "                    if current_email:\n",
    "                        new_email = f\"{current_email}, {email}\"\n",
    "                        ws.cell(row=row_index, column=email_column, value=new_email)\n",
    "                    else:\n",
    "                        ws.cell(row=row_index, column=email_column, value=email)\n",
    "                    match_found = True\n",
    "\n",
    "            if not match_found:\n",
    "                last_row = ws.max_row + 1\n",
    "                ws.cell(row=last_row, column=email_column, value=email)\n",
    "\n",
    "        return wb.save(\"arxiv_scraped_data_backup.xlsx\")\n",
    "\n",
    "import requests\n",
    "from pydantic import BaseModel\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class AnswerFormat(BaseModel):\n",
    "    email_adress: str\n",
    "\n",
    "url = \"https://api.perplexity.ai/chat/completions\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.getenv('PERPLEXITY_API_KEY')}\"}\n",
    "payload = {\n",
    "    \"model\": \"sonar-pro\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\":(\n",
    "            \"You are a web searcher assistant. The user will provide an author name.\"\n",
    "            \"Your task is: Search email addresses for provided author name.\"\n",
    "            \"If you find no email addresses, return 'None'. \"\n",
    "            \"Output only the emails or 'None'—no additional explanations.\"\n",
    "         ),\n",
    "         },\n",
    "        {\"role\": \"user\", \"content\": \"Matthew Zimmermann email adress\"},\n",
    "    ],\n",
    "    \"response_format\": {\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\"schema\": AnswerFormat.model_json_schema()},\n",
    "    },\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        response_json = response.json()\n",
    "        print(response_json[\"choices\"][0][\"message\"][\"content\"])\n",
    "        list_of_emails = response_json[\"choices\"][0][\"message\"][\"content\"].split(\"\\n\")\n",
    "        if list_of_emails:\n",
    "            print(f\"    ✅ [INFO] Email addresses listed: {list_of_emails}\")\n",
    "\n",
    "    except requests.JSONDecodeError:\n",
    "        print(\"Error: Response content is not valid JSON\")\n",
    "else:\n",
    "    print(f\"Error: Received status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "986c5a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📍 Step 9: Finding Similarity and Saving to Excel!\n",
      "Processing email: matthew.zimmerman@hmh-cdi.org\n",
      "Match Found: Matthew Zimmermann | 0.8738762303945571\n",
      "Processing email: mzimmerman@theretailconnection.net\n",
      "Match Found: Matthew Zimmermann | 0.7104343667728072\n",
      "Processing email: mzimmerman@carleton.edu\n",
      "Match Found: Matthew Zimmermann | 0.7104343667728072\n"
     ]
    }
   ],
   "source": [
    "similarity_finder = FindSimilarity()\n",
    "similarity_finder.find_email_author_and_save(list_of_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78912ae",
   "metadata": {},
   "source": [
    "# WEB SEARCH GEMINI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42777ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was unable to find the direct email address for Kaylx Jang. However, I did find the following information that may help you find it:\n",
      "\n",
      "*   **Kaylx Jang** is associated with the **Lightwave Research Laboratory at Columbia University.** You may be able to contact the lab directly to inquire about reaching Kaylx.\n",
      "*   Kaylx Jang's profile mentions previous experience at **Ayar Labs** and **Nokia (formerly Elenion).** If relevant, you could try contacting Kaylx through these past employers.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.genai import types\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=\"Please search and retrieve me Kaylx Jang email!\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        tools=[types.Tool(\n",
    "            google_search=types.GoogleSearchRetrieval(\n",
    "                dynamic_retrieval_config=types.DynamicRetrievalConfig(\n",
    "                    dynamic_threshold=0.6))\n",
    "        )]\n",
    "    )\n",
    ")\n",
    "content = response.candidates[0].content.parts[0].text\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "347901ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Part(video_metadata=None, thought=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, inline_data=None, text=\"Carlos Alcaraz won the Wimbledon men's singles title in 2024. He defeated Novak Djokovic in the final, retaining his title from the previous year.\\n\")]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].content.parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c521ec",
   "metadata": {},
   "source": [
    "# Arxiv Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eaaeb84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quantim optics']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = [\"quantim optics\"]\n",
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e653a627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quantim optics'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df5153d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
