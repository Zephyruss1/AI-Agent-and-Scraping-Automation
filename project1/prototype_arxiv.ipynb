{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T17:29:56.606173Z",
     "start_time": "2025-02-05T17:29:55.745914Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 1,
   "source": [
    "import bs4 as BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from typing import Optional, List, Dict, Any, Tuple"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:48:25.728884Z",
     "start_time": "2025-02-05T20:47:46.911347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from typing import List, Optional\n",
    "import time\n",
    "\n",
    "class ArxivScraper:\n",
    "    def __init__(self):\n",
    "        options = Options()\n",
    "        options.add_argument(\"--no-sandbox\")\n",
    "        options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        options.add_argument(\"--ignore-certificate-errors\")\n",
    "        # options.add_argument(\"--headless\")  # Uncomment to run in headless mode\n",
    "\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        self.paper_id_list : List[str] = []\n",
    "        self.author_list : List[Dict] = []\n",
    "\n",
    "    def pagination(self) -> Optional[bool]:\n",
    "        print(\"     [INFO] Pagination enabled!\")\n",
    "        next_page_locator = (By.XPATH, '//*[@id=\"main-container\"]/div[2]/nav[2]/a[2]')\n",
    "        try:\n",
    "            next_page = WebDriverWait(self.driver, 10).until(EC.visibility_of_element_located(next_page_locator))\n",
    "            actions = ActionChains(self.driver)\n",
    "            actions.click(next_page).perform()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def connect_to_arxiv(self):\n",
    "        print(\"\\nüìç Step 1: Connecting to Arxiv!\")\n",
    "        url = \"https://arxiv.org/\"\n",
    "        keyword = \"photonic circuits\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(2)\n",
    "            print(\"     [INFO] Arxiv connection successful!\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "        try:\n",
    "            search_box = self.driver.find_element(By.XPATH, '//*[@id=\"header\"]/div[2]/form/div/div[1]/input')\n",
    "            search_box.send_keys(keyword)\n",
    "            search_box.send_keys(Keys.RETURN)\n",
    "            time.sleep(2)\n",
    "            print(\"     [INFO] Searching keywords!\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def get_paper_ids(self, pagination: bool = True) -> List[str]:\n",
    "        print(\"\\nüìç Step 2: Scraping paper links!\")\n",
    "        try:\n",
    "            paper_entries = self.driver.find_elements(By.XPATH, '//*[@id=\"main-container\"]/div[2]/ol/li')\n",
    "            for paper in paper_entries:\n",
    "                paper_link = paper.find_element(By.XPATH, './/p[@class=\"list-title is-inline-block\"]/a').get_attribute(\"href\")\n",
    "                paper_id = paper_link.split(\"/\")[-1]\n",
    "                self.paper_id_list.append(paper_id)\n",
    "\n",
    "            if pagination is True:\n",
    "                self.get_paper_ids()\n",
    "                print(\"     [INFO] Page found rotating the next page!\")\n",
    "            else:\n",
    "                print(\"     [INFO] No more pages to scrape!\")\n",
    "\n",
    "            print(\"     [INFO] Paper links scraped successfully!\")\n",
    "            return self.paper_id_list\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_author_details(self) -> Optional[List[Dict]]:\n",
    "        print(\"\\nüìç Step 3: Scraping author details!\")\n",
    "\n",
    "        if not self.paper_id_list:\n",
    "            print(\"     [INFO] No paper links found!\")\n",
    "            return None\n",
    "\n",
    "        paper_id = next(iter(self.paper_id_list))\n",
    "        api = f\"https://api.semanticscholar.org/v1/paper/arXiv:{paper_id}?include_unknown_references=true\"\n",
    "        self.author_list = []\n",
    "        try:\n",
    "            response = requests.get(api)\n",
    "            data = response.json()\n",
    "            authors = data[\"authors\"]\n",
    "            for author in authors:\n",
    "                self.author_list.append({\"Author\": author[\"name\"], \"ID\": author[\"authorId\"]})\n",
    "            print(\"     [INFO] Author details scraped successfully!\")\n",
    "            return self.author_list\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    # TODO: Handling with pagination\n",
    "    def get_releated_papers(self, pagination: bool = True) -> Optional[List[Dict]]:\n",
    "        print(\"\\nüìç Step 4: Connecting to Scholar and scraping related papers!\")\n",
    "        if not self.author_list:\n",
    "            print(\"     [INFO] No author details found!\")\n",
    "            return None\n",
    "\n",
    "        author = next(iter(self.author_list))\n",
    "        url = \"https://www.semanticscholar.org/author/Iason-Gabriel/2343751686?sort=pub-date\"\n",
    "        try:\n",
    "            self.driver.get(url)\n",
    "            time.sleep(2)\n",
    "            print(\"     [INFO] Scholar connection successful!\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "        try:\n",
    "            paper_page = self.driver.find_element(By.XPATH,\n",
    "            '//*[@id=\"app\"]/div[1]/div[2]/div/main/div[2]/div/div/div/div[2]/div/div[2]/div/div/div[1]/a')\n",
    "            paper_date = self.driver.find_element(By.CSS_SELECTOR, '.cl-paper-pubdates span span')\n",
    "\n",
    "            arxiv_link = paper_page.get_attribute(\"href\")\n",
    "            print(arxiv_link)\n",
    "            print(paper_date.text)\n",
    "            for details in self.author_list:\n",
    "                details[\"LastPaperLink\"] = arxiv_link\n",
    "                details[\"LastPaperDate\"] = paper_date.text\n",
    "                details[\"AmountOfMentions\"] = 1\n",
    "\n",
    "            # if pagination:\n",
    "            #     self.get_releated_papers()\n",
    "            #     print(\"     [INFO] Page found rotating the next page!\")\n",
    "            # else:\n",
    "            #     print(\"     [INFO] No more pages to scrape!\")\n",
    "\n",
    "            print(\"     [INFO] Related papers scraped successfully!\")\n",
    "            return self.author_list\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _save_excel(self):\n",
    "        pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ArxivScraper()\n",
    "    scraper.connect_to_arxiv()\n",
    "    scraper.get_paper_ids(pagination=False)\n",
    "    scraper.get_author_details()\n",
    "    scraper.get_releated_papers(pagination=False)\n",
    "    scraper.driver.quit()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìç Step 1: Connecting to Arxiv!\n",
      "     [INFO] Arxiv connection successful!\n",
      "     [INFO] Searching keywords!\n",
      "\n",
      "üìç Step 2: Scraping paper links!\n",
      "     [INFO] No more pages to scrape!\n",
      "     [INFO] Paper links scraped successfully!\n",
      "\n",
      "üìç Step 3: Scraping author details!\n",
      "     [INFO] Author details scraped successfully!\n",
      "\n",
      "üìç Step 4: Connecting to Scholar and scraping related papers!\n",
      "     [INFO] Scholar connection successful!\n",
      "https://arxiv.org/pdf/2502.02528.pdf\n",
      "4 February 2025\n",
      "     [INFO] Related papers scraped successfully!\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
